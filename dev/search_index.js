var documenterSearchIndex = {"docs":
[{"location":"lib/double_pendulum/#Double-Pendulum-Model","page":"Double Pendulum Model","title":"Double Pendulum Model","text":"","category":"section"},{"location":"lib/double_pendulum/","page":"Double Pendulum Model","title":"Double Pendulum Model","text":"An undamped double pendulum can be constructed using the DoublePendulum function, as shown below. Models are provided for the fully-actuated version, the undriven version, and both of the underactuated versions (also accessible via the convenience functions Acrobot and Pendubot). Additionally, when also using the Plots.jl package, the convenience plotting function plot_double_pendulum is provided.","category":"page"},{"location":"lib/double_pendulum/","page":"Double Pendulum Model","title":"Double Pendulum Model","text":"(Image: Double pendulum animation)","category":"page"},{"location":"lib/double_pendulum/#NeuralLyapunovProblemLibrary.DoublePendulum","page":"Double Pendulum Model","title":"NeuralLyapunovProblemLibrary.DoublePendulum","text":"DoublePendulum(; actuation=:fully_actuated, name, defaults)\n\nCreate an ODESystem representing an undamped double pendulum.\n\nThe posture of the double pendulum is determined by θ1 and θ2, the angle of the first and second pendula, respectively. θ1 is measured counter-clockwise relative to the downward equilibrium and θ2 is measured counter-clockwise relative to θ1 (i.e., when θ2 is fixed at 0, the double pendulum appears as a single pendulum).\n\nThe ODESystem uses the explicit manipulator form of the equations:\n\nq = M^-1(q) (-C(qq)q + τ_g(q) + Bu)\n\nThe name of the ODESystem is name.\n\nActuation modes\n\nThe four actuation modes are described in the table below and selected via actuation.\n\nActuation mode (actuation) Torque around θ1 Torque around θ2\n:fully_actuated (default) τ1 τ2\n:acrobot Not actuated τ\n:pendubot τ Not actuated\n:undriven Not actuated Not actuated\n\nODESystem Parameters\n\nI1: moment of inertia of the first pendulum around its pivot (not its center of mass).\nI2:  moment of inertia of the second pendulum around its pivot (not its center of mass).\nl1: length of the first pendulum.\nl2: length of the second pendulum.\nlc1: distance from pivot to the center of mass of the first pendulum.\nlc2: distance from the link to the center of mass of the second pendulum.\nm1: mass of the first pendulum.\nm2: mass of the second pendulum.\ng: gravitational acceleration (defaults to 9.81).\n\nUsers may optionally provide default values of the parameters through defaults: a vector of the default values for [I1, I2, l1, l2, lc1, lc2, m1, m2, g].\n\n\n\n\n\n","category":"function"},{"location":"lib/double_pendulum/#NeuralLyapunovProblemLibrary.Acrobot","page":"Double Pendulum Model","title":"NeuralLyapunovProblemLibrary.Acrobot","text":"Acrobot(; name, defaults)\n\nAlias for DoublePendulum(; actuation = :acrobot, name, defaults).\n\n\n\n\n\n","category":"function"},{"location":"lib/double_pendulum/#NeuralLyapunovProblemLibrary.Pendubot","page":"Double Pendulum Model","title":"NeuralLyapunovProblemLibrary.Pendubot","text":"Pendubot(; name, defaults)\n\nAlias for DoublePendulum(; actuation = :pendubot, name, defaults).\n\n\n\n\n\n","category":"function"},{"location":"lib/double_pendulum/#Copy-Pastable-Code","page":"Double Pendulum Model","title":"Copy-Pastable Code","text":"","category":"section"},{"location":"lib/double_pendulum/","page":"Double Pendulum Model","title":"Double Pendulum Model","text":"using Random; Random.seed!(200) # hide\nusing ModelingToolkit, NeuralLyapunovProblemLibrary, Plots, OrdinaryDiffEq\n\n@named double_pendulum_undriven = DoublePendulum(; actuation = :undriven)\n\nt, = independent_variables(double_pendulum_undriven)\nDt = Differential(t)\nθ1, θ2 = unknowns(double_pendulum_undriven)\nx0 = Dict([θ1, θ2, Dt(θ1), Dt(θ2)] .=> vcat(2π * rand(2) .- π, zeros(2)))\n\n# Assume uniform rods of random mass and length\nm1, m2 = ones(2)\nl1, l2 = ones(2)\nlc1, lc2 = l1 /2, l2 / 2\nI1 = m1 * l1^2 / 3\nI2 = m2 * l2^2 / 3\ng = 1.0\np = Dict(parameters(double_pendulum_undriven) .=> [I1, I2, l1, l2, lc1, lc2, m1, m2, g])\n\nprob = ODEProblem(structural_simplify(double_pendulum_undriven), x0, 100, p)\nsol = solve(prob, Tsit5(), abstol = 1e-10, reltol = 1e-10)\n\np = [I1, I2, l1, l2, lc1, lc2, m1, m2, g]\ngif(plot_double_pendulum(sol, p); fps=50)","category":"page"},{"location":"lib/double_pendulum/#Plotting-the-Double-Pendulum","page":"Double Pendulum Model","title":"Plotting the Double Pendulum","text":"","category":"section"},{"location":"lib/double_pendulum/#NeuralLyapunovProblemLibrary.plot_double_pendulum","page":"Double Pendulum Model","title":"NeuralLyapunovProblemLibrary.plot_double_pendulum","text":"plot_double_pendulum(θ1, θ2, p, t; title)\nplot_double_pendulum(sol, p; title, N, angle_symbol)\n\nPlot the pendulum's trajectory.\n\nArguments\n\nθ1: The angle of the first pendulum link at each time step.\nθ2: The angle of the second pendulum link at each time step.\nt: The time steps.\nsol: The solution to the ODE problem.\np: The parameters of the double pendulum.\n\nKeyword arguments\n\ntitle: The title of the plot; defaults to no title (i.e., title=\"\").\nN: The number of points to plot; when using θ and t, uses length(t); defaults to 500 when using sol.\nangle1_symbol: The symbol of the angle of the first link in sol; defaults to :θ1.\nangle2_symbol: The symbol of the angle of the second link in sol; defaults to :θ2.\n\n\n\n\n\n","category":"function"},{"location":"man/minimization/#Lyapunov-Minimization-Condition","page":"Lyapunov Minimization Condition","title":"Lyapunov Minimization Condition","text":"","category":"section"},{"location":"man/minimization/","page":"Lyapunov Minimization Condition","title":"Lyapunov Minimization Condition","text":"The condition that the Lyapunov function V(x) must be minimized uniquely at the fixed point x_0 is often represented as a requirement for V(x) to be positive away from the fixed point and zero at the fixed point. Put mathematically, it is sufficient to require V(x)  0  forall x ne x_0 and V(x_0) = 0. We call such functions positive definite (around the fixed point x_0). The weaker condition that V(x) ge 0  forall x ne x_0 and V(x_0) = 0 is positive semi-definiteness.","category":"page"},{"location":"man/minimization/","page":"Lyapunov Minimization Condition","title":"Lyapunov Minimization Condition","text":"NeuralLyapunov.jl provides the LyapunovMinimizationCondition struct for users to specify the form of the minimization condition to enforce through training.","category":"page"},{"location":"man/minimization/#NeuralLyapunov.LyapunovMinimizationCondition","page":"Lyapunov Minimization Condition","title":"NeuralLyapunov.LyapunovMinimizationCondition","text":"LyapunovMinimizationCondition(check_nonnegativity, strength, rectifier, check_fixed_point)\n\nSpecifies the form of the Lyapunov minimization condition to be used.\n\nFields\n\ncheck_nonnegativity::Bool: whether or not to train for positivity/nonnegativity of V(x)\nstrength::Function: specifies the level of strictness for positivity training; should be zero when the two inputs are equal and nonnegative otherwise; used when check_nonnegativity == true\nrectifier::Function: positive when the input is positive and (approximately) zero when the input is negative\ncheck_fixed_point: whether or not to train for V(x_0) = 0.\n\nTraining conditions\n\nIf check_nonnegativity is true, training will attempt to enforce:\n\nV(x)  textttstrength(x x_0)\n\nThe inequality will be approximated by the equation:\n\ntextttrectifier(textttstrength(x x_0) - V(x_0)) = 0\n\nNote that the approximate equation and inequality are identical when textttrectifier(t) = max(0 t).\n\nIf check_fixed_point is true, then training will also attempt to enforce V(x_0) = 0.\n\nExamples\n\nWhen training for a strictly positive definite V, an example of an appropriate strength is textttstrength(x x_0) = lVert x - x_0 rVert^2. This form is used in StrictlyPositiveDefinite.\n\nIf V were structured such that it is always nonnegative, then V(x_0) = 0 is all that must be enforced in training for the Lyapunov function to be uniquely minimized at x_0. In that case, we would use     check_nonnegativity = false;  check_fixed_point = true. This can also be accomplished with DontCheckNonnegativity(true).\n\nIn either case, the rectified linear unit rectifier(t) = max(zero(t), t) exactly represents the inequality, but differentiable approximations of this function may be employed.\n\n\n\n\n\n","category":"type"},{"location":"man/minimization/#Pre-defined-minimization-conditions","page":"Lyapunov Minimization Condition","title":"Pre-defined minimization conditions","text":"","category":"section"},{"location":"man/minimization/#NeuralLyapunov.StrictlyPositiveDefinite","page":"Lyapunov Minimization Condition","title":"NeuralLyapunov.StrictlyPositiveDefinite","text":"StrictlyPositiveDefinite(; C, check_fixed_point, rectifier)\n\nConstruct a LyapunovMinimizationCondition representing     V(x)  C lVert x - x_0 rVert^2. If check_fixed_point == true (as is the default), then training will also attempt to enforce V(x_0) = 0.\n\nThe inequality is approximated by     textttrectifier(C lVert x - x_0 rVert^2 - V(x)) = 0, and the default rectifier is the rectified linear unit (t) -> max(zero(t), t), which exactly represents V(x)  C lVert x - x_0 rVert^2. C defaults to 1e-6.\n\n\n\n\n\n","category":"function"},{"location":"man/minimization/#NeuralLyapunov.PositiveSemiDefinite","page":"Lyapunov Minimization Condition","title":"NeuralLyapunov.PositiveSemiDefinite","text":"PositiveSemiDefinite(; check_fixed_point, rectifier)\n\nConstruct a LyapunovMinimizationCondition representing V(x)  0. If check_fixed_point == true (as is the default), then training will also attempt to enforce V(x_0) = 0.\n\nThe inequality is approximated by textttrectifier( -V(x) ) = 0 and the default rectifier is the rectified linear unit (t) -> max(zero(t), t), which exactly represents V(x)  0.\n\n\n\n\n\n","category":"function"},{"location":"man/minimization/#NeuralLyapunov.DontCheckNonnegativity","page":"Lyapunov Minimization Condition","title":"NeuralLyapunov.DontCheckNonnegativity","text":"DontCheckNonnegativity(; check_fixed_point)\n\nConstruct a LyapunovMinimizationCondition which represents not checking for nonnegativity of the Lyapunov function. This is appropriate in cases where this condition has been structurally enforced.\n\nEven in this case, it is still possible to check for V(x_0) = 0, for example if V is structured to be positive for x  x_0 but does not guarantee V(x_0) = 0 (such as NonnegativeNeuralLyapunov). check_fixed_point defaults to true, since in cases where V(x_0) = 0 is enforced structurally, the equation will reduce to 0.0 ~ 0.0, which gets automatically removed in most cases.\n\n\n\n\n\n","category":"function"},{"location":"man/minimization/#Defining-your-own-minimization-condition","page":"Lyapunov Minimization Condition","title":"Defining your own minimization condition","text":"","category":"section"},{"location":"man/minimization/","page":"Lyapunov Minimization Condition","title":"Lyapunov Minimization Condition","text":"If a user wishes to define their own version of the minimization condition in a form other than V(x)  textttstrength(x x_0), they must define their own subtype of AbstractLyapunovMinimizationCondition.","category":"page"},{"location":"man/minimization/#NeuralLyapunov.AbstractLyapunovMinimizationCondition","page":"Lyapunov Minimization Condition","title":"NeuralLyapunov.AbstractLyapunovMinimizationCondition","text":"AbstractLyapunovMinimizationCondition\n\nRepresents the minimization condition in a neural Lyapunov problem\n\nAll concrete AbstractLyapunovMinimizationCondition subtypes should define the check_nonnegativity, check_fixed_point, and get_minimization_condition functions.\n\n\n\n\n\n","category":"type"},{"location":"man/minimization/","page":"Lyapunov Minimization Condition","title":"Lyapunov Minimization Condition","text":"When constructing the PDESystem, NeuralLyapunovPDESystem uses check_nonnegativity to determine if it should include an equation equating the result of get_minimization_condition to zero. It additionally uses check_minimal_fixed_point to determine if it should include the equation V(x_0) = 0.","category":"page"},{"location":"man/minimization/#NeuralLyapunov.check_nonnegativity","page":"Lyapunov Minimization Condition","title":"NeuralLyapunov.check_nonnegativity","text":"check_nonnegativity(cond::AbstractLyapunovMinimizationCondition)\n\nReturn true if cond specifies training to meet the Lyapunov minimization condition, and false if cond specifies no training to meet this condition.\n\n\n\n\n\n","category":"function"},{"location":"man/minimization/#NeuralLyapunov.check_minimal_fixed_point","page":"Lyapunov Minimization Condition","title":"NeuralLyapunov.check_minimal_fixed_point","text":"check_minimal_fixed_point(cond::AbstractLyapunovMinimizationCondition)\n\nReturn true if cond specifies training for the Lyapunov function to equal zero at the fixed point, and false if cond specifies no training to meet this condition.\n\n\n\n\n\n","category":"function"},{"location":"man/minimization/#NeuralLyapunov.get_minimization_condition","page":"Lyapunov Minimization Condition","title":"NeuralLyapunov.get_minimization_condition","text":"get_minimization_condition(cond::AbstractLyapunovMinimizationCondition)\n\nReturn a function of V, x, and x_0 that equals zero when the Lyapunov minimization condition is met for the Lyapunov candidate function V at the point x, and is greater than zero if it's violated.\n\nNote that the first input, V, is a function, so the minimization condition can depend on the value of the candidate Lyapunov function at multiple points.\n\n\n\n\n\n","category":"function"},{"location":"man/pdesystem/#Solving-a-Neural-Lyapunov-Problem","page":"Solving a Neural Lyapunov Problem","title":"Solving a Neural Lyapunov Problem","text":"","category":"section"},{"location":"man/pdesystem/","page":"Solving a Neural Lyapunov Problem","title":"Solving a Neural Lyapunov Problem","text":"NeuralLyapunov.jl represents neural Lyapunov problems as systems of partial differential equations, using the ModelingToolkit.PDESystem type. Such a PDESystem can then be solved using a physics-informed neural network through NeuralPDE.jl.","category":"page"},{"location":"man/pdesystem/","page":"Solving a Neural Lyapunov Problem","title":"Solving a Neural Lyapunov Problem","text":"Candidate Lyapunov functions will be trained within a box domain subset of the state space.","category":"page"},{"location":"man/pdesystem/#NeuralLyapunov.NeuralLyapunovPDESystem","page":"Solving a Neural Lyapunov Problem","title":"NeuralLyapunov.NeuralLyapunovPDESystem","text":"NeuralLyapunovPDESystem(dynamics::ODESystem, bounds, spec; <keyword_arguments>)\nNeuralLyapunovPDESystem(dynamics::Function, lb, ub, spec; <keyword_arguments>)\n\nConstruct a ModelingToolkit.PDESystem representing the specified neural Lyapunov problem.\n\nPositional Arguments\n\ndynamics: the dynamical system being analyzed, represented as an ODESystem or the function f such that ẋ = f(x[, u], p, t); either way, the ODE should not depend on time and only t = 0.0 will be used. (For an example of when f would have a u argument, see add_policy_search.)\nbounds: an array of domains, defining the training domain by bounding the states (and derivatives, when applicable) of dynamics; only used when dynamics isa ODESystem, otherwise use lb and ub.\nlb and ub: the training domain will be lb_1 ub_1lb_2 ub_2; not used when dynamics isa ODESystem, then use bounds.\nspec: a NeuralLyapunovSpecification defining the Lyapunov function structure, as well as the minimization and decrease conditions.\n\nKeyword Arguments\n\nfixed_point: the equilibrium being analyzed; defaults to the origin.\np: the values of the parameters of the dynamical system being analyzed; defaults to SciMLBase.NullParameters(); not used when dynamics isa ODESystem, then use the default parameter values of dynamics.\nstate_syms: an array of the Symbol representing each state; not used when dynamics isa ODESystem, then the symbols from dynamics are used; if dynamics isa ODEFunction, symbols stored there are used, unless overridden here; if not provided here and cannot be inferred, [:state1, :state2, ...] will be used.\nparameter_syms: an array of the Symbol representing each parameter; not used when dynamics isa ODESystem, then the symbols from dynamics are used; if dynamics isa ODEFunction, symbols stored there are used, unless overridden here; if not provided here and cannot be inferred, [:param1, :param2, ...] will be used.\npolicy_search::Bool: whether or not to include a loss term enforcing fixed_point to actually be a fixed point; defaults to false; only used when dynamics isa Function && !(dynamics isa ODEFunction); when dynamics isa ODEFunction, policy_search should not be supplied (as it must be false); when dynamics isa ODESystem, value inferred by the presence of unbound inputs.\nname: the name of the constructed PDESystem\n\n\n\n\n\n","category":"function"},{"location":"man/pdesystem/","page":"Solving a Neural Lyapunov Problem","title":"Solving a Neural Lyapunov Problem","text":"warning: Warning\nWhen using NeuralLyapunovPDESystem, the Lyapuonv function structure, minimization and decrease conditions, and dynamics will all be symbolically traced to generate the resulting PDESystem equations. In some cases tracing results in more efficient code, but in others it can result in inefficiencies or even errors.If the generated PDESystem is then used with NeuralPDE.jl, that library's parser will convert the equations into Julia functions representing the loss, which presents another opportunity for unexpected errors.","category":"page"},{"location":"man/pdesystem/#Extracting-the-numerical-Lyapunov-function","page":"Solving a Neural Lyapunov Problem","title":"Extracting the numerical Lyapunov function","text":"","category":"section"},{"location":"man/pdesystem/","page":"Solving a Neural Lyapunov Problem","title":"Solving a Neural Lyapunov Problem","text":"We provide the following convenience function for generating the Lyapunov function after the parameters have been found. If the PDESystem was solved using NeuralPDE.jl and Optimization.jl, then the argument phi is a field of the output of NeuralPDE.discretize and the argument θ is res.u.depvar where res is the result of the optimization.","category":"page"},{"location":"man/pdesystem/#NeuralLyapunov.get_numerical_lyapunov_function","page":"Solving a Neural Lyapunov Problem","title":"NeuralLyapunov.get_numerical_lyapunov_function","text":"get_numerical_lyapunov_function(phi, θ, structure, dynamics, fixed_point;\n                                <keyword_arguments>)\n\nCombine Lyapunov function structure, dynamics, and neural network weights to generate Julia functions representing the Lyapunov function and its time derivative: V(x) V(x).\n\nThese functions can operate on a state vector or columnwise on a matrix of state vectors.\n\nPositional Arguments\n\nphi: the neural network, represented as phi(x, θ) if the neural network has a single output, or a Vector of the same with one entry per neural network output.\nθ: the parameters of the neural network; If the neural network has multiple outputs, θ[:φ1] should be the parameters of the first neural network output, θ[:φ2] the parameters of the second (if there are multiple), and so on. If the neural network has a single output, θ should be the parameters of the network.\nstructure: a NeuralLyapunovStructure representing the structure of the neural Lyapunov function.\ndynamics: the system dynamics, as a function to be used in conjunction with structure.f_call.\nfixed_point: the equilibrium point being analyzed by the Lyapunov function.\n\nKeyword Arguments\n\np: parameters to be passed into dynamics; defaults to SciMLBase.NullParameters().\nuse_V̇_structure: when true, V(x) is calculated using structure.V̇; when false, V(x) is calculated using deriv as fract V(x + t f(x)) at t = 0; defaults to false, as it is more efficient in many cases.\nderiv: a function for calculating derivatives; defaults to (and expects same arguments as) ForwardDiff.derivative; only used when use_V̇_structure is false.\njac: a function for calculating Jacobians; defaults to (and expects same arguments as) ForwardDiff.jacobian; only used when use_V̇_structure is true.\nJ_net: the Jacobian of the neural network, specified as a function J_net(phi, θ, state); if isnothing(J_net) (as is the default), J_net will be calculated using jac; only used when use_V̇_structure is true.\n\n\n\n\n\n","category":"function"},{"location":"man/policy_search/#Policy-Search-and-Network-Dependent-Dynamics","page":"Policy Search and Network-Dependent Dynamics","title":"Policy Search and Network-Dependent Dynamics","text":"","category":"section"},{"location":"man/policy_search/","page":"Policy Search and Network-Dependent Dynamics","title":"Policy Search and Network-Dependent Dynamics","text":"At times, we wish to model a component of the dynamics with a neural network. A common example is the policy search case, when the closed-loop dynamics include a neural network controller. In such cases, we consider the dynamics to take the form of fracdxdt = f(x u p t), where u is the control input/the contribution to the dynamics from the neural network. We provide the add_policy_search function to transform a NeuralLyapunovStructure to include training the neural network to represent not just the Lyapunov function, but also the relevant part of the dynamics.","category":"page"},{"location":"man/policy_search/","page":"Policy Search and Network-Dependent Dynamics","title":"Policy Search and Network-Dependent Dynamics","text":"Similar to get_numerical_lyapunov_function, we provide the get_policy convenience function to construct u(x) that can be combined with the open-loop dynamics f(x u p t) to create closed loop dynamics f_cl(x p t) = f(x u(x) p t).","category":"page"},{"location":"man/policy_search/#NeuralLyapunov.add_policy_search","page":"Policy Search and Network-Dependent Dynamics","title":"NeuralLyapunov.add_policy_search","text":"add_policy_search(lyapunov_structure, new_dims; control_structure)\n\nAdd dependence on the neural network to the dynamics in a NeuralLyapunovStructure.\n\nArguments\n\nlyapunov_structure::NeuralLyapunovStructure: provides structure for V V; should assume dynamics take a form of f(x, p, t).\nnew_dims::Integer: number of outputs of the neural network to pass into the dynamics through control_structure.\n\nKeyword Arguments\n\ncontrol_structure::Function: transforms the final new_dims outputs of the neural net before passing them into the dynamics; defaults to identity, passing in the neural network outputs unchanged.\n\nThe returned NeuralLyapunovStructure expects dynamics of the form f(x, u, p, t), where u captures the dependence of dynamics on the neural network (e.g., through a control input). When evaluating the dynamics, it uses u = control_structure(phi_end(x)) where phi_end is a function that returns the final new_dims outputs of the neural network. The other lyapunov_structure.network_dim outputs are used for calculating V and V, as specified originally by lyapunov_structure.\n\n\n\n\n\n","category":"function"},{"location":"man/policy_search/#NeuralLyapunov.get_policy","page":"Policy Search and Network-Dependent Dynamics","title":"NeuralLyapunov.get_policy","text":"get_policy(phi, θ, network_dim, control_dim; control_structure)\n\nGenerate a Julia function representing the control policy/unmodeled portion of the dynamics as a function of the state.\n\nThe returned function can operate on a state vector or columnwise on a matrix of state vectors.\n\nArguments\n\nphi: the neural network, represented as phi(state, θ) if the neural network has a single output, or a Vector of the same with one entry per neural network output.\nθ: the parameters of the neural network; θ[:φ1] should be the parameters of the first neural network output (even if there is only one), θ[:φ2] the parameters of the second (if there are multiple), and so on.\nnetwork_dim: total number of neural network outputs.\ncontrol_dim: number of neural network outputs used in the control policy.\n\nKeyword Arguments\n\ncontrol_structure: transforms the final control_dim outputs of the neural net before passing them into the dynamics; defaults to identity, passing in the neural network outputs unchanged.\n\n\n\n\n\n","category":"function"},{"location":"demos/benchmarking/#Benchmarking-a-neural-Lyapunov-method","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"","category":"section"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"In this demonstration, we'll benchmark the neural Lyapunov method used in the policy search demo. In that demonstration, we searched for a neural network policy to stabilize the upright equilibrium of the inverted pendulum. Here, we will use the benchmark function to run approximately the same training, then check the performance the of the resulting controller and neural Lyapunov function by simulating the closed loop system to see (1) how well the controller drives the pendulum to the upright equilibrium, and (2) how well the neural Lyapunov function performs as a classifier of whether a state is in the region of attraction or not. These results will be represented by a confusion matrix using the simulation results as ground truth. (Keep in mind that training does no simulation.)","category":"page"},{"location":"demos/benchmarking/#Copy-Pastable-Code","page":"Benchmarking a neural Lyapunov method","title":"Copy-Pastable Code","text":"","category":"section"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"using NeuralPDE, NeuralLyapunov, Lux\nimport Boltz.Layers: PeriodicEmbedding\nusing OptimizationOptimisers, OptimizationOptimJL\nusing StableRNGs, Random\n\nrng = StableRNG(0)\nRandom.seed!(200)\n\n# Define dynamics and domain\nfunction open_loop_pendulum_dynamics(x, u, p, t)\n    θ, ω = x\n    ζ, ω_0 = p\n    τ = u[]\n    return [ω\n            -2ζ * ω_0 * ω - ω_0^2 * sin(θ) + τ]\nend\n\nlb = [0.0, -2.0];\nub = [2π, 2.0];\nupright_equilibrium = [π, 0.0]\np = [0.5, 1.0]\nstate_syms = [:θ, :ω]\nparameter_syms = [:ζ, :ω_0]\n\n# Define neural network discretization\n# We use an input layer that is periodic with period 2π with respect to θ\ndim_state = length(lb)\ndim_hidden = 25\ndim_phi = 3\ndim_u = 1\ndim_output = dim_phi + dim_u\nchain = [Chain(\n             PeriodicEmbedding([1], [2π]),\n             Dense(3, dim_hidden, tanh),\n             Dense(dim_hidden, dim_hidden, tanh),\n             Dense(dim_hidden, 1)\n         ) for _ in 1:dim_output]\nps = Lux.initialparameters(rng, chain)\n\n# Define neural network discretization\nstrategy = QuasiRandomTraining(10000)\n\n# Define neural Lyapunov structure\nperiodic_pos_def = function (state, fixed_point)\n    θ, ω = state\n    θ_eq, ω_eq = fixed_point\n    return (sin(θ) - sin(θ_eq))^2 + (cos(θ) - cos(θ_eq))^2 + 0.1 * (ω - ω_eq)^2\nend\n\nstructure = PositiveSemiDefiniteStructure(\n    dim_phi;\n    pos_def = (x, x0) -> log(1.0 + periodic_pos_def(x, x0))\n)\nstructure = add_policy_search(structure, dim_u)\n\nminimization_condition = DontCheckNonnegativity(check_fixed_point = false)\n\n# Define Lyapunov decrease condition\ndecrease_condition = AsymptoticStability(strength = periodic_pos_def)\n\n# Construct neural Lyapunov specification\nspec = NeuralLyapunovSpecification(\n    structure,\n    minimization_condition,\n    decrease_condition\n)\n\n# Define optimization parameters\nopt = [OptimizationOptimisers.Adam(0.05), OptimizationOptimJL.BFGS()]\noptimization_args = [[:maxiters => 300], [:maxiters => 300]]\n\n# Run benchmark\nendpoint_check = (x) -> ≈([sin(x[1]), cos(x[1]), x[2]], [0, -1, 0], atol = 5e-3)\nbenchmarking_results = benchmark(\n    open_loop_pendulum_dynamics,\n    lb,\n    ub,\n    spec,\n    chain,\n    strategy,\n    opt;\n    simulation_time = 200,\n    n = 1000,\n    fixed_point = upright_equilibrium,\n    p,\n    optimization_args,\n    state_syms,\n    parameter_syms,\n    policy_search = true,\n    endpoint_check,\n    init_params = ps\n)","category":"page"},{"location":"demos/benchmarking/#Detailed-Description","page":"Benchmarking a neural Lyapunov method","title":"Detailed Description","text":"","category":"section"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"Much of the set up is the same as in the policy search demo, so see that page for details.","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"using NeuralPDE, NeuralLyapunov, Lux\nimport Boltz.Layers: PeriodicEmbedding\nusing Random, StableRNGs\n\nRandom.seed!(200)\n\n# Define dynamics and domain\nfunction open_loop_pendulum_dynamics(x, u, p, t)\n    θ, ω = x\n    ζ, ω_0 = p\n    τ = u[]\n    return [ω\n            -2ζ * ω_0 * ω - ω_0^2 * sin(θ) + τ]\nend\n\nlb = [0.0, -2.0];\nub = [2π, 2.0];\nupright_equilibrium = [π, 0.0]\np = [0.5, 1.0]\nstate_syms = [:θ, :ω]\nparameter_syms = [:ζ, :ω_0]\n\n# Define neural network discretization\n# We use an input layer that is periodic with period 2π with respect to θ\ndim_state = length(lb)\ndim_hidden = 25\ndim_phi = 3\ndim_u = 1\ndim_output = dim_phi + dim_u\nchain = [Chain(\n             PeriodicEmbedding([1], [2π]),\n             Dense(3, dim_hidden, tanh),\n             Dense(dim_hidden, dim_hidden, tanh),\n             Dense(dim_hidden, 1)\n         ) for _ in 1:dim_output]\nps = Lux.initialparameters(StableRNG(0), chain)\n\n# Define neural network discretization\nstrategy = QuasiRandomTraining(10000)\n\n# Define neural Lyapunov structure\nperiodic_pos_def = function (state, fixed_point)\n    θ, ω = state\n    θ_eq, ω_eq = fixed_point\n    return (sin(θ) - sin(θ_eq))^2 + (cos(θ) - cos(θ_eq))^2 + 0.1 * (ω - ω_eq)^2\nend\n\nstructure = PositiveSemiDefiniteStructure(\n    dim_phi;\n    pos_def = (x, x0) -> log(1.0 + periodic_pos_def(x, x0))\n)\nstructure = add_policy_search(structure, dim_u)\n\nminimization_condition = DontCheckNonnegativity(check_fixed_point = false)\n\n# Define Lyapunov decrease condition\ndecrease_condition = AsymptoticStability(strength = periodic_pos_def)\n\n# Construct neural Lyapunov specification\nspec = NeuralLyapunovSpecification(\n    structure,\n    minimization_condition,\n    decrease_condition\n)\nnothing # hide","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"At this point of the policy search demo, we constructed the PDESystem, discretized it using NeuralPDE.jl, and solved the resulting OptimizationProblem using Optimization.jl. All of that occurs in the benchmark function, so we instead provide that function with the optimizer and optimization arguments to use.","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"using OptimizationOptimisers, OptimizationOptimJL\n\n# Define optimization parameters\nopt = [OptimizationOptimisers.Adam(0.05), OptimizationOptimJL.BFGS()]\noptimization_args = [[:maxiters => 300], [:maxiters => 300]]\nnothing # hide","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"Finally, we can run the benchmark function.","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"endpoint_check = (x) -> ≈([sin(x[1]), cos(x[1]), x[2]], [0, -1, 0], atol=5e-3)\nbenchmarking_results = benchmark(\n    open_loop_pendulum_dynamics,\n    lb,\n    ub,\n    spec,\n    chain,\n    strategy,\n    opt;\n    simulation_time = 200,\n    n = 1000,\n    fixed_point = upright_equilibrium,\n    p,\n    optimization_args,\n    state_syms,\n    parameter_syms,\n    policy_search = true,\n    endpoint_check,\n    init_params = ps\n);\nnothing # hide","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"We can observe the confusion matrix and training time:","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"benchmarking_results.confusion_matrix","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"benchmarking_results.training_time","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"The benchmark function also outputs the Lyapunov function V and its time-derivative V, along with the evaluation samples states (each sample is a column in the matrix) and the corresponding samples of V (V_samples) and V (V̇_samples).","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"all(benchmarking_results.V(benchmarking_results.states) .== benchmarking_results.V_samples)","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"all(benchmarking_results.V̇(benchmarking_results.states) .== benchmarking_results.V̇_samples)","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"The returned actual labels are just endpoint_check applied to endpoints, which are the results of simulating from each element of states.","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"all(endpoint_check.(benchmarking_results.endpoints) .== benchmarking_results.actual)","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"Similarly, the predicted labels are the results of the neural Lyapunov classifier.","category":"page"},{"location":"demos/benchmarking/","page":"Benchmarking a neural Lyapunov method","title":"Benchmarking a neural Lyapunov method","text":"classifier = (V, V̇, x) -> V̇ < zero(V̇) || endpoint_check(x)\nV_samples = eachcol(benchmarking_results.V_samples)\nV̇_samples = eachcol(benchmarking_results.V̇_samples)\nstates = eachcol(benchmarking_results.states)\nall(classifier.(V_samples, V̇_samples, states) .== benchmarking_results.predicted)","category":"page"},{"location":"lib/#NeuralLyapunovProblemLibrary.jl","page":"NeuralLyapunovProblemLibrary.jl","title":"NeuralLyapunovProblemLibrary.jl","text":"","category":"section"},{"location":"lib/","page":"NeuralLyapunovProblemLibrary.jl","title":"NeuralLyapunovProblemLibrary.jl","text":"For testing and benchmarking purposes, the NeuralLyapunovProblemLibrary.jl package is provided in the same repository (in the lib folder). This package contains ModelingToolkit models for the simple pendulum (Pendulum), the double pendulum (DoublePendulum), a planar approximation of the quadrotor (QuadrotorPlanar), and a full 3D model of the quadrotor (Quadrotor3D). Additionally, when used with the Plots.jl package, methods are provided for generating animations of the trajectory of each model (shown below).","category":"page"},{"location":"lib/","page":"NeuralLyapunovProblemLibrary.jl","title":"NeuralLyapunovProblemLibrary.jl","text":"Pages = Main.NEURALLYAPUNOVPROBLEMLIBRARY_PAGES[2:end]\nDepth = 3","category":"page"},{"location":"lib/","page":"NeuralLyapunovProblemLibrary.jl","title":"NeuralLyapunovProblemLibrary.jl","text":" \n(Image: 3D quadrotor animation) (Image: Double pendulum animation)\n(Image: Planar quadrotor animation) (Image: Pendulum animation)","category":"page"},{"location":"man/structure/#Structuring-a-Neural-Lyapunov-function","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"","category":"section"},{"location":"man/structure/","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"Three simple neural Lyapunov function structures are provided, but users can always specify a custom structure using the NeuralLyapunovStructure struct.","category":"page"},{"location":"man/structure/#Pre-defined-structures","page":"Structuring a Neural Lyapunov function","title":"Pre-defined structures","text":"","category":"section"},{"location":"man/structure/","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"The simplest structure is to train the neural network directly to be the Lyapunov function, which can be accomplished using an UnstructuredNeuralLyapunov.","category":"page"},{"location":"man/structure/#NeuralLyapunov.UnstructuredNeuralLyapunov","page":"Structuring a Neural Lyapunov function","title":"NeuralLyapunov.UnstructuredNeuralLyapunov","text":"UnstructuredNeuralLyapunov()\n\nCreate a NeuralLyapunovStructure where the Lyapunov function is the neural network evaluated at the state. This does not structurally enforce any Lyapunov conditions.\n\nCorresponds to V(x) = ϕ(x), where ϕ is the neural network.\n\nDynamics are assumed to be in f(state, p, t) form, as in an ODEFunction. For f(state, input, p, t), consider using add_policy_search.\n\n\n\n\n\n","category":"function"},{"location":"man/structure/","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"The condition that the Lyapunov function V(x) must be minimized uniquely at the fixed point x_0 is often represented as a requirement for V(x) to be positive away from the fixed point and zero at the fixed point. Put mathematically, it is sufficient to require V(x)  0  forall x ne x_0 and V(x_0) = 0. We call such functions positive definite (around the fixed point x_0).","category":"page"},{"location":"man/structure/","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"Two structures are provided which partially or fully enforce the minimization condition: NonnegativeNeuralLyapunov, which structurally enforces V(x) ge 0 everywhere, and PositiveSemiDefiniteStructure, which additionally enforces V(x_0) = 0.","category":"page"},{"location":"man/structure/#NeuralLyapunov.NonnegativeNeuralLyapunov","page":"Structuring a Neural Lyapunov function","title":"NeuralLyapunov.NonnegativeNeuralLyapunov","text":"NonnegativeNeuralLyapunov(network_dim; <keyword_arguments>)\n\nCreate a NeuralLyapunovStructure where the Lyapunov function is the L2 norm of the neural network output plus a constant δ times a function pos_def.\n\nCorresponds to V(x) = lVert ϕ(x) rVert^2 + δ  textttpos_def(x x_0), where ϕ is the neural network and x_0 is the equilibrium point.\n\nThis structure ensures V(x)  0   x when δ  0 and pos_def is always nonnegative. Further, if δ  0 and pos_def is strictly positive definite around fixed_point, the structure ensures that V(x) is strictly positive away from fixed_point. In such cases, the minimization condition reduces to ensuring V(x_0) = 0, and so DontCheckNonnegativity(true) should be used.\n\nArguments\n\nnetwork_dim: output dimensionality of the neural network.\n\nKeyword Arguments\n\nδ: weight of pos_def, as above; defaults to 0.\npos_def(state, fixed_point): a function that is positive (semi-)definite in state around fixed_point; defaults to log(1 + lVert x - x_0 rVert^2).\ngrad_pos_def(state, fixed_point): the gradient of pos_def with respect to state at state. If isnothing(grad_pos_def) (as is the default), the gradient of pos_def will be evaluated using grad.\ngrad: a function for evaluating gradients to be used when isnothing(grad_pos_def); defaults to, and expects the same arguments as, ForwardDiff.gradient.\n\nDynamics are assumed to be in f(state, p, t) form, as in an ODEFunction. For f(state, input, p, t), consider using add_policy_search.\n\nSee also: DontCheckNonnegativity\n\n\n\n\n\n","category":"function"},{"location":"man/structure/#NeuralLyapunov.PositiveSemiDefiniteStructure","page":"Structuring a Neural Lyapunov function","title":"NeuralLyapunov.PositiveSemiDefiniteStructure","text":"PositiveSemiDefiniteStructure(network_dim; <keyword_arguments>)\n\nCreate a NeuralLyapunovStructure where the Lyapunov function is the product of a positive (semi-)definite function pos_def which does not depend on the network and a nonnegative function non_neg which does depend the network.\n\nCorresponds to V(x) = textttpos_def(x x_0) * textttnon_neg(ϕ x x_0), where ϕ is the neural network and x_0 is the equilibrium point.\n\nThis structure ensures V(x)  0. Further, if pos_def is strictly positive definite fixed_point and non_neg is strictly positive (as is the case for the default values of pos_def and non_neg), then this structure ensures V(x) is strictly positive definite around fixed_point. In such cases, the minimization condition is satisfied structurally, so DontCheckNonnegativity(false) should be used.\n\nArguments\n\nnetwork_dim: output dimensionality of the neural network.\n\nKeyword Arguments\n\npos_def(state, fixed_point): a function that is positive (semi-)definite in state around fixed_point; defaults to log(1 + lVert x - x_0 rVert^2).\nnon_neg(net, state, fixed_point): a nonnegative function of the neural network; note that net is the neural network ϕ, and net(state) is the value of the neural network at a point ϕ(x); defaults to 1 + lVert ϕ(x) rVert^2.\ngrad_pos_def(state, fixed_point): the gradient of pos_def with respect to state at state. If isnothing(grad_pos_def) (as is the default), the gradient of pos_def will be evaluated using grad.\ngrad_non_neg(net, J_net, state, fixed_point): the gradient of non_neg with respect to state at state; J_net is a function outputting the Jacobian of net at the input. If isnothing(grad_non_neg) (as is the default), the gradient of non_neg will be evaluated using grad.\ngrad: a function for evaluating gradients to be used when isnothing(grad_pos_def) || isnothing(grad_non_neg); defaults to, and expects the same arguments as, ForwardDiff.gradient.\n\nDynamics are assumed to be in f(state, p, t) form, as in an ODEFunction. For f(state, input, p, t), consider using add_policy_search.\n\nSee also: DontCheckNonnegativity\n\n\n\n\n\n","category":"function"},{"location":"man/structure/#Defining-your-own-neural-Lyapunov-function-structure","page":"Structuring a Neural Lyapunov function","title":"Defining your own neural Lyapunov function structure","text":"","category":"section"},{"location":"man/structure/","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"To define a new structure for a neural Lyapunov function, one must specify the form of the Lyapunov candidate V and its time derivative along a trajectory dotV, as well as how to call the dynamics f. Additionally, the dimensionality of the output of the neural network must be known in advance.","category":"page"},{"location":"man/structure/#NeuralLyapunov.NeuralLyapunovStructure","page":"Structuring a Neural Lyapunov function","title":"NeuralLyapunov.NeuralLyapunovStructure","text":"NeuralLyapunovStructure(V, V̇, f_call, network_dim)\n\nSpecifies the structure of the neural Lyapunov function and its derivative.\n\nAllows the user to define the Lyapunov in terms of the neural network, potentially structurally enforcing some Lyapunov conditions.\n\nFields\n\nV(phi::Function, state, fixed_point): outputs the value of the Lyapunov function at state.\nV̇(phi::Function, J_phi::Function, dynamics::Function, state, params, t, fixed_point): outputs the time derivative of the Lyapunov function at state.\nf_call(dynamics::Function, phi::Function, state, params, t): outputs the derivative of the state; this is useful for making closed-loop dynamics which depend on the neural network, such as in the policy search case.\nnetwork_dim: the dimension of the output of the neural network.\n\nphi and J_phi above are both functions of state alone.\n\n\n\n\n\n","category":"type"},{"location":"man/structure/#Calling-the-dynamics","page":"Structuring a Neural Lyapunov function","title":"Calling the dynamics","text":"","category":"section"},{"location":"man/structure/","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"Very generally, the dynamical system can be a system of ODEs dotx = f(x u p t), where u is a control input, p contains parameters, and f depends on the neural network in some way. To capture this variety, users must supply the function f_call(dynamics::Function, phi::Function, state, p, t).","category":"page"},{"location":"man/structure/","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"The most common example is when dynamics takes the same form as an ODEFunction.  i.e., dotx = textttdynamics(x p t). In that case, f_call(dynamics, phi, state, p, t) = dynamics(state, p, t).","category":"page"},{"location":"man/structure/","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"Suppose instead, the dynamics were supplied as a function of state alone: dotx = textttdynamics(x). Then, f_call(dynamics, phi, state, p, t) = dynamics(state).","category":"page"},{"location":"man/structure/","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"Finally, suppose dotx = textttdynamics(x u p t) has a unidimensional control input that is being trained (via policy search) to be the second output of the neural network. Then f_call(dynamics, phi, state, p, t) = dynamics(state, phi(state)[2], p, t).","category":"page"},{"location":"man/structure/","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"Note that, despite the inclusion of the time variable t, NeuralLyapunov.jl currently only supports time-invariant systems, so only t = 0.0 is used.","category":"page"},{"location":"man/structure/#Structuring-V-and-\\dot{V}","page":"Structuring a Neural Lyapunov function","title":"Structuring V and dotV","text":"","category":"section"},{"location":"man/structure/","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"The Lyapunov candidate function V gets specified as a function V(phi, state, fixed_point), where phi is the neural network as a function phi(state). Note that this form allows V(x) to depend on the neural network evaluated at points other than just the input x.","category":"page"},{"location":"man/structure/","page":"Structuring a Neural Lyapunov function","title":"Structuring a Neural Lyapunov function","text":"The time derivative dotV is similarly defined by a function V̇(phi, J_phi, dynamics, state, params, t, fixed_point). The function J_phi(state) gives the Jacobian of the neural network phi at state. The function dynamics is as above (with parameters params). ","category":"page"},{"location":"man/benchmarking/#Benchmarking-neural-Lyapunov-methods","page":"Benchmarking neural Lyapunov methods","title":"Benchmarking neural Lyapunov methods","text":"","category":"section"},{"location":"man/benchmarking/","page":"Benchmarking neural Lyapunov methods","title":"Benchmarking neural Lyapunov methods","text":"To facilitate comparison of different neural Lyapunov specifications, optimizers, hyperparameters, etc., we provide the benchmark function.","category":"page"},{"location":"man/benchmarking/","page":"Benchmarking neural Lyapunov methods","title":"Benchmarking neural Lyapunov methods","text":"Through its arguments, users may specify how a neural Lyapunov problem, the neural network structure, the physics-informed neural network discretization strategy, and the optimization strategy used to solve the problem. After solving the problem in the specified manner, the dynamical system is simulated (users can specify an ODE solver in the arguments, as well) and classification by the neural Lyapunov function is compared to the simulation results. The benchmark function returns a confusion matrix for the resultant neural Lyapunov classifier, the training time, and samples with labels, so that users can compare accuracy and computation speed of various methods.","category":"page"},{"location":"man/benchmarking/#NeuralLyapunov.benchmark","page":"Benchmarking neural Lyapunov methods","title":"NeuralLyapunov.benchmark","text":"benchmark(dynamics, bounds, spec, chain, strategy, opt; <keyword_arguments>)\nbenchmark(dynamics, lb, ub, spec, chain, strategy, opt; <keyword_arguments>)\n\nEvaluate the specified neural Lyapunov method on the given system. Return a NamedTuple containing the confusion matrix, optimization time, and other metrics listed below.\n\nTrain a neural Lyapunov function as specified, then discretize the domain using a grid discretization and use the neural Lyapnov function to and the provided classifier to predict whether grid points are in the region of attraction of the provided fixed_point. Finally, simulate the system from each grid point and check if the trajectories reach the fixed point. Return a confusion matrix for the neural Lyapunov classifier using the results of the simulated trajectories as ground truth. Additionally return the time it took for the optimization to run.\n\nTo use multiple solvers, users should supply a vector of optimizers in opt. The first optimizer will be used, then the problem will be remade with the result of the first optimization as the initial guess. Then, the second optimizer will be used, and so on. Supplying a vector of Pairs in optimization_args will use the same arguments for each optimization pass, and supplying a vector of such vectors will use potentially different arguments for each optimization pass.\n\nPositional Arguments\n\ndynamics: the dynamical system being analyzed, represented as an ODESystem or the function f such that ẋ = f(x[, u], p, t); either way, the ODE should not depend on time and only t = 0.0 will be used. (For an example of when f would have a u argument, see add_policy_search.)\nbounds: an array of domains, defining the training domain by bounding the states (and derivatives, when applicable) of dynamics; only used when dynamics isa ODESystem, otherwise use lb and ub.\nlb and ub: the training domain will be lb_1 ub_1lb_2 ub_2; not used when dynamics isa ODESystem, then use bounds.\nspec: a NeuralLyapunovSpecification defining the Lyapunov function structure, as well as the minimization and decrease conditions.\nchain: a vector of Lux/Flux chains with a d-dimensional input and a 1-dimensional output corresponding to each of the dependent variables, where d is the length of bounds or lb and ub. Note that this specification respects the order of the dependent variables as specified in the PDESystem. Flux chains will be converted to Lux internally by NeuralPDE using NeuralPDE.adapt(FromFluxAdaptor(false, false), chain).\nstrategy: determines which training strategy will be used. See the NeuralPDE Training Strategy documentation for more details.\nopt: optimizer to use in training the neural Lyapunov function.\n\nKeyword Arguments\n\nn: number of samples used for evaluating the neural Lyapunov classifier.\nsample_alg: sampling algorithm used for generating the evaluation data; defaults to LatinHypercubeSample(rng); see the QuasiMonteCarlo.jl docs for more information.\nclassifier: function of V(x), V(x), and x that predicts whether x is in the region of attraction; when constructing the confusion matrix, a point is predicted to be in the region of attraction if classifier or endpoint_check returns true; defaults to (V, V̇, x) -> V̇ < 0.\nfixed_point: the equilibrium being analyzed; defaults to the origin.\np: the values of the parameters of the dynamical system being analyzed; defaults to SciMLBase.NullParameters(); not used when dynamics isa ODESystem, then use the default parameter values of dynamics.\nstate_syms: an array of the Symbol representing each state; not used when dynamics isa ODESystem, then the symbols from dynamics are used; if dynamics isa ODEFunction, symbols stored there are used, unless overridden here; if not provided here and cannot be inferred, [:state1, :state2, ...] will be used.\nparameter_syms: an array of the Symbol representing each parameter; not used when dynamics isa ODESystem, then the symbols from dynamics are used; if dynamics isa ODEFunction, symbols stored there are used, unless overridden here; if not provided here and cannot be inferred, [:param1, :param2, ...] will be used.\npolicy_search::Bool: whether or not to include a loss term enforcing fixed_point to actually be a fixed point; defaults to false; only used when dynamics isa Function && !(dynamics isa ODEFunction); when dynamics isa ODEFunction, policy_search should not be supplied (as it must be false); when dynamics isa ODESystem, value inferred by the presence of unbound inputs.\noptimization_args: arguments to be passed into the optimization solver, as a vector of Pairs. For more information, see the Optimization.jl docs.\nsimulation_time: simulation end time for checking if trajectory from a point reaches equilibrium\node_solver: differential equation solver used in simulating the system for evaluation. For more information, see the DifferentialEquations.jl docs.\node_solver_args: arguments to be passed into the differential equation solver. For more information, see the DifferentialEquations.jl docs.\nendpoint_check: function of the endpoint of a simulation that returns true when the endpoint is approximately the fixed point and false otherwise; defaults to (x) -> ≈(x, fixed_point; atol=atol).\natol: absolute tolerance used in the default value for endpoint_check.\ninit_params: initial parameters for the neural network; defaults to nothing, in which case the initial parameters are generated using Lux.initialparameters and rng.\nrng: random number generator used to generate initial parameters; defaults to a StableRNG with seed 0.\n\nOutput Fields\n\nconfusion_matrix: confusion matrix of the neural Lyapunov classifier.\ntraining_time: time taken to train the neural Lyapunov function.\nV: the neural Lyapunov function.\nV̇: the Lyapunov decrease function.\nstates: evaluation samples matrix (each column is a sample).\nendpoints: endpoints of the simulations.\nactual: result of endpoint_check applied to endpoints.\npredicted: result of classifier applied to states.\nV_samples: V evaluated at states.\nV̇_samples: V̇ evaluated at states.\n\n\n\n\n\n","category":"function"},{"location":"man/roa/#Training-for-Region-of-Attraction-Identification","page":"Training for Region of Attraction Identification","title":"Training for Region of Attraction Identification","text":"","category":"section"},{"location":"man/roa/","page":"Training for Region of Attraction Identification","title":"Training for Region of Attraction Identification","text":"Satisfying the minimization and decrease conditions within the training region (or any region around the fixed point, however small) is sufficient for proving local stability. In many cases, however, we desire an estimate of the region of attraction, rather than simply a guarantee of local stability.","category":"page"},{"location":"man/roa/","page":"Training for Region of Attraction Identification","title":"Training for Region of Attraction Identification","text":"Any compact sublevel set wherein the minimization and decrease conditions are satisfied is an inner estimate of the region of attraction. Therefore, we can restrict training for those conditions to only within a predetermined sublevel set  x  V(x) le rho . To do so, define a LyapunovDecreaseCondition as usual and then pass it through the make_RoA_aware function, which returns an analogous RoAAwareDecreaseCondition.","category":"page"},{"location":"man/roa/#NeuralLyapunov.make_RoA_aware","page":"Training for Region of Attraction Identification","title":"NeuralLyapunov.make_RoA_aware","text":"make_RoA_aware(cond; ρ, out_of_RoA_penalty, sigmoid)\n\nAdd awareness of the region of attraction (RoA) estimation task to the supplied AbstractLyapunovDecreaseCondition.\n\nWhen estimating the region of attraction using a Lyapunov function, the decrease condition only needs to be met within a bounded sublevel set  x  V(x)  ρ . The returned RoAAwareDecreaseCondition enforces the decrease condition represented by cond only in that sublevel set.\n\nArguments\n\ncond::AbstractLyapunovDecreaseCondition: specifies the loss to be applied when\n\nV(x)  ρ.\n\nKeyword Arguments\n\nρ: the target level such that the RoA will be  x  V(x)  ρ , defaults to 1.\nout_of_RoA_penalty::Function: specifies the loss to be applied when V(x)  ρ, defaults to no loss.\nsigmoid::Function: approximately one when the input is positive and approximately zero when the input is negative, defaults to unit step function.\n\nThe loss applied to samples x such that V(x)  ρ is lvert textttout_of_RoA_penalty(V(x) V(x) x x_0 ρ) rvert^2.\n\nThe sigmoid function allows for a smooth transition between the V(x)  ρ case and the V(x)  ρ case, by combining the above equations into one:\n\ntextttsigmoid(ρ - V(x)) (textin-RoA expression) + textttsigmoid(V(x) - ρ) (textout-of-RoA expression) = 0.\n\nNote that a hard transition, which only enforces the in-RoA equation when V(x)  ρ and the out-of-RoA equation when V(x)  ρ can be provided by a sigmoid which is exactly one when the input is nonnegative and exactly zero when the input is negative. As such, the default value is sigmoid(t) = t ≥ zero(t).\n\nSee also: RoAAwareDecreaseCondition\n\n\n\n\n\n","category":"function"},{"location":"man/roa/#NeuralLyapunov.RoAAwareDecreaseCondition","page":"Training for Region of Attraction Identification","title":"NeuralLyapunov.RoAAwareDecreaseCondition","text":"RoAAwareDecreaseCondition(cond, sigmoid, ρ, out_of_RoA_penalty)\n\nSpecifies the form of the Lyapunov decrease condition to be used, training for a region of attraction estimate of  x  V(x)  ρ .\n\nFields\n\ncond::AbstractLyapunovDecreaseCondition: specifies the loss to be applied when V(x)  ρ.\nsigmoid::Function: approximately one when the input is positive and approximately zero when the input is negative.\nρ: the level of the sublevel set forming the estimate of the region of attraction.\nout_of_RoA_penalty::Function: a loss function to be applied penalizing points outside the sublevel set forming the region of attraction estimate.\n\nTraining conditions\n\nInside the region of attraction (i.e., when V(x)  ρ), the loss function specified by cond will be applied. Outside the region of attraction (i.e., when V(x)  ρ), the loss function specified by out_of_RoA_penalty will be applied.\n\nThe sigmoid function allows for a smooth transition between the V(x)  ρ case and the V(x)  ρ case, by combining the different losses into one:\n\ntextttsigmoid(ρ - V(x)) (textin-RoA expression) + textttsigmoid(V(x) - ρ) (textout-of-RoA expression) = 0.\n\nNote that a hard transition, which only enforces the in-RoA equation when V(x)  ρ and the out-of-RoA equation when V(x)  ρ can be provided by a sigmoid which is exactly one when the input is nonnegative and exactly zero when the input is negative.\n\nIf the dynamics truly have a fixed point at x_0 and V(x) is truly the rate of decrease of V(x) along the dynamics, then V(x_0) will be 0 and there is no need to train for V(x_0) = 0.\n\nExamples:\n\nAsymptotic decrease can be enforced by requiring     V(x)  -C lVert x - x_0 rVert^2, for some positive C, which can be represented by a LyapunovDecreaseCondition with\n\nrate_metric = (V, dVdt) -> dVdt\nstrength = (x, x0) -> C * (x - x0) ⋅ (x - x0)\n\nExponential decrease of rate k is proven by     V(x)  - k V(x), which can be represented by a LyapunovDecreaseCondition with\n\nrate_metric = (V, dVdt) -> dVdt + k * V\nstrength = (x, x0) -> 0.0\n\nEnforcing either condition only in the region of attraction and not penalizing any points outside that region would correspond to\n\nout_of_RoA_penalty = (V, dVdt, state, fixed_point, ρ) -> 0.0\n\nwhereas an example of a penalty that decays farther in state space from the fixed point is\n\nout_of_RoA_penalty = (V, dVdt, x, x0, ρ) -> 1.0 / ((x - x0) ⋅ (x - x0))\n\nNote that this penalty could also depend on values of V and V at various points, as well as ρ.\n\nIn any of these cases, the rectified linear unit rectifier = (t) -> max(zero(t), t) exactly represents the inequality, but differentiable approximations of this function may be employed.\n\nSee also: AbstractLyapunovDecreaseCondition, LyapunovDecreaseCondition\n\n\n\n\n\n","category":"type"},{"location":"demos/damped_SHO/#Damped-Simple-Harmonic-Oscillator","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"","category":"section"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"Let's train a neural network to prove the exponential stability of the damped simple harmonic oscillator (SHO).","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"The damped SHO is represented by the system of differential equations","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"beginalign*\n    fracdxdt = v \n    fracdvdt = -2 zeta omega_0 v - omega_0^2 x\nendalign*","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"where x is the position, v is the velocity, t is time, and zeta omega_0 are parameters.","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"We'll consider just the box domain x in -5 5 v in -2 2.","category":"page"},{"location":"demos/damped_SHO/#Copy-Pastable-Code","page":"Damped Simple Harmonic Oscillator","title":"Copy-Pastable Code","text":"","category":"section"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"using NeuralPDE, Lux, NeuralLyapunov\nusing Optimization, OptimizationOptimisers, OptimizationOptimJL\nusing StableRNGs, Random\n\nrng = StableRNG(0)\nRandom.seed!(200)\n\n######################### Define dynamics and domain ##########################\n\n\"Simple Harmonic Oscillator Dynamics\"\nfunction f(state, p, t)\n    ζ, ω_0 = p\n    pos = state[1]\n    vel = state[2]\n    vcat(vel, -2ζ * ω_0 * vel - ω_0^2 * pos)\nend\nlb = [-5.0, -2.0];\nub = [ 5.0,  2.0];\np = [0.5, 1.0];\nfixed_point = [0.0, 0.0];\ndynamics = ODEFunction(f; sys = SciMLBase.SymbolCache([:x, :v], [:ζ, :ω_0]))\n\n####################### Specify neural Lyapunov problem #######################\n\n# Define neural network discretization\ndim_state = length(lb)\ndim_hidden = 10\ndim_output = 4\nchain = [Lux.Chain(\n             Dense(dim_state, dim_hidden, tanh),\n             Dense(dim_hidden, dim_hidden, tanh),\n             Dense(dim_hidden, 1)\n         ) for _ in 1:dim_output]\nps = Lux.initialparameters(rng, chain)\n\n# Define training strategy\nstrategy = QuasiRandomTraining(1000)\ndiscretization = PhysicsInformedNN(chain, strategy; init_params = ps)\n\n# Define neural Lyapunov structure\nstructure = NonnegativeNeuralLyapunov(\n    dim_output;\n    δ = 1e-6\n)\nminimization_condition = DontCheckNonnegativity(check_fixed_point = true)\n\n# Define Lyapunov decrease condition\n# Damped SHO has exponential stability at a rate of k = ζ * ω_0, so we train to certify that\ndecrease_condition = ExponentialStability(prod(p))\n\n# Construct neural Lyapunov specification\nspec = NeuralLyapunovSpecification(\n    structure,\n    minimization_condition,\n    decrease_condition\n)\n\n############################# Construct PDESystem #############################\n\n@named pde_system = NeuralLyapunovPDESystem(\n    dynamics,\n    lb,\n    ub,\n    spec;\n    p = p\n)\n\n######################## Construct OptimizationProblem ########################\n\nprob = discretize(pde_system, discretization)\n\n########################## Solve OptimizationProblem ##########################\n\nres = Optimization.solve(prob, OptimizationOptimisers.Adam(); maxiters = 500)\n\n###################### Get numerical numerical functions ######################\nnet = discretization.phi\nθ = res.u.depvar\n\nV, V̇ = get_numerical_lyapunov_function(\n    net,\n    θ,\n    structure,\n    f,\n    fixed_point;\n    p = p\n)","category":"page"},{"location":"demos/damped_SHO/#Detailed-description","page":"Damped Simple Harmonic Oscillator","title":"Detailed description","text":"","category":"section"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"In this example, we set the dynamics up as an ODEFunction and use a SciMLBase.SymbolCache to tell the ultimate PDESystem what to call our state and parameter variables.","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"using SciMLBase # for ODEFunction and SciMLBase.SymbolCache\n\n\"Simple Harmonic Oscillator Dynamics\"\nfunction f(state, p, t)\n    ζ, ω_0 = p\n    pos = state[1]\n    vel = state[2]\n    vcat(vel, -2ζ * ω_0 * vel - ω_0^2 * pos)\nend\nlb = [-5.0, -2.0];\nub = [ 5.0,  2.0];\np = [0.5, 1.0];\nfixed_point = [0.0, 0.0];\ndynamics = ODEFunction(f; sys = SciMLBase.SymbolCache([:x, :v], [:ζ, :ω_0]))\nnothing # hide","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"Setting up the neural network using Lux and NeuralPDE training strategy is no different from any other physics-informed neural network problem. For more on that aspect, see the NeuralPDE documentation.","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"using Lux, StableRNGs\n\n# Stable random number generator for doc stability\nrng = StableRNG(0)\n\n# Define neural network discretization\ndim_state = length(lb)\ndim_hidden = 10\ndim_output = 3\nchain = [Lux.Chain(\n             Dense(dim_state, dim_hidden, tanh),\n             Dense(dim_hidden, dim_hidden, tanh),\n             Dense(dim_hidden, 1)\n         ) for _ in 1:dim_output]\nps = Lux.initialparameters(rng, chain)","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"using NeuralPDE\n\n# Define training strategy\nstrategy = QuasiRandomTraining(1000)\ndiscretization = PhysicsInformedNN(chain, strategy; init_params = ps)\nnothing # hide","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"We now define our Lyapunov candidate structure along with the form of the Lyapunov conditions we'll be using.","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"For this example, let's use a Lyapunov candidate","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"V(x) = lVert phi(x) rVert^2 + delta log left( 1 + lVert x rVert^2 right)","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"which structurally enforces nonnegativity, but doesn't guarantee V(0 0) = 0. We therefore don't need a term in the loss function enforcing V(x)  0  forall x ne 0, but we do need something enforcing V(0 0) = 0. So, we use DontCheckNonnegativity(check_fixed_point = true).","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"To train for exponential stability we use ExponentialStability, but we must specify the rate of exponential decrease, which we know in this case to be zeta omega_0.","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"using NeuralLyapunov\n\n# Define neural Lyapunov structure\nstructure = NonnegativeNeuralLyapunov(\n    dim_output;\n    δ = 1e-6\n)\nminimization_condition = DontCheckNonnegativity(check_fixed_point = true)\n\n# Define Lyapunov decrease condition\n# Damped SHO has exponential stability at a rate of k = ζ * ω_0, so we train to certify that\ndecrease_condition = ExponentialStability(prod(p))\n\n# Construct neural Lyapunov specification\nspec = NeuralLyapunovSpecification(\n    structure,\n    minimization_condition,\n    decrease_condition\n)\n\n# Construct PDESystem\n@named pde_system = NeuralLyapunovPDESystem(\n    dynamics,\n    lb,\n    ub,\n    spec;\n    p = p\n)","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"Now, we solve the PDESystem using NeuralPDE the same way we would any PINN problem.","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"prob = discretize(pde_system, discretization)\n\nusing Optimization, OptimizationOptimisers, OptimizationOptimJL\n\nres = Optimization.solve(prob, OptimizationOptimisers.Adam(); maxiters = 500)\n\nnet = discretization.phi\nθ = res.u.depvar","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"We can use the result of the optimization problem to build the Lyapunov candidate as a Julia function.","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"V, V̇ = get_numerical_lyapunov_function(\n    net,\n    θ,\n    structure,\n    f,\n    fixed_point;\n    p = p\n)\nnothing # hide","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"Now let's see how we did. We'll evaluate both V and dotV on a 101 times 101 grid:","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"Δx = (ub[1] - lb[1]) / 100\nΔv = (ub[2] - lb[2]) / 100\nxs = lb[1]:Δx:ub[1]\nvs = lb[2]:Δv:ub[2]\nstates = Iterators.map(collect, Iterators.product(xs, vs))\nV_samples = vec(V(hcat(states...)))\nV̇_samples = vec(V̇(hcat(states...)))\n\n# Print statistics\nV_min, i_min = findmin(V_samples)\nstate_min = collect(states)[i_min]\nV_min, state_min = if V(fixed_point) ≤ V_min\n        V(fixed_point), fixed_point\n    else\n        V_min, state_min\n    end\n\nprintln(\"V(0.,0.) = \", V(fixed_point))\nprintln(\"V ∋ [\", V_min, \", \", maximum(V_samples), \"]\")\nprintln(\"Minimal sample of V is at \", state_min)\nprintln(\n    \"V̇ ∋ [\",\n    minimum(V̇_samples),\n    \", \",\n    max(V̇(fixed_point), maximum(V̇_samples)),\n    \"]\",\n)","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"At least at these validation samples, the conditions that dotV be negative semi-definite and V be minimized at the origin are nearly satisfied.","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"using Plots\n\np1 = plot(xs, vs, V_samples, linetype = :contourf, title = \"V\", xlabel = \"x\", ylabel = \"v\");\np1 = scatter!([0], [0], label = \"Equilibrium\");\np2 = plot(\n    xs,\n    vs,\n    V̇_samples,\n    linetype = :contourf,\n    title = \"V̇\",\n    xlabel = \"x\",\n    ylabel = \"v\",\n);\np2 = scatter!([0], [0], label = \"Equilibrium\");\nplot(p1, p2)","category":"page"},{"location":"demos/damped_SHO/","page":"Damped Simple Harmonic Oscillator","title":"Damped Simple Harmonic Oscillator","text":"Each sublevel set of V completely contained in the plot above has been verified as a subset of the region of attraction.","category":"page"},{"location":"lib/quadrotor/#Quadrotor-Models","page":"Quadrotor Models","title":"Quadrotor Models","text":"","category":"section"},{"location":"lib/quadrotor/","page":"Quadrotor Models","title":"Quadrotor Models","text":"Two versions of the quadrotor are provided: a planar approximation (QuadrotorPlanar) and a 3D model (Quadrotor3D). Additionally, when also using the Plots.jl package, the convenience plotting functions plot_quadrotor_planar and plot_quadrotor_3d are provided.","category":"page"},{"location":"lib/quadrotor/","page":"Quadrotor Models","title":"Quadrotor Models","text":"Pages = [\"quadrotor.md\"]\nDepth = 2:3","category":"page"},{"location":"lib/quadrotor/#Planar-Approximation","page":"Quadrotor Models","title":"Planar Approximation","text":"","category":"section"},{"location":"lib/quadrotor/","page":"Quadrotor Models","title":"Quadrotor Models","text":"The planar quadrotor (QuadrotorPlanar, technically a birotor) is a rigid body with two rotors in line with the center of mass.","category":"page"},{"location":"lib/quadrotor/","page":"Quadrotor Models","title":"Quadrotor Models","text":"(Image: Planar quadrotor animation)","category":"page"},{"location":"lib/quadrotor/#NeuralLyapunovProblemLibrary.QuadrotorPlanar","page":"Quadrotor Models","title":"NeuralLyapunovProblemLibrary.QuadrotorPlanar","text":"QuadrotorPlanar(; name, defaults)\n\nCreate an ODESystem representing a planar approximation of the quadrotor (technically a birotor).\n\nThis birotor is a rigid body with two rotors in line with the center of mass. The location of the center of mass is determined by x and y. Its orientation is determined by θ, measured counter-clockwise from the x-axis. The thrust from the right rotor (on the positive x-axis when θ = 0) is the input u1. The thrust from the other rotor is u2. Note that these thrusts should be nonnegative and if a negative input is provided, the model replaces it with 0.\n\nThe equations governing the planar quadrotor are:\n\nbeginalign\n    mx = -(u_1 + u_2)sin(θ) \n    my = (u_1 + u_2)cos(θ) - mg \n    I_quad ddotθ = r (u_1 - u_2)\nendalign\n\nThe name of the ODESystem is name.\n\nODESystem Parameters\n\nm: mass of the quadrotor.\nI_quad: moment of inertia of the quadrotor around its center of mass.\ng: gravitational acceleration in the direction of the negative y-axis (defaults to 9.81).\nr: distance from center of mass to each rotor.\n\nUsers may optionally provide default values of the parameters through defaults: a vector of the default values for [m, I_quad, g, r].\n\n\n\n\n\n","category":"function"},{"location":"lib/quadrotor/#Copy-Pastable-Code","page":"Quadrotor Models","title":"Copy-Pastable Code","text":"","category":"section"},{"location":"lib/quadrotor/","page":"Quadrotor Models","title":"Quadrotor Models","text":"using Random; Random.seed!(200) # hide\nusing ModelingToolkit\nusing NeuralLyapunovProblemLibrary\nusing OrdinaryDiffEq\nusing Plots\nusing LinearAlgebra\nusing ControlSystemsBase: lqr, Continuous\n\n@named quadrotor_planar = QuadrotorPlanar()\n\nfunction π_lqr(p; x_eq = zeros(6), Q = I(6), R = I(2))\n    m, I_quad, g, r = p\n\n    # Assumes linearization around a fixed point\n    # x_eq = (x*, y*, 0, 0, 0, 0), u_eq = (mg / 2, mg / 2)\n    A_lin = zeros(6, 6)\n    A_lin[1:3,4:6] .= I(3)\n    A_lin[4,3] = -g\n\n    B_lin = zeros(6, 2)\n    B_lin[5,:] .= 1 / m\n    B_lin[6,:] .= r / I_quad, -r / I_quad\n\n    K = lqr(Continuous, A_lin, B_lin, Q, R)\n\n    T0 = m * g / 2\n    return (x) -> -K * (x - x_eq) + [T0, T0]\nend\n\nt, = independent_variables(quadrotor_planar)\nDt = Differential(t)\n\nparams = [quadrotor_planar.m, quadrotor_planar.I_quad, quadrotor_planar.g, quadrotor_planar.r]\nq = [quadrotor_planar.x, quadrotor_planar.y, quadrotor_planar.θ]\nu = [quadrotor_planar.u1, quadrotor_planar.u2]\nx = vcat(q, Dt.(q))\n\n# Assume rotors are negligible mass when calculating the moment of inertia\nm, r = ones(2)\ng = 1.0\nI_quad = m * r^2 / 12\np = [m, I_quad, g, r]\n\n# Create controller system and combine with quadrotor_planar, then simplify\n@named lqr_controller = ODESystem(\n    u .~ π_lqr(p)(x),\n    t,\n    vcat(x, u),\n    params\n)\n@named quadrotor_planar_lqr = compose(lqr_controller, quadrotor_planar)\nquadrotor_planar_lqr = structural_simplify(quadrotor_planar_lqr)\n\n# Random initialization\n# structural_simplify sometimes rearranges variables, so we use a Dict to provide the\n# initialization and parameters when constructing the ODEProblem\nx0 = Dict(x .=> 2 * rand(6) .- 1)\np = Dict(params .=> [m, I_quad, g, r])\nτ = sqrt(r / g)\n\nprob = ODEProblem(quadrotor_planar_lqr, x0, 15τ, p)\nsol = solve(prob, Tsit5())\n\ngif(\n    plot_quadrotor_planar(\n        sol,\n        [m, I_quad, g, r];\n        x_symbol=q[1],\n        y_symbol=q[2],\n        θ_symbol=q[3],\n        u1_symbol=u[1],\n        u2_symbol=u[2]\n    );\n    fps = 50\n)","category":"page"},{"location":"lib/quadrotor/#Plotting-the-Planar-Quadrotor","page":"Quadrotor Models","title":"Plotting the Planar Quadrotor","text":"","category":"section"},{"location":"lib/quadrotor/#NeuralLyapunovProblemLibrary.plot_quadrotor_planar","page":"Quadrotor Models","title":"NeuralLyapunovProblemLibrary.plot_quadrotor_planar","text":"plot_quadrotor_planar(x, y, θ, [u1, u2,] p, t; title)\nplot_quadrotor_planar(sol, p; title, N, x_symbol, y_symbol, θ_symbol)\n\nPlot the planar quadrotor's trajectory.\n\nWhen thrusts are supplied, the arrows scale with thrust, otherwise the arrows are of constant length.\n\nArguments\n\nx: The x-coordinate of the quadrotor at each time step.\ny: The y-coordinate of the quadrotor at each time step.\nθ: The angle of the quadrotor at each time step.\nu1: The thrust of the first rotor at each time step.\nu2: The thrust of the second rotor at each time step.\nt: The time steps.\nsol: The solution to the ODE problem.\np: The parameters of the quadrotor.\n\nKeyword arguments\n\ntitle: The title of the plot; defaults to no title (i.e., title=\"\").\nN: The number of points to plot; when using x, y, θ, and t, uses length(t); defaults to 500 when using sol.\nx_symbol: The symbol of the x-coordinate in sol; defaults to :x.\ny_symbol: The symbol of the y-coordinate in sol; defaults to :y.\nθ_symbol: The symbol of the angle in sol; defaults to :θ.\nu1_symbol: The symbol of the thrust of the first rotor in sol; defaults to :u1.\nu2_symbol: The symbol of the thrust of the second rotor in sol; defaults to :u2.\n\n\n\n\n\n","category":"function"},{"location":"lib/quadrotor/#3D-Model","page":"Quadrotor Models","title":"3D Model","text":"","category":"section"},{"location":"lib/quadrotor/","page":"Quadrotor Models","title":"Quadrotor Models","text":"A full 3D model from (Mellinger and Kumar, 2011) is provided via Quadrotor3D.","category":"page"},{"location":"lib/quadrotor/","page":"Quadrotor Models","title":"Quadrotor Models","text":"(Image: 3D quadrotor animation)","category":"page"},{"location":"lib/quadrotor/#NeuralLyapunovProblemLibrary.Quadrotor3D","page":"Quadrotor Models","title":"NeuralLyapunovProblemLibrary.Quadrotor3D","text":"Quadrotor3D(; name, defaults)\n\nCreate an ODESystem representing a quadrotor in 3D space.\n\nThe quadrotor is a rigid body in an X-shape (90°-angles between the rotors). The equations governing the quadrotor can be found in (Mellinger and Kumar, 2011).\n\nODESystem State Variables\n\nx: x-position (world frame).\ny: y-position (world frame).\nz: z-position (world frame).\nφ: roll around body x-axis (Z-X-Y Euler angles).\nθ: pitch around body y-axis (Z-X-Y Euler angles).\nψ: yaw around body z-axis (Z-X-Y Euler angles).\nvx: x-velocity (world frame).\nvy: y-velocity (world frame).\nvz: z-velocity (world frame).\nωφ: roll angular velocity (world frame).\nωθ: pitch angular velocity (world frame).\nωψ: yaw angular velocity (world frame).\n\nODESystem Input Variables\n\nT: thrust (should be nonnegative).\nτφ: roll torque.\nτθ: pitch torque.\nτψ: yaw torque.\n\nNot only should the aggregate thrust be nonnegative, but the torques should have been generated from nonnegative individual rotor thrusts. The model calculates individual rotor thrusts and replaces any negative values with 0.\n\nODESystem Parameters\n\nm: mass of the quadrotor.\ng: gravitational acceleration in the direction of the negative z-axis (defaults to 9.81).\nIxx,Ixy,Ixz,Iyy,Iyz,Izz: components of the moment of inertia matrix of the quadrotor around its center of mass:\nI = beginpmatrix\n        I_xx  I_xy  I_xz \n        I_xy  I_yy  I_yz \n        I_xz  I_yz  I_zz\n    endpmatrix\n\nUsers may optionally provide default values of the parameters through defaults: a vector of the default values for [m, g, Ixx, Ixy, Ixz, Iyy, Iyz, Izz].\n\n\n\n\n\n","category":"function"},{"location":"lib/quadrotor/#Copy-Pastable-Code-2","page":"Quadrotor Models","title":"Copy-Pastable Code","text":"","category":"section"},{"location":"lib/quadrotor/","page":"Quadrotor Models","title":"Quadrotor Models","text":"using Random; Random.seed!(200) # hide\nusing ModelingToolkit\nimport ModelingToolkit: inputs\nusing NeuralLyapunovProblemLibrary\nusing OrdinaryDiffEq\nusing Plots\nusing LinearAlgebra\nusing ControlSystemsBase: lqr, Continuous\n\n# Define LQR controller\nfunction π_lqr(p; x_eq = zeros(12), u_eq = [p[1]*p[2], 0, 0, 0], Q = I(12), R = I(4))\n    @named quadrotor_3d = Quadrotor3D()\n\n    # Use equilibrium as linearization point\n    u = inputs(quadrotor_3d)\n    x = setdiff(unknowns(quadrotor_3d), u)\n    params = parameters(quadrotor_3d)\n    op = Dict(vcat(x .=> x_eq, u .=> u_eq, params .=> p))\n\n    # Linearize with ModelingToolkit\n    mats, sys = linearize(quadrotor_3d, u, x; op)\n\n    # Sometimes linearization will reorder the variables, but we can undo that with\n    # permutation matrices Px : x_new = Px * x and Pu : u_new = Pu * u\n    x_new = unknowns(sys)\n    u_new = inputs(sys)\n\n    Px = (x_new .- x') .=== 0\n    Pu = (u_new .- u') .=== 0\n\n    A_lin = Px' * mats[:A] * Px\n    B_lin = Px' * mats[:B] * Pu\n\n    K = lqr(Continuous, A_lin, B_lin, Q, R)\n    return (x) -> -K * (x - x_eq) + u_eq\nend\n\n@named quadrotor_3d = Quadrotor3D()\n\n# Set up variable symbols\nt, = independent_variables(quadrotor_3d)\nDt = Differential(t)\n\nx = [\n    quadrotor_3d.x,\n    quadrotor_3d.y,\n    quadrotor_3d.z,\n    quadrotor_3d.φ,\n    quadrotor_3d.θ,\n    quadrotor_3d.ψ,\n    quadrotor_3d.vx,\n    quadrotor_3d.vy,\n    quadrotor_3d.vz,\n    quadrotor_3d.ωφ,\n    quadrotor_3d.ωθ,\n    quadrotor_3d.ωψ\n]\n\nu = [\n    quadrotor_3d.T,\n    quadrotor_3d.τφ,\n    quadrotor_3d.τθ,\n    quadrotor_3d.τψ\n]\n\nparams = [\n    quadrotor_3d.m,\n    quadrotor_3d.g,\n    quadrotor_3d.Ixx,\n    quadrotor_3d.Ixy,\n    quadrotor_3d.Ixz,\n    quadrotor_3d.Iyy,\n    quadrotor_3d.Iyz,\n    quadrotor_3d.Izz\n]\n\n# Assume rotors are negligible mass when calculating the moment of inertia\nm, L = ones(2)\ng = 1.0\nIxx = Iyy = m * L^2 / 6\nIzz = m * L^2 / 3\nIxy = Ixz = Iyz = 0.0\np = [m, g, Ixx, Ixy, Ixz, Iyy, Iyz, Izz]\n\n# Create controller system and combine with quadrotor_3d, then simplify\n@named lqr_controller = ODESystem(\n    u .~ π_lqr(p)(x),\n    t,\n    vcat(x, u),\n    params\n)\n\n@named quadrotor_3d_lqr = compose(lqr_controller, quadrotor_3d)\nquadrotor_3d_lqr = structural_simplify(quadrotor_3d_lqr)\n\n# Random initialization\n# structural_simplify sometimes rearranges variables, so we use a Dict to provide the\n# initialization and parameters when constructing the ODEProblemp = Dict(params .=> p)\nδ = 0.5\nx0 = Dict(x .=> δ .* (2 .* rand(12) .- 1))\nτ = sqrt(L / g)\np = Dict(params .=> [m, g, Ixx, Ixy, Ixz, Iyy, Iyz, Izz])\n\nprob = ODEProblem(quadrotor_3d_lqr, x0, 15τ, p)\nsol = solve(prob, Tsit5())\n\ngif(\n    plot_quadrotor_3d(\n        sol,\n        [m, g, Ixx, Ixy, Ixz, Iyy, Iyz, Izz];\n        x_symbol=x[1],\n        y_symbol=x[2],\n        z_symbol=x[3],\n        φ_symbol=x[4],\n        θ_symbol=x[5],\n        ψ_symbol=x[6],\n        T_symbol=u[1],\n        τφ_symbol=u[2],\n        τθ_symbol=u[3],\n        τψ_symbol=u[4]\n    );\n    fps=50\n)","category":"page"},{"location":"lib/quadrotor/#Plotting-the-3D-Quadrotor","page":"Quadrotor Models","title":"Plotting the 3D Quadrotor","text":"","category":"section"},{"location":"lib/quadrotor/#NeuralLyapunovProblemLibrary.plot_quadrotor_3d","page":"Quadrotor Models","title":"NeuralLyapunovProblemLibrary.plot_quadrotor_3d","text":"plot_quadrotor_3d(x, y, z, φ, θ, ψ, [T, τφ, τθ, τψ,] p, t; title)\nplot_quadrotor_3d(sol, p; title, N, x_symbol, y_symbol, z_symbol, φ_symbol, θ_symbol, ψ_symbol, T_symbol, τφ_symbol, τθ_symbol, τψ_symbol)\n\nPlot the 3D quadrotor's trajectory.\n\nWhen thrusts are supplied, the arrows scale with thrust, otherwise the arrows are of constant length.\n\nArguments\n\nx: The x-coordinate of the quadrotor at each time step.\ny: The y-coordinate of the quadrotor at each time step.\nz: The z-coordinate of the quadrotor at each time step.\nφ: The roll of the quadrotor at each time step.\nθ: The pitch of the quadrotor at each time step.\nψ: The yaw of the quadrotor at each time step.\nT: The thrust of the quadrotor at each time step.\nτφ: The roll torque of the quadrotor at each time step.\nτθ: The pitch torque of the quadrotor at each time step.\nτψ: The yaw torque of the quadrotor at each time step.\nt: The time steps.\nsol: The solution to the ODE problem.\np: The parameters of the quadrotor.\n\nKeyword arguments\n\ntitle: The title of the plot; defaults to no title (i.e., title=\"\").\nN: The number of points to plot; when using x, y, z, etc., uses length(t); defaults to 500 when using sol.\nx_symbol: The symbol of the x-coordinate in sol; defaults to :x.\ny_symbol: The symbol of the y-coordinate in sol; defaults to :y.\nz_symbol: The symbol of the z-coordinate in sol; defaults to :z.\nφ_symbol: The symbol of the roll in sol; defaults to :φ.\nθ_symbol: The symbol of the pitch in sol; defaults to :θ.\nψ_symbol: The symbol of the yaw in sol; defaults to :ψ.\nT_symbol: The symbol of the thrust in sol; defaults to :T.\nτφ_symbol: The symbol of the roll torque in sol; defaults to :τφ.\nτθ_symbol: The symbol of the pitch torque in sol; defaults to :τθ.\nτψ_symbol: The symbol of the yaw torque in sol; defaults to :τψ.\n\n\n\n\n\n","category":"function"},{"location":"lib/quadrotor/#References","page":"Quadrotor Models","title":"References","text":"","category":"section"},{"location":"lib/quadrotor/","page":"Quadrotor Models","title":"Quadrotor Models","text":"Mellinger, D. and Kumar, V. (2011). Minimum snap trajectory generation and control for quadrotors. In: 2011 IEEE International Conference on Robotics and Automation; pp. 2520–2525.\n\n\n\n","category":"page"},{"location":"man/internals/#Internals","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"man/internals/#NeuralLyapunov.phi_to_net","page":"Internals","title":"NeuralLyapunov.phi_to_net","text":"phi_to_net(phi, θ[; idx])\n\nReturn the network as a function of state alone.\n\nArguments\n\nphi: the neural network, represented as phi(x, θ) if the neural network has a single output, or a Vector of the same with one entry per neural network output.\nθ: the parameters of the neural network; If the neural network has multiple outputs, θ[:φ1] should be the parameters of the first neural network output, θ[:φ2] the parameters of the second (if there are multiple), and so on. If the neural network has a single output, θ should be the parameters of the network.\nidx: the neural network outputs to include in the returned function; defaults to all and only applicable when phi isa Vector.\n\n\n\n\n\n","category":"function"},{"location":"man/local_lyapunov/#Local-Lyapunov-analysis","page":"Local Lyapunov analysis","title":"Local Lyapunov analysis","text":"","category":"section"},{"location":"man/local_lyapunov/","page":"Local Lyapunov analysis","title":"Local Lyapunov analysis","text":"For comparison with neural Lyapunov methods, we also provide a function for local Lyapunov analysis by linearization.","category":"page"},{"location":"man/local_lyapunov/#NeuralLyapunov.local_lyapunov","page":"Local Lyapunov analysis","title":"NeuralLyapunov.local_lyapunov","text":"local_lyapunov(dynamics, state_dim, optimizer_factory[, jac]; fixed_point, p)\n\nUse semidefinite programming to derive a quadratic Lyapunov function for the linearization of dynamics around fixed_point. Return (V, dV/dt).\n\nTo solve the semidefinite program, JuMP.Model requires an optimizer_factory capable of semidefinite programming (SDP). See the JuMP documentation for examples.\n\nIf jac is not supplied, the Jacobian of the dynamics(x, p, t) with respect to x is calculated using ForwardDiff. Otherwise, jac is expected to be either a function or an AbstractMatrix. If jac isa Function, it should take in the state and parameters and output the Jacobian of dynamics with respect to the state x. If jac isa AbstractMatrix, it should be the value of the Jacobian at fixed_point.\n\nIf fixed_point is not specified, it defaults to the origin, i.e., zeros(state_dim). Parameters p for the dynamics should be supplied when the dynamics depend on them.\n\n\n\n\n\n","category":"function"},{"location":"man/#Components-of-a-Neural-Lyapunov-Problem","page":"Components of a Neural Lyapunov Problem","title":"Components of a Neural Lyapunov Problem","text":"","category":"section"},{"location":"man/","page":"Components of a Neural Lyapunov Problem","title":"Components of a Neural Lyapunov Problem","text":"For a candidate Lyapunov function V(x) to certify the stability of an equilibrium point x_0 of the dynamical system fracdxdt = f(x(t)), it must satisfy two conditions:","category":"page"},{"location":"man/","page":"Components of a Neural Lyapunov Problem","title":"Components of a Neural Lyapunov Problem","text":"The function V must be uniquely minimized at x_0, and \nThe function V must decrease along system trajectories (i.e., V(x(t)) decreases as long as x(t) is a trajectory of the dynamical system).","category":"page"},{"location":"man/","page":"Components of a Neural Lyapunov Problem","title":"Components of a Neural Lyapunov Problem","text":"A neural Lyapunov function represents the candidate Lyapunov function V using a neural network, sometimes modifying the output of the network slightly so as to enforce one of the above conditions.","category":"page"},{"location":"man/","page":"Components of a Neural Lyapunov Problem","title":"Components of a Neural Lyapunov Problem","text":"Thus, we specify our neural Lyapunov problems with three components, each answering a different question:","category":"page"},{"location":"man/","page":"Components of a Neural Lyapunov Problem","title":"Components of a Neural Lyapunov Problem","text":"How is V structured in terms of the neural network?\nHow is the minimization condition to be enforced?\nHow is the decrease condition to be enforced?","category":"page"},{"location":"man/","page":"Components of a Neural Lyapunov Problem","title":"Components of a Neural Lyapunov Problem","text":"These three components are represented by the three fields of a NeuralLyapunovSpecification object.","category":"page"},{"location":"man/#NeuralLyapunov.NeuralLyapunovSpecification","page":"Components of a Neural Lyapunov Problem","title":"NeuralLyapunov.NeuralLyapunovSpecification","text":"NeuralLyapunovSpecification(structure, minimization_condition, decrease_condition)\n\nSpecifies a neural Lyapunov problem.\n\nFields\n\nstructure: a NeuralLyapunovStructure specifying the relationship between the neural network and the candidate Lyapunov function.\nminimization_condition: an AbstractLyapunovMinimizationCondition specifying how the minimization condition will be enforced.\ndecrease_condition: an AbstractLyapunovDecreaseCondition specifying how the decrease condition will be enforced.\n\n\n\n\n\n","category":"type"},{"location":"demos/policy_search/#Policy-Search-on-the-Driven-Inverted-Pendulum","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"","category":"section"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"In this demonstration, we'll search for a neural network policy to stabilize the upright equilibrium of the inverted pendulum.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"The governing differential equation for the driven pendulum is:","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"fracd^2 thetadt^2 + 2 zeta omega_0 fracd thetadt + omega_0^2 sin(theta) = tau","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"where theta is the counterclockwise angle from the downward equilibrium, zeta and omega_0 are system parameters, and tau is our control input&mdash;the torque.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"We'll jointly train a neural controller tau = u left( theta fracdthetadt right) and neural Lyapunov function V which will certify the stability of the closed-loop system.","category":"page"},{"location":"demos/policy_search/#Copy-Pastable-Code","page":"Policy Search on the Driven Inverted Pendulum","title":"Copy-Pastable Code","text":"","category":"section"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"using NeuralPDE, Lux, ModelingToolkit, NeuralLyapunov\nusing ModelingToolkit: inputs\nusing NeuralLyapunovProblemLibrary\nimport Boltz.Layers: PeriodicEmbedding\nimport Optimization, OptimizationOptimisers, OptimizationOptimJL\nusing Random, StableRNGs\n\nrng = StableRNG(0)\nRandom.seed!(200)\n\n######################### Define dynamics and domain ##########################\n\n@named pendulum = Pendulum(; defaults = [0.5, 1.0])\n\nt, = independent_variables(pendulum)\nDt = Differential(t)\nθ, = setdiff(unknowns(pendulum), inputs(pendulum))\n\nbounds = [\n    θ ∈ (0, 2π),\n    Dt(θ) ∈ (-2.0, 2.0)\n]\n\nupright_equilibrium = [π, 0.0]\n\n####################### Specify neural Lyapunov problem #######################\n\n# Define neural network discretization\n# We use an input layer that is periodic with period 2π with respect to θ\ndim_state = length(bounds)\ndim_hidden = 20\ndim_phi = 2\ndim_u = 1\ndim_output = dim_phi + dim_u\nchain = [Chain(\n             PeriodicEmbedding([1], [2π]),\n             Dense(3, dim_hidden, tanh),\n             Dense(dim_hidden, dim_hidden, tanh),\n             Dense(dim_hidden, 1)\n         ) for _ in 1:dim_output]\nps = Lux.initialparameters(rng, chain)\n\n# Define neural network discretization\nstrategy = QuasiRandomTraining(5000)\ndiscretization = PhysicsInformedNN(chain, strategy; init_params = ps)\n\n# Define neural Lyapunov structure\nperiodic_pos_def = function (state, fixed_point)\n    θ, ω = state\n    θ_eq, ω_eq = fixed_point\n    return (sin(θ) - sin(θ_eq))^2 + (cos(θ) - cos(θ_eq))^2 + 0.1 * (ω - ω_eq)^2\nend\n\nstructure = PositiveSemiDefiniteStructure(\n    dim_phi;\n    pos_def = (x, x0) -> log(1.0 + periodic_pos_def(x, x0))\n)\nstructure = add_policy_search(structure, dim_u)\n\nminimization_condition = DontCheckNonnegativity(check_fixed_point = false)\n\n# Define Lyapunov decrease condition\ndecrease_condition = AsymptoticStability(strength = periodic_pos_def)\n\n# Construct neural Lyapunov specification\nspec = NeuralLyapunovSpecification(\n    structure,\n    minimization_condition,\n    decrease_condition\n)\n\n############################# Construct PDESystem #############################\n\n@named pde_system = NeuralLyapunovPDESystem(\n    pendulum,\n    bounds,\n    spec;\n    fixed_point = upright_equilibrium\n)\n\n######################## Construct OptimizationProblem ########################\n\nprob = discretize(pde_system, discretization)\n\n########################## Solve OptimizationProblem ##########################\n\nres = Optimization.solve(prob, OptimizationOptimisers.Adam(0.01); maxiters = 300)\nprob = Optimization.remake(prob, u0 = res.u)\nres = Optimization.solve(prob, OptimizationOptimJL.BFGS(); maxiters = 300)\n\n###################### Get numerical numerical functions ######################\n\nnet = discretization.phi\n_θ = res.u.depvar\n\n(open_loop_pendulum_dynamics, _), state_order, p_order = ModelingToolkit.generate_control_function(\n    pendulum; simplify = true, split = false)\np = [defaults(pendulum)[param] for param in p_order]\n\nV_func, V̇_func = get_numerical_lyapunov_function(\n    net,\n    _θ,\n    structure,\n    open_loop_pendulum_dynamics,\n    upright_equilibrium;\n    p\n)\n\nu = get_policy(net, _θ, dim_output, dim_u)","category":"page"},{"location":"demos/policy_search/#Detailed-description","page":"Policy Search on the Driven Inverted Pendulum","title":"Detailed description","text":"","category":"section"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"In this example, we'll use the Pendulum model in NeuralLyaupnovProblemLibrary.jl.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"Since the angle theta is periodic with period 2pi, our box domain will be one period in theta and an interval in fracdthetadt.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"using ModelingToolkit, NeuralLyapunovProblemLibrary\nusing ModelingToolkit: inputs\n\n@named pendulum = Pendulum(; defaults = [0.5, 1.0])\n\nt, = independent_variables(pendulum)\nDt = Differential(t)\nθ, = setdiff(unknowns(pendulum), inputs(pendulum))\n\nbounds = [\n    θ ∈ (0, 2π),\n    Dt(θ) ∈ (-2.0, 2.0)\n]\n\nupright_equilibrium = [π, 0.0]","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"We'll use an architecture that's 2pi-periodic in theta so that we can train on just one period of theta and don't need to add any periodic boundary conditions. To achieve that, we use Boltz.Layers.PeriodicEmbedding([1], [2pi]), enforces 2pi-periodicity in input number 1. Additionally, we include output dimensions for both the neural Lyapunov function and the neural controller.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"Other than that, setting up the neural network using Lux and NeuralPDE training strategy is no different from any other physics-informed neural network problem. For more on that aspect, see the NeuralPDE documentation.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"using Lux\nimport Boltz.Layers: PeriodicEmbedding\nusing StableRNGs\n\n# Stable random number generator for doc stability\nrng = StableRNG(0)\n\n# Define neural network discretization\n# We use an input layer that is periodic with period 2π with respect to θ\ndim_state = length(bounds)\ndim_hidden = 20\ndim_phi = 2\ndim_u = 1\ndim_output = dim_phi + dim_u\nchain = [Chain(\n             PeriodicEmbedding([1], [2π]),\n             Dense(3, dim_hidden, tanh),\n             Dense(dim_hidden, dim_hidden, tanh),\n             Dense(dim_hidden, 1)\n         ) for _ in 1:dim_output]\nps = Lux.initialparameters(rng, chain)","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"using NeuralPDE\n\n# Define neural network discretization\nstrategy = QuasiRandomTraining(5000)\ndiscretization = PhysicsInformedNN(chain, strategy; init_params = ps)\nnothing # hide","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"We now define our Lyapunov candidate structure along with the form of the Lyapunov conditions we'll be using.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"The default Lyapunov candidate from PositiveSemiDefiniteStructure is:","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"V(x) = left( 1 + lVert phi(x) rVert^2 right) log left( 1 + lVert x rVert^2 right)","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"which structurally enforces positive definiteness. We'll modify the second factor to be 2pi-periodic in theta:","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"using NeuralLyapunov\n\n# Define neural Lyapunov structure\nperiodic_pos_def = function (state, fixed_point)\n    θ, ω = state\n    θ_eq, ω_eq = fixed_point\n    return (sin(θ) - sin(θ_eq))^2 + (cos(θ) - cos(θ_eq))^2 + 0.1 * (ω - ω_eq)^2\nend\n\nstructure = PositiveSemiDefiniteStructure(\n    dim_phi;\n    pos_def = (x, x0) -> log(1.0 + periodic_pos_def(x, x0))\n)\nnothing # hide","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"In addition to representing the neural Lyapunov function, our neural network must also represent the controller. For this, we use the add_policy_search function, which tells NeuralLyapunov to expect dynamics with a control input and to treat the last dim_u dimensions of the neural network as the output of our controller.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"structure = add_policy_search(structure, dim_u)\nnothing # hide","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"Since our Lyapunov candidate structurally enforces positive definiteness, we use DontCheckNonnegativity.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"minimization_condition = DontCheckNonnegativity(check_fixed_point = false)\n\n# Define Lyapunov decrease condition\ndecrease_condition = AsymptoticStability(strength = periodic_pos_def)\n\n# Construct neural Lyapunov specification\nspec = NeuralLyapunovSpecification(\n    structure,\n    minimization_condition,\n    decrease_condition\n)\n\n# Construct PDESystem \n@named pde_system = NeuralLyapunovPDESystem(\n    pendulum,\n    bounds,\n    spec;\n    fixed_point = upright_equilibrium\n)","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"Now, we solve the PDESystem using NeuralPDE the same way we would any PINN problem.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"prob = discretize(pde_system, discretization)\n\nimport Optimization, OptimizationOptimisers, OptimizationOptimJL\n\nres = Optimization.solve(prob, OptimizationOptimisers.Adam(0.01); maxiters = 300)\nprob = Optimization.remake(prob, u0 = res.u)\nres = Optimization.solve(prob, OptimizationOptimJL.BFGS(); maxiters = 300)\n\nnet = discretization.phi\n_θ = res.u.depvar","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"We can use the result of the optimization problem to build the Lyapunov candidate as a Julia function, as well as extract our controller, using the get_policy function.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"(open_loop_pendulum_dynamics, _), state_order, p_order = ModelingToolkit.generate_control_function(pendulum; simplify = true, split = false)\np = [defaults(pendulum)[param] for param in p_order]\n\nV_func, V̇_func = get_numerical_lyapunov_function(\n    net,\n    _θ,\n    structure,\n    open_loop_pendulum_dynamics,\n    upright_equilibrium;\n    p = p\n)\n\nu = get_policy(net, _θ, dim_output, dim_u)\nnothing # hide","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"Now, let's evaluate our controller. First, we'll get the usual summary statistics on the Lyapunov function and plot V, dotV, and the violations of the decrease condition.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"lb = [0.0, -2.0];\nub = [2π, 2.0];\nxs = (-2π):0.1:(2π)\nys = lb[2]:0.1:ub[2]\nstates = Iterators.map(collect, Iterators.product(xs, ys))\nV_samples = vec(V_func(hcat(states...)))\nV̇_samples = vec(V̇_func(hcat(states...)))\n\n# Print statistics\nprintln(\"V(π, 0) = \", V_func(upright_equilibrium))\nprintln(\n    \"f([π, 0], u([π, 0])) = \",\n    open_loop_pendulum_dynamics(upright_equilibrium, u(upright_equilibrium), p, 0.0)\n)\nprintln(\n    \"V ∋ [\",\n    min(V_func(upright_equilibrium),\n    minimum(V_samples)),\n    \", \",\n    maximum(V_samples),\n    \"]\"\n)\nprintln(\n    \"V̇ ∋ [\",\n    minimum(V̇_samples),\n    \", \",\n    max(V̇_func(upright_equilibrium), maximum(V̇_samples)),\n    \"]\"\n)","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"using Plots\n\np1 = plot(\n    xs / pi,\n    ys,\n    V_samples,\n    linetype =\n    :contourf,\n    title = \"V\",\n    xlabel = \"θ/π\",\n    ylabel = \"ω\",\n    c = :bone_1\n);\np1 = scatter!([-2 * pi, 0, 2 * pi] / pi, [0, 0, 0],\n    label = \"Downward Equilibria\", color = :red, markershape = :x);\np1 = scatter!(\n    [-pi, pi] / pi, [0, 0], label = \"Upward Equilibria\", color = :green, markershape = :+);\np2 = plot(\n    xs / pi,\n    ys,\n    V̇_samples,\n    linetype = :contourf,\n    title = \"dV/dt\",\n    xlabel = \"θ/π\",\n    ylabel = \"ω\",\n    c = :binary\n);\np2 = scatter!([-2 * pi, 0, 2 * pi] / pi, [0, 0, 0],\n    label = \"Downward Equilibria\", color = :red, markershape = :x);\np2 = scatter!([-pi, pi] / pi, [0, 0], label = \"Upward Equilibria\", color = :green,\n    markershape = :+, legend = false);\np3 = plot(\n    xs / pi,\n    ys,\n    V̇_samples .< 0,\n    linetype = :contourf,\n    title = \"dV/dt < 0\",\n    xlabel = \"θ/π\",\n    ylabel = \"ω\",\n    colorbar = false,\n    linewidth = 0\n);\np3 = scatter!([-2 * pi, 0, 2 * pi] / pi, [0, 0, 0],\n    label = \"Downward Equilibria\", color = :green, markershape = :+);\np3 = scatter!([-pi, pi] / pi, [0, 0], label = \"Upward Equilibria\",\n    color = :red, markershape = :x, legend = false);\nplot(p1, p2, p3)","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"Now, let's simulate the closed-loop dynamics to verify that the controller can get our system to the upward equilibrium.","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"First, we'll start at the downward equilibrium:","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"state_order = map(st -> SymbolicUtils.isterm(st) ? operation(st) : st, state_order)\nstate_syms = Symbol.(state_order)\n\nclosed_loop_dynamics = ODEFunction(\n    (x, p, t) -> open_loop_pendulum_dynamics(x, u(x), p, t);\n    sys = SciMLBase.SymbolCache(state_syms, Symbol.(p_order))\n)\n\nusing OrdinaryDiffEq: Tsit5\n\n# Starting still at bottom ...\ndownward_equilibrium = zeros(2)\node_prob = ODEProblem(closed_loop_dynamics, downward_equilibrium, [0.0, 120.0], p)\nsol = solve(ode_prob, Tsit5())\nplot(sol)","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"# ...the system should make it to the top\nθ_end, ω_end = sol.u[end]\nx_end, y_end = sin(θ_end), -cos(θ_end)\n[x_end, y_end, ω_end] # Should be approximately [0.0, 1.0, 0.0]","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"Then, we'll start at a random state:","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"# Starting at a random point ...\nx0 = lb .+ rand(2) .* (ub .- lb)\node_prob = ODEProblem(closed_loop_dynamics, x0, [0.0, 150.0], p)\nsol = solve(ode_prob, Tsit5())\nplot(sol)","category":"page"},{"location":"demos/policy_search/","page":"Policy Search on the Driven Inverted Pendulum","title":"Policy Search on the Driven Inverted Pendulum","text":"# ...the system should make it to the top\nθ_end, ω_end = sol.u[end]\nx_end, y_end = sin(θ_end), -cos(θ_end)\n[x_end, y_end, ω_end] # Should be approximately [0.0, 1.0, 0.0]","category":"page"},{"location":"demos/roa_estimation/#Estimating-the-Region-of-Attraction","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"","category":"section"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"In this demonstration, we add awareness of the region of attraction (RoA) estimation task to our training.","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"We'll be examining the simple one-dimensional differential equation:","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"fracdxdt = - x + x^3","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"This system has a fixed point at x = 0 which has a RoA of x in (-1 1), which we will attempt to identify.","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"We'll train in the larger domain x in -2 2.","category":"page"},{"location":"demos/roa_estimation/#Copy-Pastable-Code","page":"Estimating the Region of Attraction","title":"Copy-Pastable Code","text":"","category":"section"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"using NeuralPDE, Lux, NeuralLyapunov\nusing Optimization, OptimizationOptimisers, OptimizationOptimJL\nusing Random, StableRNGs\n\nrng = StableRNG(0)\nRandom.seed!(200)\n\n######################### Define dynamics and domain ##########################\n\nf(x, p, t) = -x .+ x .^ 3\nlb = [-2.0];\nub = [ 2.0];\nfixed_point = [0.0];\n\n####################### Specify neural Lyapunov problem #######################\n\n# Define neural network discretization\ndim_state = length(lb)\ndim_hidden = 5\ndim_output = 2\nchain = [Lux.Chain(\n             Dense(dim_state, dim_hidden, tanh),\n             Dense(dim_hidden, dim_hidden, tanh),\n             Dense(dim_hidden, 1, use_bias = false)\n         ) for _ in 1:dim_output]\nps = Lux.initialparameters(rng, chain)\n\n# Define training strategy\nstrategy = GridTraining(0.1)\ndiscretization = PhysicsInformedNN(chain, strategy; init_params = ps)\n\n# Define neural Lyapunov structure\nstructure = PositiveSemiDefiniteStructure(dim_output)\nminimization_condition = DontCheckNonnegativity()\n\n# Define Lyapunov decrease condition\ndecrease_condition = make_RoA_aware(AsymptoticStability())\n\n# Construct neural Lyapunov specification\nspec = NeuralLyapunovSpecification(\n    structure,\n    minimization_condition,\n    decrease_condition\n)\n\n############################# Construct PDESystem #############################\n\n@named pde_system = NeuralLyapunovPDESystem(\n    f,\n    lb,\n    ub,\n    spec\n)\n\n######################## Construct OptimizationProblem ########################\n\nprob = discretize(pde_system, discretization)\n\n########################## Solve OptimizationProblem ##########################\n\nres = Optimization.solve(prob, OptimizationOptimisers.Adam(); maxiters = 300)\nprob = Optimization.remake(prob, u0 = res.u)\nres = Optimization.solve(prob, OptimizationOptimJL.BFGS(); maxiters = 300)\n\n###################### Get numerical numerical functions ######################\nnet = discretization.phi\nθ = res.u.depvar\n\nV, V̇ = get_numerical_lyapunov_function(\n    net,\n    θ,\n    structure,\n    f,\n    fixed_point\n)\n\n################################## Simulate ###################################\nstates = lb[]:0.001:ub[]\nV_samples = vec(V(states'))\nV̇_samples = vec(V̇(states'))\n\n# Calculated RoA estimate\nρ = decrease_condition.ρ\nRoA_states = states[vec(V(transpose(states))) .≤ ρ]\nRoA = (first(RoA_states), last(RoA_states))","category":"page"},{"location":"demos/roa_estimation/#Detailed-description","page":"Estimating the Region of Attraction","title":"Detailed description","text":"","category":"section"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"In this example, we set up the dynamics as a Julia function and don't bother specifying the symbols for the variables (so x will be called the default state1 in the PDESystem).","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"f(x, p, t) = -x .+ x .^ 3\nlb = [-2.0];\nub = [ 2.0];\nfixed_point = [0.0];\nnothing # hide","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"Setting up the neural network using Lux and NeuralPDE training strategy is no different from any other physics-informed neural network problem. For more on that aspect, see the NeuralPDE documentation. Since we're only considering one dimension, training on a grid isn't so bad in this case.","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"using Lux, StableRNGs\n\n# Stable random number generator for doc stability\nrng = StableRNG(0)\n\n# Define neural network discretization\ndim_state = length(lb)\ndim_hidden = 5\ndim_output = 2\nchain = [Lux.Chain(\n             Dense(dim_state, dim_hidden, tanh),\n             Dense(dim_hidden, dim_hidden, tanh),\n             Dense(dim_hidden, 1, use_bias = false)\n         ) for _ in 1:dim_output]\nps = Lux.initialparameters(rng, chain)","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"using NeuralPDE\n\n# Define training strategy\nstrategy = GridTraining(0.1)\ndiscretization = PhysicsInformedNN(chain, strategy; init_params = ps)\nnothing # hide","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"We now define our Lyapunov candidate structure along with the form of the Lyapunov conditions we'll be using.","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"For this example, let's use the default Lyapunov candidate from PositiveSemiDefiniteStructure:","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"V(x) = left( 1 + lVert phi(x) rVert^2 right) log left( 1 + lVert x rVert^2 right)","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"which structurally enforces positive definiteness. We therefore use DontCheckNonnegativity().","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"We only require asymptotic stability in this example, but we use make_RoA_aware to only penalize positive values of dotV(x) when V(x) le 1.","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"using NeuralLyapunov\n\n# Define neural Lyapunov structure\nstructure = PositiveSemiDefiniteStructure(dim_output)\nminimization_condition = DontCheckNonnegativity()\n\n# Define Lyapunov decrease condition\ndecrease_condition = make_RoA_aware(AsymptoticStability())\n\n# Construct neural Lyapunov specification\nspec = NeuralLyapunovSpecification(\n    structure,\n    minimization_condition,\n    decrease_condition\n)\n\n# Construct PDESystem \n@named pde_system = NeuralLyapunovPDESystem(\n    f,\n    lb,\n    ub,\n    spec\n)","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"Now, we solve the PDESystem using NeuralPDE the same way we would any PINN problem.","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"prob = discretize(pde_system, discretization)\n\nimport Optimization, OptimizationOptimisers, OptimizationOptimJL\n\nres = Optimization.solve(prob, OptimizationOptimisers.Adam(); maxiters = 300)\nprob = Optimization.remake(prob, u0 = res.u)\nres = Optimization.solve(prob, OptimizationOptimJL.BFGS(); maxiters = 300)\n\nnet = discretization.phi\nθ = res.u.depvar","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"We can use the result of the optimization problem to build the Lyapunov candidate as a Julia function, then sample on a finer grid than we trained on to find the estimated region of attraction.","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"V, V̇ = get_numerical_lyapunov_function(\n    net,\n    θ,\n    structure,\n    f,\n    fixed_point\n)\n\n# Sample\nstates = lb[]:0.001:ub[]\nV_samples = vec(V(states'))\nV̇_samples = vec(V̇(states'))\n\n# Calculate RoA estimate\nρ = decrease_condition.ρ\nRoA_states = states[vec(V(transpose(states))) .≤ ρ]\nRoA = (first(RoA_states), last(RoA_states))\n\n# Print statistics\nprintln(\"V(0.,0.) = \", V(fixed_point))\nprintln(\"V ∋ [\", min(V(fixed_point), minimum(V_samples)), \", \", maximum(V_samples), \"]\")\nprintln(\n    \"V̇ ∋ [\",\n    minimum(V̇_samples),\n    \", \",\n    max(V̇(fixed_point), maximum(V̇_samples)),\n    \"]\",\n)\nprintln(\"True region of attraction: (-1, 1)\")\nprintln(\"Estimated region of attraction: \", RoA)","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"The estimated region of attraction is within the true region of attraction.","category":"page"},{"location":"demos/roa_estimation/","page":"Estimating the Region of Attraction","title":"Estimating the Region of Attraction","text":"using Plots\n\np_V = plot(states, V_samples, label = \"V\", xlabel = \"x\", linewidth=2);\np_V = hline!([ρ], label = \"V = ρ\", legend = :top);\np_V = vspan!(collect(RoA); label = \"Estimated Region of Attraction\", color = :gray, fillstyle = :/);\np_V = vspan!([-1, 1]; label = \"True Region of Attraction\", opacity = 0.2, color = :green);\n\np_V̇ = plot(states, V̇_samples, label = \"dV/dt\", xlabel = \"x\", linewidth=2);\np_V̇ = hline!([0.0], label = \"dV/dt = 0\", legend = :top);\np_V̇ = vspan!(collect(RoA); label = \"Estimated Region of Attraction\", color = :gray, fillstyle = :/);\np_V̇ = vspan!([-1, 1]; label = \"True Region of Attraction\", opacity = 0.2, color = :green);\n\nplt = plot(p_V, p_V̇)","category":"page"},{"location":"man/decrease/#Lyapunov-Decrease-Condition","page":"Lyapunov Decrease Condition","title":"Lyapunov Decrease Condition","text":"","category":"section"},{"location":"man/decrease/","page":"Lyapunov Decrease Condition","title":"Lyapunov Decrease Condition","text":"To represent the condition that the Lyapunov function V(x) must decrease along system trajectories, we typically introduce a new function dotV(x) = nabla V(x) cdot f(x). This function represents the rate of change of V along system trajectories. That is to say, if x(t) is a trajectory defined by fracdxdt = f(x), then dotV(x(t)) = fracddt  V(x(t)) . It is then sufficient to show that dotV(x) is negative away from the fixed point and zero at the fixed point, since a negative derivative means a decreasing function.","category":"page"},{"location":"man/decrease/","page":"Lyapunov Decrease Condition","title":"Lyapunov Decrease Condition","text":"Put mathematically, it is sufficient to require dotV(x)  0  forall x ne x_0 and dotV(x_0) = 0. We call such functions negative definite (around the fixed point x_0). The weaker condition that dotV(x) le 0  forall x ne x_0 and dotV(x_0) = 0 is negative semi-definiteness.","category":"page"},{"location":"man/decrease/","page":"Lyapunov Decrease Condition","title":"Lyapunov Decrease Condition","text":"The condition that dotV(x_0) = 0 is satisfied by the definition of dotV and the fact that x_0 is a fixed point, so we do not need to train for it. In cases where the dynamics have some dependence on the neural network (e.g., in policy search), we should rather train directly for f(x_0) = 0, since the minimization condition will also guarantee nabla V(x_0) = 0, so dotV(x_0) = 0.","category":"page"},{"location":"man/decrease/","page":"Lyapunov Decrease Condition","title":"Lyapunov Decrease Condition","text":"NeuralLyapunov.jl provides the LyapunovDecreaseCondition struct for users to specify the form of the decrease condition to enforce through training.","category":"page"},{"location":"man/decrease/#NeuralLyapunov.LyapunovDecreaseCondition","page":"Lyapunov Decrease Condition","title":"NeuralLyapunov.LyapunovDecreaseCondition","text":"LyapunovDecreaseCondition(check_decrease, rate_metric, strength, rectifier)\n\nSpecifies the form of the Lyapunov decrease condition to be used.\n\nFields\n\ncheck_decrease::Bool: whether or not to train for negativity/nonpositivity of V(x).\nrate_metric::Function: should increase with V(x); used when check_decrease == true.\nstrength::Function: specifies the level of strictness for negativity training; should be zero when the two inputs are equal and nonnegative otherwise; used when check_decrease == true.\nrectifier::Function: positive when the input is positive and (approximately) zero when the input is negative.\n\nTraining conditions\n\nIf check_decrease == true, training will enforce:\n\ntextttrate_metric(V(x) V(x))  -textttstrength(x x_0)\n\nThe inequality will be approximated by the equation:\n\ntextttrectifier(textttrate_metric(V(x) V(x)) + textttstrength(x x_0)) = 0\n\nNote that the approximate equation and inequality are identical when textttrectifier(t) = max(0 t).\n\nIf the dynamics truly have a fixed point at x_0 and V(x) is truly the rate of decrease of V(x) along the dynamics, then V(x_0) will be 0 and there is no need to train for V(x_0) = 0.\n\nExamples:\n\nAsymptotic decrease can be enforced by requiring     V(x)  -C lVert x - x_0 rVert^2, for some positive C, which corresponds to\n\nrate_metric = (V, dVdt) -> dVdt\nstrength = (x, x0) -> C * (x - x0) ⋅ (x - x0)\n\nThis can also be accomplished with AsymptoticStability.\n\nExponential decrease of rate k is proven by     V(x)  - k V(x), which corresponds to\n\nrate_metric = (V, dVdt) -> dVdt + k * V\nstrength = (x, x0) -> 0.0\n\nThis can also be accomplished with ExponentialStability.\n\nIn either case, the rectified linear unit rectifier = (t) -> max(zero(t), t) exactly represents the inequality, but differentiable approximations of this function may be employed.\n\n\n\n\n\n","category":"type"},{"location":"man/decrease/#Pre-defined-decrease-conditions","page":"Lyapunov Decrease Condition","title":"Pre-defined decrease conditions","text":"","category":"section"},{"location":"man/decrease/#NeuralLyapunov.AsymptoticStability","page":"Lyapunov Decrease Condition","title":"NeuralLyapunov.AsymptoticStability","text":"AsymptoticStability(; C, strength, rectifier)\n\nConstruct a LyapunovDecreaseCondition corresponding to asymptotic stability.\n\nThe decrease condition for asymptotic stability is V(x)  0, which is here represented as V(x)  - textttstrength(x x_0), where strength is positive definite around x_0. By default, textttstrength(x x_0) = C lVert x - x_0 rVert^2 for the supplied C  0. C defaults to 1e-6.\n\nThe inequality is represented by textttrectifier(V(x) + textttstrength(x x_0)) = 0. The default value rectifier = (t) -> max(zero(t), t) exactly represents the inequality, but differentiable approximations of this function may be employed.\n\n\n\n\n\n","category":"function"},{"location":"man/decrease/#NeuralLyapunov.ExponentialStability","page":"Lyapunov Decrease Condition","title":"NeuralLyapunov.ExponentialStability","text":"ExponentialStability(k; rectifier)\n\nConstruct a LyapunovDecreaseCondition corresponding to exponential stability of rate k.\n\nThe Lyapunov condition for exponential stability is V(x)  -k V(x) for some k  0.\n\nThe inequality is represented by textttrectifier(V(x) + k V(x)) = 0. The default value rectifier = (t) -> max(zero(t), t) exactly represents the inequality, but differentiable approximations of this function may be employed.\n\n\n\n\n\n","category":"function"},{"location":"man/decrease/#NeuralLyapunov.StabilityISL","page":"Lyapunov Decrease Condition","title":"NeuralLyapunov.StabilityISL","text":"StabilityISL(; rectifier)\n\nConstruct a LyapunovDecreaseCondition corresponding to stability in the sense of Lyapunov (i.s.L.).\n\nStability i.s.L. is proven by V(x)  0. The inequality is represented by textttrectifier(V(x)) = 0. The default value rectifier = (t) -> max(zero(t), t) exactly represents the inequality, but differentiable approximations of this function may be employed.\n\n\n\n\n\n","category":"function"},{"location":"man/decrease/#NeuralLyapunov.DontCheckDecrease","page":"Lyapunov Decrease Condition","title":"NeuralLyapunov.DontCheckDecrease","text":"DontCheckDecrease()\n\nConstruct a LyapunovDecreaseCondition which represents not checking for decrease of the Lyapunov function along system trajectories. This is appropriate in cases when the Lyapunov decrease condition has been structurally enforced.\n\n\n\n\n\n","category":"function"},{"location":"man/decrease/#Defining-your-own-decrease-condition","page":"Lyapunov Decrease Condition","title":"Defining your own decrease condition","text":"","category":"section"},{"location":"man/decrease/","page":"Lyapunov Decrease Condition","title":"Lyapunov Decrease Condition","text":"If a user wishes to define their own version of the decrease condition in a form other than textttrate_metric(V(x) dotV(x)) le - textttstrength(x x_0), they must define their own subtype of AbstractLyapunovDecreaseCondition.","category":"page"},{"location":"man/decrease/#NeuralLyapunov.AbstractLyapunovDecreaseCondition","page":"Lyapunov Decrease Condition","title":"NeuralLyapunov.AbstractLyapunovDecreaseCondition","text":"AbstractLyapunovDecreaseCondition\n\nRepresents the decrease condition in a neural Lyapunov problem\n\nAll concrete AbstractLyapunovDecreaseCondition subtypes should define the check_decrease and get_decrease_condition functions.\n\n\n\n\n\n","category":"type"},{"location":"man/decrease/","page":"Lyapunov Decrease Condition","title":"Lyapunov Decrease Condition","text":"When constructing the PDESystem, NeuralLyapunovPDESystem uses check_decrease to determine if it should include an equation equating the result of get_decrease_condition to zero.","category":"page"},{"location":"man/decrease/#NeuralLyapunov.check_decrease","page":"Lyapunov Decrease Condition","title":"NeuralLyapunov.check_decrease","text":"check_decrease(cond::AbstractLyapunovDecreaseCondition)\n\nReturn true if cond specifies training to meet the Lyapunov decrease condition, and false if cond specifies no training to meet this condition.\n\n\n\n\n\n","category":"function"},{"location":"man/decrease/#NeuralLyapunov.get_decrease_condition","page":"Lyapunov Decrease Condition","title":"NeuralLyapunov.get_decrease_condition","text":"get_decrease_condition(cond::AbstractLyapunovDecreaseCondition)\n\nReturn a function of V, V, x, and x_0 that returns zero when the Lyapunov decrease condition is met and a value greater than zero when it is violated.\n\nNote that the first two inputs, V and V, are functions, so the decrease condition can depend on the value of these functions at multiple points.\n\n\n\n\n\n","category":"function"},{"location":"lib/pendulum/#Pendulum-Model","page":"Pendulum Model","title":"Pendulum Model","text":"","category":"section"},{"location":"lib/pendulum/","page":"Pendulum Model","title":"Pendulum Model","text":"A simple damped pendulum can be constructed using the Pendulum function, as shown below. Additionally, when also using the Plots.jl package, the convenience plotting function plot_pendulum is provided.","category":"page"},{"location":"lib/pendulum/","page":"Pendulum Model","title":"Pendulum Model","text":"(Image: Pendulum animation)","category":"page"},{"location":"lib/pendulum/#NeuralLyapunovProblemLibrary.Pendulum","page":"Pendulum Model","title":"NeuralLyapunovProblemLibrary.Pendulum","text":"Pendulum(; driven = true, name, defaults)\n\nCreate an ODESystem representing a damped, driven or undriven pendulum, depending on the value of driven (defaults to true, i.e., driven pendulum).\n\nThe equation used in this model is ddotθ + 2 ζ ω_0 dotθ + ω_0^2 sin(θ) = τ where θ is the angle counter-clockwise from the downward equilibrium, ζ is the damping parameter, ω_0 is the resonant angular frequency, and τ is the input torque divided by the moment of inertia of the pendulum around the pivot (driven = false sets τ  0).\n\nThe name of the ODESystem is name.\n\nUsers may optionally provide default values of the parameters through defaults: a vector of the default values for [ζ, ω_0].\n\nExample\n\n@named pendulum = Pendulum(driven = false)\npendulum = structural_simplify(pendulum)\n\nx0 = π * rand(2)\np = rand(2)\nτ = 1 / prod(p)\n\nprob = ODEProblem(pendulum, x0, 15τ, p)\nsol = solve(prob, Tsit5())\n\n# Check that the undriven pendulum fell to the downward equilibrium\nθ_end, ω_end = sol.u[end]\nx_end, y_end = sin(θ_end), -cos(θ_end)\n\nsqrt(sum(abs2, [x_end, y_end, ω_end] .- [0, -1, 0])) < 1e-4\n\n\n\n\n\n","category":"function"},{"location":"lib/pendulum/#Copy-Pastable-Code","page":"Pendulum Model","title":"Copy-Pastable Code","text":"","category":"section"},{"location":"lib/pendulum/","page":"Pendulum Model","title":"Pendulum Model","text":"using Random; Random.seed!(200) # hide\nusing ModelingToolkit, NeuralLyapunovProblemLibrary, Plots, OrdinaryDiffEq\n\nx0 = [π * rand(), -π * rand()]\np = [0.5, 1]\n\n@named pendulum = Pendulum(driven = false)\npendulum = structural_simplify(pendulum)\nprob = ODEProblem(pendulum, x0, 15, p)\nsol = solve(prob, Tsit5())\n\ngif(plot_pendulum(sol); fps=50)","category":"page"},{"location":"lib/pendulum/#Plotting-the-Pendulum","page":"Pendulum Model","title":"Plotting the Pendulum","text":"","category":"section"},{"location":"lib/pendulum/#NeuralLyapunovProblemLibrary.plot_pendulum","page":"Pendulum Model","title":"NeuralLyapunovProblemLibrary.plot_pendulum","text":"plot_pendulum(θ, t; title)\nplot_pendulum(sol; title, N, angle_symbol)\n\nPlot the pendulum's trajectory.\n\nArguments\n\nθ: The angle of the pendulum at each time step.\nt: The time steps.\nsol: The solution to the ODE problem.\n\nKeyword arguments\n\ntitle: The title of the plot; defaults to no title (i.e., title=\"\").\nN: The number of points to plot; when using θ and t, uses length(t); defaults to 500 when using sol.\nangle_symbol: The symbol of the angle in sol; defaults to :θ.\n\n\n\n\n\n","category":"function"},{"location":"#NeuralLyapunov.jl","page":"Home","title":"NeuralLyapunov.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"NeuralLyapunov.jl is a library for searching for neural Lyapunov functions in Julia.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package provides an API for setting up the search for a neural Lyapunov function. Such a search can be formulated as a partial differential inequality, and this library generates a ModelingToolkit.jl PDESystem to be solved using NeuralPDE.jl. Since the Lyapunov conditions can be formulated in several different ways and a neural Lyapunov function can be set up in many different forms, this library presents an extensible interface for users to choose how they wish to set up the search, with useful pre-built options for common setups.","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If this is your first time using the library, start by familiarizing yourself with the components of a neural Lyapunov problem in NeuralLyapunov.jl. Then, you can dive in with any of the following demonstrations (the damped simple harmonic oscillator is recommended to begin):","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = Main.DEMONSTRATION_PAGES\nDepth = 1","category":"page"},{"location":"","page":"Home","title":"Home","text":"When you begin to write your own neural Lyapunov code, especially if you hope to define your own neural Lyapunov formulation, you may find any of the following manual pages useful:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = Main.MANUAL_PAGES","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finally, when you wish to test your neural Lyapunov code, you may wish to use one of the built-in example systems:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = Main.NEURALLYAPUNOVPROBLEMLIBRARY_PAGES","category":"page"}]
}
