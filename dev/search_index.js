var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = NeuralLyapunov","category":"page"},{"location":"#NeuralLyapunov","page":"Home","title":"NeuralLyapunov","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for NeuralLyapunov.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [NeuralLyapunov]","category":"page"},{"location":"#NeuralLyapunov.LyapunovDecreaseCondition","page":"Home","title":"NeuralLyapunov.LyapunovDecreaseCondition","text":"LyapunovDecreaseCondition(decrease, strength, checkfixedpoint)\n\nSpecifies the form of the Lyapunov conditions to be used; training will enforce     decrease(V, dVdt) ≤ strength(state, fixedpoint) The inequality will be approximated by the equation     relu(decrease(V, dVdt) - strength(state, fixedpoint)) = 0.0 If checkfixedpoint is false, then training assumes dVdt(fixedpoint) = 0, but if checkfixedpoint is true, then training will enforce dVdt(fixedpoint) = 0.\n\nIf the dynamics truly have a fixed point at fixedpoint and dVdt has been  defined properly in terms of the dynamics, then dVdt(fixedpoint) will be 0 and there is no need to enforce dVdt(fixedpoint) = 0, so checkfixed_point defaults to false.\n\nExamples:\n\nAsymptotic decrease can be enforced by requiring     dVdt ≤ -C |state - fixed_point|^2, which corresponds to     decrease = (V, dVdt) -> dVdt     strength = (x, x0) -> -C * (x - x0) ⋅ (x - x0)\n\nExponential decrease of rate k is proven by dVdt ≤ - k * V, so corresponds to     decrease = (V, dVdt) -> dVdt + k * V     strength = (x, x0) -> 0.0\n\n\n\n\n\n","category":"type"},{"location":"#NeuralLyapunov.LyapunovMinimizationCondition","page":"Home","title":"NeuralLyapunov.LyapunovMinimizationCondition","text":"LyapunovMinimizationCondition\n\nSpecifies the form of the Lyapunov conditions to be used.\n\nIf checknonnegativity is true, training will attempt to enforce     V(state) ≥ strength(state, fixedpoint) The inequality will be approximated by the equation     relu(strength(state, fixedpoint) - V(state)) = 0.0 If checkfixedpoint is true, then training will attempt to enforce      V(fixedpoint) = 0\n\nExamples\n\nThe condition that the Lyapunov function must be minimized uniquely at the fixed point can be represented as V(fixedpoint) = 0, V(state) > 0 when  state != fixedpoint. This could be enfored by V ≥ ||state - fixedpoint||^2, which would be represented, with checknonnegativity = true, by     strength(state, fixedpoint) = ||state - fixedpoint||^2, paired with V(fixedpoint) = 0, which can be enforced with      checkfixed_point = true\n\nIf V were structured such that it is always nonnegative, then V(fixedpoint) = 0 is all that must be enforced in training for the Lyapunov function to be  uniquely minimized at fixedpoint. So, in that case, we would use      checknonnegativity = false;  checkfixed_point = true\n\nIn either case, relu = (t) -> max(0.0, t) exactly represents the inequality,  but approximations of this function are allowed.\n\n\n\n\n\n","category":"type"},{"location":"#NeuralLyapunov.NeuralLyapunovStructure","page":"Home","title":"NeuralLyapunov.NeuralLyapunovStructure","text":"NeuralLyapunovStructure\n\nSpecifies the structure of the neural Lyapunov function and its derivative.\n\nAllows the user to define the Lyapunov in terms of the neural network to  structurally enforce Lyapunov conditions.  networkdim is the dimension of the output of the neural network. V(phi::Function, state, fixedpoint) takes in the neural network, the state, and the fixedpoint, and outputs the value of the Lyapunov function at state V̇(phi::Function, Jphi::Function, f::Function, state, fixedpoint) takes in the neural network, the jacobian of the neural network, the dynamics (as a function of the state alone), the state, and the fixedpoint, and outputs the time  derivative of the Lyapunov function at state.\n\n\n\n\n\n","category":"type"},{"location":"#NeuralLyapunov.AsymptoticDecrease-Tuple{}","page":"Home","title":"NeuralLyapunov.AsymptoticDecrease","text":"AsymptoticDecrease(strict; check_fixed_point, C)\n\nConstructs a LyapunovDecreaseCondition corresponding to asymptotic decrease.\n\nIf strict is false, the condition is dV/dt ≤ 0, and if strict is true, the  condition is dV/dt ≤ - C | state - fixed_point |^2\n\nThe inequality is represented by a ≥ b <==> relu(b-a) = 0.0\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.DontCheckDecrease","page":"Home","title":"NeuralLyapunov.DontCheckDecrease","text":"DontCheckDecrease(check_fixed_point = false)\n\nConstructs a LyapunovDecreaseCondition which represents not checking for  decrease of the Lyapunov function along system trajectories. This is appropriate in cases when the Lyapunov decrease condition has been structurally enforced.\n\nIt is still possible to check for dV/dt = 0 at fixed_point, even in this case.\n\n\n\n\n\n","category":"function"},{"location":"#NeuralLyapunov.DontCheckNonnegativity-Tuple{}","page":"Home","title":"NeuralLyapunov.DontCheckNonnegativity","text":"DontCheckNonnegativity(check_fixed_point)\n\nConstructs a LyapunovMinimizationCondition which represents not checking for  nonnegativity of the Lyapunov function. This is appropriate in cases where this condition has been structurally enforced.\n\nIt is still possible to check for V(fixedpoint) = 0, even in this case, for example if V is structured to be positive for state != fixedpoint, but it is not guaranteed structurally that V(fixed_point) = 0.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.ExponentialDecrease-Tuple{Real}","page":"Home","title":"NeuralLyapunov.ExponentialDecrease","text":"ExponentialDecrease(k, strict; check_fixed_point, C)\n\nConstructs a LyapunovDecreaseCondition corresponding to exponential decrease of rate k.\n\nIf strict is false, the condition is dV/dt ≤ -k * V, and if strict is true, the  condition is dV/dt ≤ -k * V - C * ||state - fixed_point||^2\n\nThe inequality is represented by a ≥ b <==> relu(b-a) = 0.0\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.NeuralLyapunovPDESystem-Tuple{SciMLBase.ODEFunction, Any, Any, NeuralLyapunovSpecification}","page":"Home","title":"NeuralLyapunov.NeuralLyapunovPDESystem","text":"NeuralLyapunovPDESystem(dynamics, lb, ub, spec; fixed_point, ps)\n\nConstructs a ModelingToolkit PDESystem to train a neural Lyapunov function\n\nReturns the PDESystem and a function representing the neural network, which operates columnwise.\n\nThe neural Lyapunov function will only be trained for { x : lb .≤ x .≤ ub }. The Lyapunov function will be for the dynamical system represented by dynamics If dynamics is an ODEProblem or ODEFunction, then the corresponding ODE; if dynamics is a function, then the ODE is ẋ = dynamics(x, p, t). This ODE should not depend on t (time t=0.0 alone will be used) and should have a fixed point at x = fixed_point. The particular Lyapunov conditions to be used and structure of the neural Lyapunov function are specified through spec, which is a NeuralLyapunovSpecification.\n\nThe returned neural network function takes three inputs: the neural network structure phi, the trained network parameters, and a matrix of inputs to operate on columnwise.\n\nIf dynamics requires parameters, their values can be supplied through the Vector p, or through dynamics.p if dynamics isa ODEProblem (in which case, let the other be SciMLBase.NullParameters()). If dynamics is an ODEFunction and dynamics.paramsyms is defined, then p should have the same order.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.NonnegativeNeuralLyapunov-Tuple{Integer}","page":"Home","title":"NeuralLyapunov.NonnegativeNeuralLyapunov","text":"NonnegativeNeuralLyapunov(network_dim, δ, pos_def; grad_pos_def, grad)\n\nCreates a NeuralLyapunovStructure where the Lyapunov function is the L2 norm of the neural network output plus a constant δ times a function pos_def.\n\nThe condition that the Lyapunov function must be minimized uniquely at the fixed point can be represented as V(fixedpoint) = 0, V(state) > 0 when  state != fixedpoint. This structure ensures V(state) ≥ 0. Further, if δ > 0 and posdef(fixedpoint, fixedpoint) = 0, but posdef(state, fixedpoint) > 0  when state != fixedpoint, this ensures that V(state) > 0 when  state != fixedpoint. This does not enforce V(fixedpoint) = 0, so that  condition must included in the neural Lyapunov loss function.\n\ngradposdef(state, fixedpoint) should be the gradient of posdef with respect to state at state. If gradposdef is not defined, it is evaluated using grad, which defaults to ForwardDiff.gradient.\n\nThe neural network output has dimension network_dim.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.NumericalNeuralLyapunovFunctions-Tuple{Any, Any, Function, Function, Function, Any}","page":"Home","title":"NeuralLyapunov.NumericalNeuralLyapunovFunctions","text":"NumericalNeuralLyapunovFunctions(phi, θ, network_func, V_structure, dynamics, fixed_point, grad)\n\nReturns the Lyapunov function, its time derivative, and its gradient: V(state), V̇(state), and ∇V(state)\n\nThese functions can operate on a state vector or columnwise on a matrix of state vectors. phi is the neural network with parameters θ. network_func is an output of NeuralLyapunovPDESystem.\n\nThe Lyapunov function structure is defined by     Vstructure(networkfunc, state, fixedpoint) Its gradient is calculated using grad, which defaults to ForwardDiff.gradient.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.NumericalNeuralLyapunovFunctions-Tuple{Any, Any, Function, NeuralLyapunovStructure, Function, Any}","page":"Home","title":"NeuralLyapunov.NumericalNeuralLyapunovFunctions","text":"NumericalNeuralLyapunovFunctions(phi, θ, network_func, structure, dynamics, fixed_point; jac, J_net)\n\nReturns the Lyapunov function, its time derivative, and its gradient: V(state), V̇(state), and ∇V(state)\n\nThese functions can operate on a state vector or columnwise on a matrix of state vectors. phi is the neural network with parameters θ. network_func(phi, θ, state) is an output of NeuralLyapunovPDESystem, which evaluates the neural network represented phi with parameters θ at state.\n\nThe Lyapunov function structure is specified in structure, which is a NeuralLyapunovStructure. The Jacobian of the network is either specified via Jnet(phi, _θ, state) or calculated using jac, which defaults to ForwardDiff.jacobian\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.PositiveSemiDefinite-Tuple{}","page":"Home","title":"NeuralLyapunov.PositiveSemiDefinite","text":"PositiveSemiDefinite(check_fixed_point)\n\nConstructs a LyapunovMinimizationCondition representing      V(state) ≥ 0 If checkfixedpoint is true, then training will also attempt to enforce      V(fixed_point) = 0\n\nThe inequality is represented by a ≥ b <==> relu(b-a) = 0.0\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.PositiveSemiDefiniteStructure-Tuple{Integer}","page":"Home","title":"NeuralLyapunov.PositiveSemiDefiniteStructure","text":"PositiveSemiDefiniteStructure(network_dim; pos_def, non_neg, grad_pos_def, grad_non_neg, grad)\n\nCreates a NeuralLyapunovStructure where the Lyapunov function is the product of a positive (semi-)definite function posdef which does not depend on the  network and a nonnegative function nonneg which does depend the network.\n\nThe condition that the Lyapunov function must be minimized uniquely at the fixed point can be represented as V(fixedpoint) = 0, V(state) > 0 when  state != fixedpoint. This structure ensures V(state) ≥ 0. Further, if posdef is 0 only at fixedpoint (and positive elsewhere) and if nonneg is strictly  positive away from fixedpoint (as is the case for the default values of posdef and nonneg), then this structure ensures V(fixedpoint) = 0 and V(state) > 0  when state != fixedpoint.\n\ngradposdef(state, fixedpoint) should be the gradient of posdef with respect to state at state. Similarly, gradnonneg(net, Jnet, state, fixedpoint)  should be the gradient of nonneg(net, state, fixedpoint) with respect to  state at state. If gradposdef or gradnonneg is not defined, it is evaluated  using grad, which defaults to ForwardDiff.gradient. \n\nThe neural network output has dimension network_dim.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.StrictlyPositiveDefinite-Tuple{}","page":"Home","title":"NeuralLyapunov.StrictlyPositiveDefinite","text":"StrictlyPositiveDefinite(C; check_fixed_point, relu)\n\nConstructs a LyapunovMinimizationCondition representing      V(state) ≥ C * ||state - fixedpoint||^2 If checkfixedpoint is true, then training will also attempt to enforce      V(fixedpoint) = 0\n\nThe inequality is represented by a ≥ b <==> relu(b-a) = 0.0\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.UnstructuredNeuralLyapunov-Tuple{}","page":"Home","title":"NeuralLyapunov.UnstructuredNeuralLyapunov","text":"UnstructuredNeuralLyapunov()\n\nCreates a NeuralLyapunovStructure where the Lyapunov function is the neural network evaluated at state. This does not structurally enforce any Lyapunov  conditions.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.check_decrease-Tuple{NeuralLyapunov.AbstractLyapunovDecreaseCondition}","page":"Home","title":"NeuralLyapunov.check_decrease","text":"check_decrease(cond::AbstractLyapunovDecreaseCondition)\n\nTrue if cond specifies training to meet the Lyapunov decrease condition, false if cond specifies no training to meet this condition.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.check_fixed_point-Tuple{NeuralLyapunov.AbstractLyapunovMinimizationCondition}","page":"Home","title":"NeuralLyapunov.check_fixed_point","text":"check_fixed_point(cond::AbstractLyapunovMinimizationCondition)\n\nTrue if cond specifies training for the Lyapunov function to equal zero at the fixed point, false if cond specifies no training to meet this condition.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.check_nonnegativity-Tuple{NeuralLyapunov.AbstractLyapunovMinimizationCondition}","page":"Home","title":"NeuralLyapunov.check_nonnegativity","text":"check_nonnegativity(cond::AbstractLyapunovMinimizationCondition)\n\nTrue if cond specifies training to meet the Lyapunov minimization condition,  false if cond specifies no training to meet this condition.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.check_stationary_fixed_point-Tuple{NeuralLyapunov.AbstractLyapunovDecreaseCondition}","page":"Home","title":"NeuralLyapunov.check_stationary_fixed_point","text":"check_stationary_fixed_point(cond::AbstractLyapunovDecreaseCondition)\n\nTrue if cond specifies training for the Lyapunov function not to change at the  fixed point, false if cond specifies no training to meet this condition.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.get_decrease_condition-Tuple{NeuralLyapunov.AbstractLyapunovDecreaseCondition}","page":"Home","title":"NeuralLyapunov.get_decrease_condition","text":"get_decrease_condition(cond::AbstractLyapunovDecreaseCondition)\n\nReturns a function of V, dVdt, state, and fixed_point that is equal to zero when the Lyapunov decrease condition is met and greater than zero when it is  violated\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.get_minimization_condition-Tuple{NeuralLyapunov.AbstractLyapunovMinimizationCondition}","page":"Home","title":"NeuralLyapunov.get_minimization_condition","text":"get_minimization_condition(cond::AbstractLyapunovMinimizationCondition)\n\nReturns a function of V, state, and fixed_point that equals zero when the  Lyapunov minimization condition is met and greater than zero when it's violated\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.local_Lyapunov-Tuple{Function, Any, Any}","page":"Home","title":"NeuralLyapunov.local_Lyapunov","text":"get_local_Lyapunov(dynamics, state_dim; fixed_point, dynamics_jac)\n\nUses semidefinite programming to derive a quadratic Lyapunov function for the linearization of dynamics around fixed_point. Returns (V, dV/dt, ∇V).\n\nIf dynamicsjac is nothing, the Jacobian of the dynamics is calculated using  ForwardDiff. Other allowable forms are a function which takes in the state and outputs the jacobian of dynamics or an AbstractMatrix representing the Jacobian at fixedpoint. If fixed_point is not specified, it defaults to the origin.\n\n\n\n\n\n","category":"method"}]
}
