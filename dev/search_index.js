var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = NeuralLyapunov","category":"page"},{"location":"#NeuralLyapunov","page":"Home","title":"NeuralLyapunov","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for NeuralLyapunov.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [NeuralLyapunov]","category":"page"},{"location":"#NeuralLyapunov.AbstractLyapunovDecreaseCondition","page":"Home","title":"NeuralLyapunov.AbstractLyapunovDecreaseCondition","text":"AbstractLyapunovDecreaseCondition\n\nRepresents the decrease condition in a neural Lyapunov problem\n\nAll concrete AbstractLyapunovDecreaseCondition subtypes should define the check_decrease, check_stationary_fixed_point, and get_decrease_condition functions.\n\n\n\n\n\n","category":"type"},{"location":"#NeuralLyapunov.AbstractLyapunovMinimizationCondition","page":"Home","title":"NeuralLyapunov.AbstractLyapunovMinimizationCondition","text":"AbstractLyapunovMinimizationCondition\n\nRepresents the minimization condition in a neural Lyapunov problem\n\nAll concrete AbstractLyapunovMinimizationCondition subtypes should define the check_nonnegativity, check_fixed_point, and get_minimization_condition functions.\n\n\n\n\n\n","category":"type"},{"location":"#NeuralLyapunov.LyapunovDecreaseCondition","page":"Home","title":"NeuralLyapunov.LyapunovDecreaseCondition","text":"LyapunovDecreaseCondition(check_decrease, decrease, strength, relu, check_fixed_point)\n\nSpecifies the form of the Lyapunov conditions to be used; if check_decrease, training will enforce decrease(V, dVdt) ≤ strength(state, fixed_point).\n\nThe inequality will be approximated by the equation     relu(decrease(V, dVdt) - strength(state, fixed_point)) = 0.0. If check_fixed_point is false, then training assumes dVdt(fixed_point) = 0, but if check_fixed_point is true, then training will enforce dVdt(fixed_point) = 0.\n\nIf the dynamics truly have a fixed point at fixed_point and dVdt has been defined properly in terms of the dynamics, then dVdt(fixed_point) will be 0 and there is no need to enforce dVdt(fixed_point) = 0, so check_fixed_point defaults to false.\n\nExamples:\n\nAsymptotic decrease can be enforced by requiring     dVdt ≤ -C |state - fixed_point|^2, which corresponds to     decrease = (V, dVdt) -> dVdt     strength = (x, x0) -> -C * (x - x0) ⋅ (x - x0)\n\nExponential decrease of rate k is proven by dVdt ≤ - k * V, so corresponds to     decrease = (V, dVdt) -> dVdt + k * V     strength = (x, x0) -> 0.0\n\n\n\n\n\n","category":"type"},{"location":"#NeuralLyapunov.LyapunovMinimizationCondition","page":"Home","title":"NeuralLyapunov.LyapunovMinimizationCondition","text":"LyapunovMinimizationCondition\n\nSpecifies the form of the Lyapunov conditions to be used.\n\nIf check_nonnegativity is true, training will attempt to enforce     V(state) ≥ strength(state, fixed_point). The inequality will be approximated by the equation     relu(strength(state, fixed_point) - V(state)) = 0.0. If check_fixed_point is true, then training will also attempt to enforce     V(fixed_point) = 0.\n\nExamples\n\nThe condition that the Lyapunov function must be minimized uniquely at the fixed point can be represented as V(fixed_point) = 0, V(state) > 0 when state ≠ fixed_point. This could be enfored by V(fixed_point) ≥ ||state - fixed_point||^2, which would be represented, with check_nonnegativity = true, by     strength(state, fixedpoint) = ||state - fixedpoint||^2, paired with V(fixed_point) = 0, which can be enforced with check_fixed_point = true.\n\nIf V were structured such that it is always nonnegative, then V(fixed_point) = 0 is all that must be enforced in training for the Lyapunov function to be uniquely minimized at fixed_point. So, in that case, we would use     check_nonnegativity = false;  check_fixed_point = true.\n\nIn either case, relu = (t) -> max(0.0, t) exactly represents the inequality, but approximations of this function are allowed.\n\n\n\n\n\n","category":"type"},{"location":"#NeuralLyapunov.NeuralLyapunovSpecification","page":"Home","title":"NeuralLyapunov.NeuralLyapunovSpecification","text":"NeuralLyapunovSpecification\n\nSpecifies a neural Lyapunov problem\n\n\n\n\n\n","category":"type"},{"location":"#NeuralLyapunov.NeuralLyapunovStructure","page":"Home","title":"NeuralLyapunov.NeuralLyapunovStructure","text":"NeuralLyapunovStructure\n\nSpecifies the structure of the neural Lyapunov function and its derivative.\n\nAllows the user to define the Lyapunov in terms of the neural network to structurally enforce Lyapunov conditions. network_dim is the dimension of the output of the neural network. V(phi::Function, state, fixed_point) takes in the neural network, the state, and the fixed point, and outputs the value of the Lyapunov function at state. V̇(phi::Function, J_phi::Function, f::Function, state, fixed_point) takes in the neural network, the jacobian of the neural network, the dynamics (as a function of the state alone), the state, and the fixed point, and outputs the time derivative of the Lyapunov function at state.\n\n\n\n\n\n","category":"type"},{"location":"#NeuralLyapunov.RoAAwareDecreaseCondition","page":"Home","title":"NeuralLyapunov.RoAAwareDecreaseCondition","text":"RoAAwareDecreaseCondition(check_decrease, decrease, strength, relu, check_fixed_point,\n                          ρ, out_of_RoA_penalty)\n\nSpecifies the form of the Lyapunov conditions to be used, training for a region of attraction estimate of { x : V(x) ≤ ρ }\n\nIf check_decrease, training will enforce decrease(V(state), dVdt(state)) ≤ strength(state, fixed_point) whenever V(state) ≤ ρ, and will instead apply |out_of_RoA_penalty(V(state), dVdt(state), state, fixed_point, ρ)|^2 when V(state) > ρ.\n\nThe inequality will be approximated by the equation     relu(decrease(V, dVdt) - strength(state, fixed_point)) = 0.0. If check_fixed_point is false, then training assumes dVdt(fixed_point) = 0, but if check_fixed_point is true, then training will attempt to enforce dVdt(fixed_point) = 0.\n\nIf the dynamics truly have a fixed point at fixed_point and dVdt has been defined properly in terms of the dynamics, then dVdt(fixed_point) will be 0 and there is no need to enforce dVdt(fixed_point) = 0, so check_fixed_point defaults to false.\n\nExamples:\n\nAsymptotic decrease can be enforced by requiring     dVdt ≤ -C |state - fixed_point|^2, which corresponds to     decrease = (V, dVdt) -> dVdt and     strength = (x, x0) -> -C * (x - x0) ⋅ (x - x0).\n\nExponential decrease of rate k is proven by dVdt ≤ - k * V, so corresponds to     decrease = (V, dVdt) -> dVdt + k * V and     strength = (x, x0) -> 0.0.\n\nEnforcing either condition only in the region of attraction and not penalizing any points outside that region would correspond to     out_of_RoA_penalty = (V, dVdt, state, fixed_point, ρ) -> 0.0, whereas an example of a penalty that decays farther in state space from the fixed point is     out_of_RoA_penalty = (V, dVdt, state, fixed_point, ρ) -> 1.0 / ((x - x0) ⋅ (x - x0)). Note that this penalty could also depend on values of V, dVdt, and ρ.\n\n\n\n\n\n","category":"type"},{"location":"#NeuralLyapunov.AsymptoticDecrease-Tuple{}","page":"Home","title":"NeuralLyapunov.AsymptoticDecrease","text":"AsymptoticDecrease(strict; check_fixed_point, C)\n\nConstructs a LyapunovDecreaseCondition corresponding to asymptotic decrease.\n\nIf strict is false, the condition is dV/dt ≤ 0, and if strict is true, the condition is dV/dt ≤ - C | state - fixed_point |^2.\n\nThe inequality is represented by a ≥ b <==> relu(b-a) = 0.0.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.DontCheckDecrease","page":"Home","title":"NeuralLyapunov.DontCheckDecrease","text":"DontCheckDecrease(check_fixed_point = false)\n\nConstructs a LyapunovDecreaseCondition which represents not checking for decrease of the Lyapunov function along system trajectories. This is appropriate in cases when the Lyapunov decrease condition has been structurally enforced.\n\nIt is still possible to check for dV/dt = 0 at fixed_point, even in this case.\n\n\n\n\n\n","category":"function"},{"location":"#NeuralLyapunov.DontCheckNonnegativity-Tuple{}","page":"Home","title":"NeuralLyapunov.DontCheckNonnegativity","text":"DontCheckNonnegativity(check_fixed_point)\n\nConstructs a LyapunovMinimizationCondition which represents not checking for nonnegativity of the Lyapunov function. This is appropriate in cases where this condition has been structurally enforced.\n\nIt is still possible to check for V(fixed_point) = 0, even in this case, for example if V is structured to be positive for state ≠ fixed_point, but it is not guaranteed structurally that V(fixed_point) = 0.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.ExponentialDecrease-Tuple{Real}","page":"Home","title":"NeuralLyapunov.ExponentialDecrease","text":"ExponentialDecrease(k, strict; check_fixed_point, C)\n\nConstructs a LyapunovDecreaseCondition corresponding to exponential decrease of rate k.\n\nIf strict is false, the condition is dV/dt ≤ -k * V, and if strict is true, the condition is dV/dt ≤ -k * V - C * ||state - fixed_point||^2.\n\nThe inequality is represented by a ≥ b <==> relu(b-a) = 0.0.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.NeuralLyapunovPDESystem-Tuple{SciMLBase.ODEFunction, Any, Any, NeuralLyapunovSpecification}","page":"Home","title":"NeuralLyapunov.NeuralLyapunovPDESystem","text":"NeuralLyapunovPDESystem(dynamics, lb, ub, spec; fixed_point, ps)\n\nConstructs a ModelingToolkit PDESystem to train a neural Lyapunov function\n\nReturns the PDESystem and a function representing the neural network, which operates columnwise.\n\nThe neural Lyapunov function will only be trained for { x : lb .≤ x .≤ ub }. The Lyapunov function will be for the dynamical system represented by dynamics. If dynamics is an ODEProblem or ODEFunction, then the corresponding ODE; if dynamics is a function, then the ODE is ẋ = dynamics(x, p, t). This ODE should not depend on t (time t=0.0 alone will be used) and should have a fixed point at x = fixed_point. The particular Lyapunov conditions to be used and structure of the neural Lyapunov function are specified through spec, which is a NeuralLyapunovSpecification.\n\nThe returned neural network function takes three inputs: the neural network structure phi, the trained network parameters, and a matrix of inputs to operate on columnwise.\n\nIf dynamics requires parameters, their values can be supplied through the Vector p, or through the parameters of dynamics if dynamics isa ODEProblem (in which case, let the other be SciMLBase.NullParameters()). If dynamics is an ODEFunction and dynamics.paramsyms is defined, then p should have the same order.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.NonnegativeNeuralLyapunov-Tuple{Integer}","page":"Home","title":"NeuralLyapunov.NonnegativeNeuralLyapunov","text":"NonnegativeNeuralLyapunov(network_dim, δ, pos_def; grad_pos_def, grad)\n\nCreates a NeuralLyapunovStructure where the Lyapunov function is the L2 norm of the neural network output plus a constant δ times a function pos_def.\n\nThe condition that the Lyapunov function must be minimized uniquely at the fixed point can be represented as V(fixed_point) = 0, V(state) > 0 when state ≠ fixed_point. This structure ensures V(state) ≥ 0. Further, if δ > 0 and pos_def(fixed_point, fixed_point) = 0, but pos_def(state, fixed_point) > 0 when state ≠ fixed_point, this ensures that V(state) > 0 when state != fixed_point. This does not enforce V(fixed_point) = 0, so that condition must included in the neural Lyapunov loss function.\n\ngrad_pos_def(state, fixed_point) should be the gradient of pos_def with respect to state at state. If grad_pos_def is not defined, it is evaluated using grad, which defaults to ForwardDiff.gradient.\n\nThe neural network output has dimension network_dim.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.NumericalNeuralLyapunovFunctions-Tuple{Any, Any, Function, Function, Function, Any}","page":"Home","title":"NeuralLyapunov.NumericalNeuralLyapunovFunctions","text":"NumericalNeuralLyapunovFunctions(phi, θ, network_func, V_structure, dynamics,\n                                 fixed_point, grad)\n\nReturns the Lyapunov function, its time derivative, and its gradient: V(state), V̇(state), and ∇V(state).\n\nThese functions can operate on a state vector or columnwise on a matrix of state vectors. phi is the neural network with parameters θ. network_func is an output of NeuralLyapunovPDESystem.\n\nThe Lyapunov function structure is defined by     V_structure(_network_func, state, fixed_point) Its gradient is calculated using grad, which defaults to ForwardDiff.gradient.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.NumericalNeuralLyapunovFunctions-Tuple{Any, Any, Function, NeuralLyapunovStructure, Function, Any}","page":"Home","title":"NeuralLyapunov.NumericalNeuralLyapunovFunctions","text":"NumericalNeuralLyapunovFunctions(phi, θ, network_func, structure, dynamics, fixed_point;\n                                 jac, J_net)\n\nReturns the Lyapunov function, its time derivative, and its gradient: V(state), V̇(state), and ∇V(state)\n\nThese functions can operate on a state vector or columnwise on a matrix of state vectors. phi is the neural network with parameters θ. network_func(phi, θ, state) is an output of NeuralLyapunovPDESystem, which evaluates the neural network represented by phi with parameters θ at state.\n\nThe Lyapunov function structure is specified in structure, which is a NeuralLyapunovStructure. The Jacobian of the network is either specified via J_net(_phi, _θ, state) or calculated using jac, which defaults to ForwardDiff.jacobian.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.PositiveSemiDefinite-Tuple{}","page":"Home","title":"NeuralLyapunov.PositiveSemiDefinite","text":"PositiveSemiDefinite(check_fixed_point)\n\nConstructs a LyapunovMinimizationCondition representing     V(state) ≥ 0. If check_fixed_point is true, then training will also attempt to enforce     V(fixed_point) = 0.\n\nThe inequality is represented by a ≥ b <==> relu(b-a) = 0.0.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.PositiveSemiDefiniteStructure-Tuple{Integer}","page":"Home","title":"NeuralLyapunov.PositiveSemiDefiniteStructure","text":"PositiveSemiDefiniteStructure(network_dim; pos_def, non_neg, grad_pos_def, grad_non_neg, grad)\n\nCreates a NeuralLyapunovStructure where the Lyapunov function is the product of a positive (semi-)definite function pos_def which does not depend on the network and a nonnegative function non_neg which does depend the network.\n\nThe condition that the Lyapunov function must be minimized uniquely at the fixed point can be represented as V(fixed_point) = 0, V(state) > 0 when state ≠ fixed_point. This structure ensures V(state) ≥ 0. Further, if pos_def is 0 only at fixed_point (and positive elsewhere) and if non_neg is strictly positive away from fixed_point (as is the case for the default values of pos_def and non_neg), then this structure ensures V(fixed_point) = 0 and V(state) > 0 when state ≠ fixed_point.\n\ngrad_pos_def(state, fixed_point) should be the gradient of pos_def with respect to state at state. Similarly, grad_non_neg(net, J_net, state, fixed_point) should be the gradient of non_neg(net, state, fixed_point) with respect to state at state. If grad_pos_def or grad_non_neg is not defined, it is evaluated using grad, which defaults to ForwardDiff.gradient.\n\nThe neural network output has dimension network_dim.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.StrictlyPositiveDefinite-Tuple{}","page":"Home","title":"NeuralLyapunov.StrictlyPositiveDefinite","text":"StrictlyPositiveDefinite(C; check_fixed_point, relu)\n\nConstructs a LyapunovMinimizationCondition representing     V(state) ≥ C * ||state - fixed_point||^2. If check_fixed_point is true, then training will also attempt to enforce     V(fixed_point) = 0.\n\nThe inequality is represented by a ≥ b <==> relu(b-a) = 0.0.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.UnstructuredNeuralLyapunov-Tuple{}","page":"Home","title":"NeuralLyapunov.UnstructuredNeuralLyapunov","text":"UnstructuredNeuralLyapunov()\n\nCreates a NeuralLyapunovStructure where the Lyapunov function is the neural network evaluated at the state. This does not structurally enforce any Lyapunov conditions.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.check_decrease-Tuple{NeuralLyapunov.AbstractLyapunovDecreaseCondition}","page":"Home","title":"NeuralLyapunov.check_decrease","text":"check_decrease(cond::AbstractLyapunovDecreaseCondition)\n\ntrue if cond specifies training to meet the Lyapunov decrease condition, false if cond specifies no training to meet this condition.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.check_fixed_point-Tuple{NeuralLyapunov.AbstractLyapunovMinimizationCondition}","page":"Home","title":"NeuralLyapunov.check_fixed_point","text":"check_fixed_point(cond::AbstractLyapunovMinimizationCondition)\n\ntrue if cond specifies training for the Lyapunov function to equal zero at the fixed point, false if cond specifies no training to meet this condition.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.check_nonnegativity-Tuple{NeuralLyapunov.AbstractLyapunovMinimizationCondition}","page":"Home","title":"NeuralLyapunov.check_nonnegativity","text":"check_nonnegativity(cond::AbstractLyapunovMinimizationCondition)\n\ntrue if cond specifies training to meet the Lyapunov minimization condition, false if cond specifies no training to meet this condition.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.check_stationary_fixed_point-Tuple{NeuralLyapunov.AbstractLyapunovDecreaseCondition}","page":"Home","title":"NeuralLyapunov.check_stationary_fixed_point","text":"check_stationary_fixed_point(cond::AbstractLyapunovDecreaseCondition)\n\ntrue if cond specifies training for the Lyapunov function not to change at the fixed point, false if cond specifies no training to meet this condition.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.get_decrease_condition-Tuple{NeuralLyapunov.AbstractLyapunovDecreaseCondition}","page":"Home","title":"NeuralLyapunov.get_decrease_condition","text":"get_decrease_condition(cond::AbstractLyapunovDecreaseCondition)\n\nReturns a function of V, dVdt, state, and fixed_point that is equal to zero when the Lyapunov decrease condition is met and greater than zero when it is violated.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.get_minimization_condition-Tuple{NeuralLyapunov.AbstractLyapunovMinimizationCondition}","page":"Home","title":"NeuralLyapunov.get_minimization_condition","text":"get_minimization_condition(cond::AbstractLyapunovMinimizationCondition)\n\nReturns a function of V, state, and fixed_point that equals zero when the Lyapunov minimization condition is met and greater than zero when it's violated.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.local_Lyapunov-Tuple{Function, Any, Any}","page":"Home","title":"NeuralLyapunov.local_Lyapunov","text":"get_local_Lyapunov(dynamics, state_dim; fixed_point, dynamics_jac)\n\nUses semidefinite programming to derive a quadratic Lyapunov function for the linearization of dynamics around fixed_point. Returns (V, dV/dt, ∇V).\n\nIf dynamicsjac is nothing, the Jacobian of the dynamics is calculated using  ForwardDiff. Other allowable forms are a function which takes in the state and outputs the jacobian of dynamics or an AbstractMatrix representing the Jacobian at fixedpoint. If fixed_point is not specified, it defaults to the origin.\n\n\n\n\n\n","category":"method"},{"location":"#NeuralLyapunov.make_RoA_aware-Tuple{LyapunovDecreaseCondition}","page":"Home","title":"NeuralLyapunov.make_RoA_aware","text":"make_RoA_aware(cond; ρ, out_of_RoA_penalty)\n\nAdds awareness of the region of attraction (RoA) estimation task to the supplied LyapunovDecreaseCondition\n\nρ is the target level such that the RoA will be { x : V(x) ≤ ρ }. cond specifies the loss applied when V(state) ≤ ρ, and |out_of_RoA_penalty(V(state), dVdt(state), state, fixed_point, ρ)|^2 is the loss from state values such that V(state) > ρ.\n\n\n\n\n\n","category":"method"}]
}
