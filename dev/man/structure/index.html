<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Structuring a Neural Lyapunov function · NeuralLyapunov.jl</title><meta name="title" content="Structuring a Neural Lyapunov function · NeuralLyapunov.jl"/><meta property="og:title" content="Structuring a Neural Lyapunov function · NeuralLyapunov.jl"/><meta property="twitter:title" content="Structuring a Neural Lyapunov function · NeuralLyapunov.jl"/><meta name="description" content="Documentation for NeuralLyapunov.jl."/><meta property="og:description" content="Documentation for NeuralLyapunov.jl."/><meta property="twitter:description" content="Documentation for NeuralLyapunov.jl."/><meta property="og:url" content="https://SciML.github.io/NeuralLyapunov.jl/man/structure/"/><meta property="twitter:url" content="https://SciML.github.io/NeuralLyapunov.jl/man/structure/"/><link rel="canonical" href="https://SciML.github.io/NeuralLyapunov.jl/man/structure/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralLyapunov.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../">Components of a Neural Lyapunov Problem</a></li><li><a class="tocitem" href="../pdesystem/">Solving a Neural Lyapunov Problem</a></li><li><a class="tocitem" href="../minimization/">Lyapunov Minimization Condition</a></li><li><a class="tocitem" href="../decrease/">Lyapunov Decrease Condition</a></li><li class="is-active"><a class="tocitem" href>Structuring a Neural Lyapunov function</a><ul class="internal"><li><a class="tocitem" href="#Pre-defined-NeuralLyapunov-transformations"><span>Pre-defined NeuralLyapunov transformations</span></a></li><li><a class="tocitem" href="#Pre-defined-Lux-structures"><span>Pre-defined Lux structures</span></a></li><li><a class="tocitem" href="#Defining-your-own-neural-Lyapunov-function-structure-with-[NeuralLyapunovStructure](@ref)"><span>Defining your own neural Lyapunov function structure with <code>NeuralLyapunovStructure</code></span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../roa/">Training for Region of Attraction Identification</a></li><li><a class="tocitem" href="../policy_search/">Policy Search and Network-Dependent Dynamics</a></li><li><a class="tocitem" href="../local_lyapunov/">Local Lyapunov analysis</a></li></ul></li><li><span class="tocitem">Demonstrations</span><ul><li><a class="tocitem" href="../../demos/damped_SHO/">Damped Simple Harmonic Oscillator</a></li><li><a class="tocitem" href="../../demos/roa_estimation/">Estimating the Region of Attraction</a></li><li><a class="tocitem" href="../../demos/policy_search/">Policy Search on the Driven Inverted Pendulum</a></li><li><a class="tocitem" href="../../demos/benchmarking/">Benchmarking a neural Lyapunov method</a></li></ul></li><li><span class="tocitem">Test Problem Library</span><ul><li><a class="tocitem" href="../../lib/">NeuralLyapunovProblemLibrary.jl</a></li><li><a class="tocitem" href="../../lib/pendulum/">Pendulum Model</a></li><li><a class="tocitem" href="../../lib/double_pendulum/">Double Pendulum Model</a></li><li><a class="tocitem" href="../../lib/quadrotor/">Quadrotor Models</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Structuring a Neural Lyapunov function</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Structuring a Neural Lyapunov function</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/NeuralLyapunov.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/NeuralLyapunov.jl/blob/master/docs/src/man/structure.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Structuring-a-Neural-Lyapunov-function"><a class="docs-heading-anchor" href="#Structuring-a-Neural-Lyapunov-function">Structuring a Neural Lyapunov function</a><a id="Structuring-a-Neural-Lyapunov-function-1"></a><a class="docs-heading-anchor-permalink" href="#Structuring-a-Neural-Lyapunov-function" title="Permalink"></a></h1><p>Users have two ways to specify the structure of their neural Lyapunov function: through Lux and through NeuralLyapunov. NeuralLyapunov.jl is intended for use with <a href="https://github.com/SciML/NeuralPDE.jl">NeuralPDE.jl</a>, which is itself intended for use with <a href="https://github.com/LuxDL/Lux.jl">Lux.jl</a>. Users therefore must provide a Lux model representing the neural network <span>$\phi(x)$</span> which will be trained, regardless of the transformation they use to go from <span>$\phi$</span> to <span>$V$</span>.</p><p>In some cases, users will find it simplest to make <span>$\phi(x)$</span> a simple multilayer perceptron and specify their neural Lyapunov function structure as a transformation of the function <span>$\phi$</span> to the function <span>$V$</span> using the <a href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunovStructure</code></a> struct, detailed below.</p><p>In other cases (particularly when they find the NeuralPDE parser has trouble tracing their structure), users may wish to represent their neuralLyapunov function structure using <a href="https://github.com/LuxDL/Lux.jl">Lux.jl</a>/<a href="https://github.com/LuxDL/Boltz.jl">Boltz.jl</a> layers, integrating them into <span>$\phi(x)$</span> and letting <span>$V(x) = \phi(x)$</span> (as in <a href="#NeuralLyapunov.NoAdditionalStructure"><code>NoAdditionalStructure</code></a>, detailed below).</p><p>Users may also combine the two methods, particularly if they find that their structure can be broken down into a component that the NeuralPDE parser has trouble tracing but exists in Lux/Boltz, and another aspect that can be written easily using a <a href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunovStructure</code></a> but does not correspond any existing Lux/Boltz layer. (Such an example will be provided below.)</p><p>NeuralLyapunov.jl supplies two Lux structures and two pooling layers for structuring <span>$\phi(x)$</span>, along with three <a href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunovStructure</code></a> transformations. Additionally, users can always specify a custom structure using the <a href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunovStructure</code></a> struct.</p><h2 id="Pre-defined-NeuralLyapunov-transformations"><a class="docs-heading-anchor" href="#Pre-defined-NeuralLyapunov-transformations">Pre-defined NeuralLyapunov transformations</a><a id="Pre-defined-NeuralLyapunov-transformations-1"></a><a class="docs-heading-anchor-permalink" href="#Pre-defined-NeuralLyapunov-transformations" title="Permalink"></a></h2><p>The simplest structure is to train the neural network directly to be the Lyapunov function, which can be accomplished using an <a href="#NeuralLyapunov.NoAdditionalStructure"><code>NoAdditionalStructure</code></a>. This is particularly useful with the pre-defined Lux structures detailed in the following section.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralLyapunov.NoAdditionalStructure" href="#NeuralLyapunov.NoAdditionalStructure"><code>NeuralLyapunov.NoAdditionalStructure</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NoAdditionalStructure()</code></pre><p>Create a <a href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunovStructure</code></a> where the Lyapunov function is the neural network evaluated at the state. This does impose any additional structure to enforce any Lyapunov conditions.</p><p>Corresponds to <span>$V(x) = ϕ(x)$</span>, where <span>$ϕ$</span> is the neural network.</p><p>Dynamics are assumed to be in <code>f(state, p, t)</code> form, as in an <code>ODEFunction</code>. For <code>f(state, input, p, t)</code>, consider using <a href="../policy_search/#NeuralLyapunov.add_policy_search"><code>add_policy_search</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/bfe41018da3e51fc7948e4208ae41d5538aaf8bd/src/structure_specification.jl#L1-L12">source</a></section></article><p>The condition that the Lyapunov function <span>$V(x)$</span> must be minimized uniquely at the fixed point <span>$x_0$</span> is often represented as a requirement for <span>$V(x)$</span> to be positive away from the fixed point and zero at the fixed point. Put mathematically, it is sufficient to require <span>$V(x) &gt; 0 \, \forall x \ne x_0$</span> and <span>$V(x_0) = 0$</span>. We call such functions positive definite (around the fixed point <span>$x_0$</span>).</p><p>Two structures are provided which partially or fully enforce the minimization condition: <a href="#NeuralLyapunov.NonnegativeStructure"><code>NonnegativeStructure</code></a>, which structurally enforces <span>$V(x) \ge 0$</span> everywhere, and <a href="#NeuralLyapunov.PositiveSemiDefiniteStructure"><code>PositiveSemiDefiniteStructure</code></a>, which additionally enforces <span>$V(x_0) = 0$</span>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralLyapunov.NonnegativeStructure" href="#NeuralLyapunov.NonnegativeStructure"><code>NeuralLyapunov.NonnegativeStructure</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NonnegativeStructure(network_dim; &lt;keyword_arguments&gt;)</code></pre><p>Create a <a href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunovStructure</code></a> where the Lyapunov function is the L2 norm of the neural network output plus a constant δ times a function <code>pos_def</code>.</p><p>Corresponds to <span>$V(x) = \lVert ϕ(x) \rVert^2 + δ \, \texttt{pos\_def}(x, x_0)$</span>, where <span>$ϕ$</span> is the neural network and <span>$x_0$</span> is the equilibrium point.</p><p>This structure ensures <span>$V(x) ≥ 0 \, ∀ x$</span> when <span>$δ ≥ 0$</span> and <code>pos_def</code> is always nonnegative. Further, if <span>$δ &gt; 0$</span> and <code>pos_def</code> is strictly positive definite around <code>fixed_point</code>, the structure ensures that <span>$V(x)$</span> is strictly positive away from <code>fixed_point</code>. In such cases, the minimization condition reduces to ensuring <span>$V(x_0) = 0$</span>, and so <a href="../minimization/#NeuralLyapunov.DontCheckNonnegativity"><code>DontCheckNonnegativity(true)</code></a> should be used.</p><p><strong>Arguments</strong></p><ul><li><code>network_dim</code>: output dimensionality of the neural network.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>δ</code>: weight of <code>pos_def</code>, as above; defaults to 0.</li><li><code>pos_def(state, fixed_point)</code>: a function that is positive (semi-)definite in <code>state</code> around <code>fixed_point</code>; defaults to <span>$\log(1 + \lVert x - x_0 \rVert^2)$</span>.</li><li><code>grad_pos_def(state, fixed_point)</code>: the gradient of <code>pos_def</code> with respect to <code>state</code> at <code>state</code>. If <code>isnothing(grad_pos_def)</code> (as is the default), the gradient of <code>pos_def</code> will be evaluated using <code>grad</code>.</li><li><code>grad</code>: a function for evaluating gradients to be used when <code>isnothing(grad_pos_def)</code>; defaults to, and expects the same arguments as, <code>ForwardDiff.gradient</code>.</li></ul><p>Dynamics are assumed to be in <code>f(state, p, t)</code> form, as in an <code>ODEFunction</code>. For <code>f(state, input, p, t)</code>, consider using <a href="../policy_search/#NeuralLyapunov.add_policy_search"><code>add_policy_search</code></a>.</p><p>See also: <a href="../minimization/#NeuralLyapunov.DontCheckNonnegativity"><code>DontCheckNonnegativity</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/bfe41018da3e51fc7948e4208ae41d5538aaf8bd/src/structure_specification.jl#L22-L55">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralLyapunov.PositiveSemiDefiniteStructure" href="#NeuralLyapunov.PositiveSemiDefiniteStructure"><code>NeuralLyapunov.PositiveSemiDefiniteStructure</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PositiveSemiDefiniteStructure(network_dim; &lt;keyword_arguments&gt;)</code></pre><p>Create a <a href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunovStructure</code></a> where the Lyapunov function is the product of a positive (semi-)definite function <code>pos_def</code> which does not depend on the network and a nonnegative function <code>non_neg</code> which does depend the network.</p><p>Corresponds to <span>$V(x) = \texttt{pos\_def}(x, x_0) * \texttt{non\_neg}(ϕ, x, x_0)$</span>, where <span>$ϕ$</span> is the neural network and <span>$x_0$</span> is the equilibrium point.</p><p>This structure ensures <span>$V(x) ≥ 0$</span>. Further, if <code>pos_def</code> is strictly positive definite <code>fixed_point</code> and <code>non_neg</code> is strictly positive (as is the case for the default values of <code>pos_def</code> and <code>non_neg</code>), then this structure ensures <span>$V(x)$</span> is strictly positive definite around <code>fixed_point</code>. In such cases, the minimization condition is satisfied structurally, so <a href="../minimization/#NeuralLyapunov.DontCheckNonnegativity"><code>DontCheckNonnegativity(false)</code></a> should be used.</p><p><strong>Arguments</strong></p><ul><li>network_dim: output dimensionality of the neural network.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>pos_def(state, fixed_point)</code>: a function that is positive (semi-)definite in <code>state</code> around <code>fixed_point</code>; defaults to <span>$\log(1 + \lVert x - x_0 \rVert^2)$</span>.</li><li><code>non_neg(net, state, fixed_point)</code>: a nonnegative function of the neural network; note that <code>net</code> is the neural network <span>$ϕ$</span>, and <code>net(state)</code> is the value of the neural network at a point <span>$ϕ(x)$</span>; defaults to <span>$1 + \lVert ϕ(x) \rVert^2$</span>.</li><li><code>grad_pos_def(state, fixed_point)</code>: the gradient of <code>pos_def</code> with respect to <code>state</code> at <code>state</code>. If <code>isnothing(grad_pos_def)</code> (as is the default), the gradient of <code>pos_def</code> will be evaluated using <code>grad</code>.</li><li><code>grad_non_neg(net, J_net, state, fixed_point)</code>: the gradient of <code>non_neg</code> with respect to <code>state</code> at <code>state</code>; <code>J_net</code> is a function outputting the Jacobian of <code>net</code> at the input. If <code>isnothing(grad_non_neg)</code> (as is the default), the gradient of <code>non_neg</code> will be evaluated using <code>grad</code>.</li><li><code>grad</code>: a function for evaluating gradients to be used when <code>isnothing(grad_pos_def) || isnothing(grad_non_neg)</code>; defaults to, and expects the same arguments as, <code>ForwardDiff.gradient</code>.</li></ul><p>Dynamics are assumed to be in <code>f(state, p, t)</code> form, as in an <code>ODEFunction</code>. For <code>f(state, input, p, t)</code>, consider using <a href="../policy_search/#NeuralLyapunov.add_policy_search"><code>add_policy_search</code></a>.</p><p>See also: <a href="../minimization/#NeuralLyapunov.DontCheckNonnegativity"><code>DontCheckNonnegativity</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/bfe41018da3e51fc7948e4208ae41d5538aaf8bd/src/structure_specification.jl#L87-L127">source</a></section></article><h2 id="Pre-defined-Lux-structures"><a class="docs-heading-anchor" href="#Pre-defined-Lux-structures">Pre-defined Lux structures</a><a id="Pre-defined-Lux-structures-1"></a><a class="docs-heading-anchor-permalink" href="#Pre-defined-Lux-structures" title="Permalink"></a></h2><p>Regardless of what NeuralLyapunov transformation is used to transform <span>$\phi$</span> into <span>$V$</span>, users should carefully consider their choice of <span>$\phi$</span>. Two options provided by NeuralLyapunov, intended to be used with <a href="#NeuralLyapunov.NoAdditionalStructure"><code>NoAdditionalStructure</code></a>, are <a href="#NeuralLyapunov.AdditiveLyapunovNet"><code>AdditiveLyapunovNet</code></a> and <a href="#NeuralLyapunov.MultiplicativeLyapunovNet"><code>MultiplicativeLyapunovNet</code></a>. These each wrap a different Lux model, effectively performing the transformation from <span>$\phi$</span> to <span>$V$</span> within the Lux ecosystem, rather than in the NeuralPDE/ModelingToolkit symbolic ecosystem. </p><p><a href="#NeuralLyapunov.AdditiveLyapunovNet"><code>AdditiveLyapunovNet</code></a> is based on (<a href="#gaby_lyapunov-net_2021">Gaby <em>et al.</em>, 2021</a>), and <a href="#NeuralLyapunov.MultiplicativeLyapunovNet"><code>MultiplicativeLyapunovNet</code></a> is an analogous structure combining the neural term and the positive definite term via multiplication instead of addition.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralLyapunov.AdditiveLyapunovNet" href="#NeuralLyapunov.AdditiveLyapunovNet"><code>NeuralLyapunov.AdditiveLyapunovNet</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AdditiveLyapunovNet(ϕ; ψ, m, r, dim_ϕ, dim_m, fixed_point)</code></pre><p>Construct a Lyapunov-Net with the following structure:</p><p class="math-container">\[    V(x) = ψ(ϕ(x) - ϕ(x_0)) + r(m(x) - m(x_0)),\]</p><p>where <span>$x_0$</span> is <code>fixed_point</code> and the functions are defined as below. If the functions meet the conditions listed below, the resulting model will be positive definite (around <code>fixed_point</code>), as the <span>$r$</span> term will be positive definite and the <span>$ψ$</span> term will be positive semidefinite.</p><p><strong>Arguments</strong></p><ul><li><code>ϕ</code>: The base neural network model; its output dimension should be <code>dim_ϕ</code>.</li><li><code>ψ</code>: A Lux layer representing a positive semidefinite function that maps the output of <code>ϕ</code> to a scalar value; defaults to <a href="#NeuralLyapunov.SoSPooling"><code>SoSPooling()</code></a> (i.e., <span>$\lVert ⋅ \rVert^2$</span>). Users may provide a function instead of a Lux layer, in which case it will be wrapped into a layer via <a href="https://lux.csail.mit.edu/dev/api/Lux/layers#Misc.-Helper-Layers"><code>Lux.WrappedFunction</code></a>.</li><li><code>m</code>: Optional pre-processing layer for use before <code>r</code>. This layer should output a vector of dimension <code>dim_m</code> and <span>$m(x) = m(x_0)$</span> should imply that <span>$x$</span> is an equilibrium to be analyzed by the Lyapunov function. Defaults to <a href="https://lux.csail.mit.edu/dev/api/Lux/layers#Misc.-Helper-Layers"><code>Lux.NoOpLayer()</code></a>, which is typically the right choice when analyzing a single equilibrium point. Consider using a <a href="https://luxdl.github.io/Boltz.jl/dev/api/layers#Boltz.Layers-API-Reference"><code>Boltz.Layers.PeriodicEmbedding</code></a> if any of the state variables are periodic. Users may provide a function instead of a Lux layer, in which case it will be wrapped into a layer via <a href="https://lux.csail.mit.edu/dev/api/Lux/layers#Misc.-Helper-Layers"><code>Lux.WrappedFunction</code></a>.</li><li><code>r</code>: A Lux layer representing a positive definite function that maps the output of <code>m</code> to a scalar value; defaults to <a href="#NeuralLyapunov.SoSPooling"><code>SoSPooling()</code></a> (i.e., <span>$\lVert ⋅ \rVert^2$</span>). Users may provide a function instead of a Lux layer, in which case it will be wrapped into a layer via <a href="https://lux.csail.mit.edu/dev/api/Lux/layers#Misc.-Helper-Layers"><code>Lux.WrappedFunction</code></a>.</li><li><code>dim_ϕ</code>: The dimension of the output of <code>ϕ</code>.</li><li><code>dim_m</code>: The dimension of the output of <code>m</code>; defaults to <code>length(fixed_point)</code> when <code>fixed_point</code> is provided and <code>dim_m</code> isn&#39;t. Users must provide at least one of <code>dim_m</code> and <code>fixed_point</code>.</li><li><code>fixed_point</code>: A vector of length <code>dim_m</code> representing the fixed point; defaults to <code>zeros(dim_m)</code> when <code>dim_m</code> is provided and <code>fixed_point</code> isn&#39;t. Users must provide at least one of <code>dim_m</code> and <code>fixed_point</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/bfe41018da3e51fc7948e4208ae41d5538aaf8bd/src/lux_structures.jl#L1-L42">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralLyapunov.MultiplicativeLyapunovNet" href="#NeuralLyapunov.MultiplicativeLyapunovNet"><code>NeuralLyapunov.MultiplicativeLyapunovNet</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MultiplicativeLyapunovNet(ϕ; ζ, m, r, dim_m, fixed_point)</code></pre><p>Construct a Lyapunov-Net with the following structure:</p><p class="math-container">\[    V(x) = ζ(ϕ(x)) (r(m(x) - m(x_0))),\]</p><p>where <span>$x_0$</span> is <code>fixed_point</code> and the functions are defined as below. If the functions meet the conditions listed below, the resulting model will be positive definite (around <code>fixed_point</code>), as the <span>$r$</span> term will be positive definite and the <span>$ζ$</span> term will be strictly positive.</p><p><strong>Arguments</strong></p><ul><li><code>ϕ</code>: The base neural network model.</li><li><code>ζ</code>: A Lux layer representing a strictly positive function that maps the output of <code>ϕ</code> to a scalar value; defaults to <a href="#NeuralLyapunov.StrictlyPositiveSoSPooling"><code>StrictlyPositiveSoSPooling()</code></a> (i.e., <span>$1 + \lVert ⋅ \rVert^2$</span>). Users may provide a function instead of a Lux layer, in which case it will be wrapped into a layer via <a href="https://lux.csail.mit.edu/dev/api/Lux/layers#Misc.-Helper-Layers"><code>Lux.WrappedFunction</code></a>.</li><li><code>m</code>: Optional pre-processing layer for use before <code>r</code>. This layer should output a vector of dimension <code>dim_m</code> and <span>$m(x) = m(x_0)$</span> should imply that <span>$x$</span> is an equilibrium to be analyzed by the Lyapunov function. Defaults to <a href="https://lux.csail.mit.edu/dev/api/Lux/layers#Misc.-Helper-Layers"><code>Lux.NoOpLayer()</code></a>, which is typically the right choice when analyzing a single equilibrium point. Consider using a <a href="https://luxdl.github.io/Boltz.jl/dev/api/layers#Boltz.Layers-API-Reference"><code>Boltz.Layers.PeriodicEmbedding</code></a> if any of the state variables are periodic. Users may provide a function instead of a Lux layer, in which case it will be wrapped into a layer via <a href="https://lux.csail.mit.edu/dev/api/Lux/layers#Misc.-Helper-Layers"><code>Lux.WrappedFunction</code></a>.</li><li><code>r</code>: A Lux layer representing a positive definite function that maps the output of <code>m</code> to a scalar value; defaults to <a href="#NeuralLyapunov.SoSPooling"><code>SoSPooling()</code></a> (i.e., <span>$\lVert ⋅ \rVert^2$</span>). Users may provide a function instead of a Lux layer, in which case it will be wrapped into a layer via <a href="https://lux.csail.mit.edu/dev/api/Lux/layers#Misc.-Helper-Layers"><code>Lux.WrappedFunction</code></a>.</li><li><code>dim_m</code>: The dimension of the output of <code>m</code>; defaults to <code>length(fixed_point)</code> when <code>fixed_point</code> is provided and <code>dim_m</code> isn&#39;t.</li><li><code>fixed_point</code>: A vector of length <code>dim_m</code> representing the fixed point; defaults to <code>zeros(dim_m)</code> when <code>dim_m</code> is provided and <code>fixed_point</code> isn&#39;t.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/bfe41018da3e51fc7948e4208ae41d5538aaf8bd/src/lux_structures.jl#L96-L133">source</a></section></article><p>Note that using <a href="#NeuralLyapunov.NoAdditionalStructure"><code>NoAdditionalStructure</code></a> with <a href="#NeuralLyapunov.MultiplicativeLyapunovNet"><code>MultiplicativeLyapunovNet</code></a> wrapping a Lux model <span>$\phi$</span> is the same as using <a href="#NeuralLyapunov.PositiveSemiDefiniteStructure"><code>PositiveSemiDefiniteStructure</code></a> the same <span>$\phi$</span>, but in the former the transformation is handled in the Lux ecosystem and in the latter the transformation is handled in the NeuralPDE/ModelingToolkit ecosystem. Similarly, using <a href="#NeuralLyapunov.NonnegativeStructure"><code>NonnegativeStructure</code></a> with <a href="https://luxdl.github.io/Boltz.jl/dev/api/layers#Boltz.Layers-API-Reference"><code>Boltz.Layers.ShiftTo</code></a> is analogous to using <a href="#NeuralLyapunov.NoAdditionalStructure"><code>NoAdditionalStructure</code></a> with <a href="#NeuralLyapunov.AdditiveLyapunovNet"><code>AdditiveLyapunovNet</code></a>. Because the NeuralPDE parser cannot process <span>$\phi$</span> being evaluated at two different points (in this case <span>$x$</span> and <span>$x_0$</span>), we cannot represent this structure purely in the NeuralPDE/ModelingToolkit ecosystem.</p><p>Helper layers provided for the above structures are also exported:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralLyapunov.SoSPooling" href="#NeuralLyapunov.SoSPooling"><code>NeuralLyapunov.SoSPooling</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SoSPooling(; dim = 1)</code></pre><p>Construct a pooling function that computes the sum of squares along the dimension <code>dim</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/bfe41018da3e51fc7948e4208ae41d5538aaf8bd/src/lux_structures.jl#L179-L183">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralLyapunov.StrictlyPositiveSoSPooling" href="#NeuralLyapunov.StrictlyPositiveSoSPooling"><code>NeuralLyapunov.StrictlyPositiveSoSPooling</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StrictlyPositiveSoSPooling(; dim = 1)</code></pre><p>Construct a pooling function that computes 1 + the sum of squares along the dimension <code>dim</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/bfe41018da3e51fc7948e4208ae41d5538aaf8bd/src/lux_structures.jl#L186-L190">source</a></section></article><h2 id="Defining-your-own-neural-Lyapunov-function-structure-with-[NeuralLyapunovStructure](@ref)"><a class="docs-heading-anchor" href="#Defining-your-own-neural-Lyapunov-function-structure-with-[NeuralLyapunovStructure](@ref)">Defining your own neural Lyapunov function structure with <a href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunovStructure</code></a></a><a id="Defining-your-own-neural-Lyapunov-function-structure-with-[NeuralLyapunovStructure](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-your-own-neural-Lyapunov-function-structure-with-[NeuralLyapunovStructure](@ref)" title="Permalink"></a></h2><p>To define a new structure for a neural Lyapunov function, one must specify the form of the Lyapunov candidate <span>$V$</span> and its time derivative along a trajectory <span>$\dot{V}$</span>, as well as how to call the dynamics <span>$f$</span>. Additionally, the dimensionality of the output of the neural network must be known in advance.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralLyapunov.NeuralLyapunovStructure" href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunov.NeuralLyapunovStructure</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NeuralLyapunovStructure(V, V̇, f_call, network_dim)</code></pre><p>Specifies the structure of the neural Lyapunov function and its derivative.</p><p>Allows the user to define the Lyapunov in terms of the neural network, potentially structurally enforcing some Lyapunov conditions.</p><p><strong>Fields</strong></p><ul><li><code>V(phi, state, fixed_point)</code>: outputs the value of the Lyapunov function at <code>state</code>.</li><li><code>V̇(phi, J_phi, dynamics, state, params, t, fixed_point)</code>: outputs the time derivative of the Lyapunov function at <code>state</code>.</li><li><code>f_call(dynamics, phi, state, params, t)</code>: outputs the derivative of the state; this is useful for making closed-loop dynamics which depend on the neural network, such as in the policy search case.</li><li><code>network_dim</code>: the dimension of the output of the neural network.</li></ul><p><code>phi</code> and <code>J_phi</code> above are both functions of <code>state</code> alone.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/bfe41018da3e51fc7948e4208ae41d5538aaf8bd/src/conditions_specification.jl#L1-L19">source</a></section></article><h3 id="Calling-the-dynamics"><a class="docs-heading-anchor" href="#Calling-the-dynamics">Calling the dynamics</a><a id="Calling-the-dynamics-1"></a><a class="docs-heading-anchor-permalink" href="#Calling-the-dynamics" title="Permalink"></a></h3><p>Very generally, the dynamical system can be a system of ODEs <span>$\dot{x} = f(x, u, p, t)$</span>, where <span>$u$</span> is a control input, <span>$p$</span> contains parameters, and <span>$f$</span> depends on the neural network in some way. To capture this variety, users must supply the function <code>f_call(dynamics, phi, state, p, t)</code>.</p><p>The most common example is when <code>dynamics</code> takes the same form as an <code>ODEFunction</code>.  i.e., <span>$\dot{x} = \texttt{dynamics}(x, p, t)$</span>. In that case, <code>f_call(dynamics, phi, state, p, t) = dynamics(state, p, t)</code>.</p><p>Suppose instead, the dynamics were supplied as a function of state alone: <span>$\dot{x} = \texttt{dynamics}(x)$</span>. Then, <code>f_call(dynamics, phi, state, p, t) = dynamics(state)</code>.</p><p>Finally, suppose <span>$\dot{x} = \texttt{dynamics}(x, u, p, t)$</span> has a unidimensional control input that is being trained (via <a href="../policy_search/">policy search</a>) to be the second output of the neural network. Then <code>f_call(dynamics, phi, state, p, t) = dynamics(state, phi(state)[2], p, t)</code>.</p><p>Note that, despite the inclusion of the time variable <span>$t$</span>, NeuralLyapunov.jl currently only supports time-invariant systems, so only <code>t = 0.0</code> is used.</p><h3 id="Structuring-V-and-\\dot{V}"><a class="docs-heading-anchor" href="#Structuring-V-and-\\dot{V}">Structuring <span>$V$</span> and <span>$\dot{V}$</span></a><a id="Structuring-V-and-\\dot{V}-1"></a><a class="docs-heading-anchor-permalink" href="#Structuring-V-and-\\dot{V}" title="Permalink"></a></h3><p>The Lyapunov candidate function <span>$V$</span> gets specified as a function <code>V(phi, state, fixed_point)</code>, where <code>phi</code> is the neural network as a function <code>phi(state)</code>. Note that this form allows <span>$V(x)$</span> to depend on the neural network evaluated at points other than just the input <span>$x$</span>.</p><p>The time derivative <span>$\dot{V}$</span> is similarly defined by a function <code>V̇(phi, J_phi, dynamics, state, params, t, fixed_point)</code>. The function <code>J_phi(state)</code> gives the Jacobian of the neural network <code>phi</code> at <code>state</code>. The function <code>dynamics</code> is as above (with parameters <code>params</code>). </p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation canonical"><ul><li><div id="gaby_lyapunov-net_2021">Gaby, N.; Zhang, F. and Ye, X. (2021). <a href="https://arxiv.org/abs/2109.13359"><em>Lyapunov-Net: A Deep Neural Network Architecture for Lyapunov Function                   Approximation</em></a>. CoRR <strong>abs/2109.13359</strong>, <a href="https://arxiv.org/abs/2109.13359">arXiv:2109.13359</a>.</div></li></ul></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../decrease/">« Lyapunov Decrease Condition</a><a class="docs-footer-nextpage" href="../roa/">Training for Region of Attraction Identification »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Tuesday 30 September 2025 15:00">Tuesday 30 September 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
