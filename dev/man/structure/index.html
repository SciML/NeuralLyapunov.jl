<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Structuring a Neural Lyapunov function · NeuralLyapunov.jl</title><meta name="title" content="Structuring a Neural Lyapunov function · NeuralLyapunov.jl"/><meta property="og:title" content="Structuring a Neural Lyapunov function · NeuralLyapunov.jl"/><meta property="twitter:title" content="Structuring a Neural Lyapunov function · NeuralLyapunov.jl"/><meta name="description" content="Documentation for NeuralLyapunov.jl."/><meta property="og:description" content="Documentation for NeuralLyapunov.jl."/><meta property="twitter:description" content="Documentation for NeuralLyapunov.jl."/><meta property="og:url" content="https://SciML.github.io/NeuralLyapunov.jl/man/structure/"/><meta property="twitter:url" content="https://SciML.github.io/NeuralLyapunov.jl/man/structure/"/><link rel="canonical" href="https://SciML.github.io/NeuralLyapunov.jl/man/structure/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralLyapunov.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../">Components of a Neural Lyapunov Problem</a></li><li><a class="tocitem" href="../pdesystem/">Solving a Neural Lyapunov Problem</a></li><li><a class="tocitem" href="../minimization/">Lyapunov Minimization Condition</a></li><li><a class="tocitem" href="../decrease/">Lyapunov Decrease Condition</a></li><li class="is-active"><a class="tocitem" href>Structuring a Neural Lyapunov function</a><ul class="internal"><li><a class="tocitem" href="#Pre-defined-structures"><span>Pre-defined structures</span></a></li><li><a class="tocitem" href="#Defining-your-own-neural-Lyapunov-function-structure"><span>Defining your own neural Lyapunov function structure</span></a></li></ul></li><li><a class="tocitem" href="../roa/">Training for Region of Attraction Identification</a></li><li><a class="tocitem" href="../policy_search/">Policy Search and Network-Dependent Dynamics</a></li><li><a class="tocitem" href="../local_lyapunov/">Local Lyapunov analysis</a></li></ul></li><li><span class="tocitem">Demonstrations</span><ul><li><a class="tocitem" href="../../demos/damped_SHO/">Damped Simple Harmonic Oscillator</a></li><li><a class="tocitem" href="../../demos/roa_estimation/">Estimating the Region of Attraction</a></li><li><a class="tocitem" href="../../demos/policy_search/">Policy Search on the Driven Inverted Pendulum</a></li><li><a class="tocitem" href="../../demos/benchmarking/">Benchmarking a neural Lyapunov method</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Structuring a Neural Lyapunov function</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Structuring a Neural Lyapunov function</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/NeuralLyapunov.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/NeuralLyapunov.jl/blob/master/docs/src/man/structure.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Structuring-a-Neural-Lyapunov-function"><a class="docs-heading-anchor" href="#Structuring-a-Neural-Lyapunov-function">Structuring a Neural Lyapunov function</a><a id="Structuring-a-Neural-Lyapunov-function-1"></a><a class="docs-heading-anchor-permalink" href="#Structuring-a-Neural-Lyapunov-function" title="Permalink"></a></h1><p>Three simple neural Lyapunov function structures are provided, but users can always specify a custom structure using the <a href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunovStructure</code></a> struct.</p><h2 id="Pre-defined-structures"><a class="docs-heading-anchor" href="#Pre-defined-structures">Pre-defined structures</a><a id="Pre-defined-structures-1"></a><a class="docs-heading-anchor-permalink" href="#Pre-defined-structures" title="Permalink"></a></h2><p>The simplest structure is to train the neural network directly to be the Lyapunov function, which can be accomplished using an <a href="#NeuralLyapunov.UnstructuredNeuralLyapunov"><code>UnstructuredNeuralLyapunov</code></a>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralLyapunov.UnstructuredNeuralLyapunov" href="#NeuralLyapunov.UnstructuredNeuralLyapunov"><code>NeuralLyapunov.UnstructuredNeuralLyapunov</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UnstructuredNeuralLyapunov()</code></pre><p>Create a <a href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunovStructure</code></a> where the Lyapunov function is the neural network evaluated at the state. This does not structurally enforce any Lyapunov conditions.</p><p>Corresponds to <span>$V(x) = ϕ(x)$</span>, where <span>$ϕ$</span> is the neural network.</p><p>Dynamics are assumed to be in <code>f(state, p, t)</code> form, as in an <code>ODEFunction</code>. For <code>f(state, input, p, t)</code>, consider using <a href="../policy_search/#NeuralLyapunov.add_policy_search"><code>add_policy_search</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/56a160fe28893be24b2b86b241bba7debb1c4e16/src/structure_specification.jl#L1-L11">source</a></section></article><p>The condition that the Lyapunov function <span>$V(x)$</span> must be minimized uniquely at the fixed point <span>$x_0$</span> is often represented as a requirement for <span>$V(x)$</span> to be positive away from the fixed point and zero at the fixed point. Put mathematically, it is sufficient to require <span>$V(x) &gt; 0 \, \forall x \ne x_0$</span> and <span>$V(x_0) = 0$</span>. We call such functions positive definite (around the fixed point <span>$x_0$</span>).</p><p>Two structures are provided which partially or fully enforce the minimization condition: <a href="#NeuralLyapunov.NonnegativeNeuralLyapunov"><code>NonnegativeNeuralLyapunov</code></a>, which structurally enforces <span>$V(x) \ge 0$</span> everywhere, and <a href="#NeuralLyapunov.PositiveSemiDefiniteStructure"><code>PositiveSemiDefiniteStructure</code></a>, which additionally enforces <span>$V(x_0) = 0$</span>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralLyapunov.NonnegativeNeuralLyapunov" href="#NeuralLyapunov.NonnegativeNeuralLyapunov"><code>NeuralLyapunov.NonnegativeNeuralLyapunov</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NonnegativeNeuralLyapunov(network_dim; &lt;keyword_arguments&gt;)</code></pre><p>Create a <a href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunovStructure</code></a> where the Lyapunov function is the L2 norm of the neural network output plus a constant δ times a function <code>pos_def</code>.</p><p>Corresponds to <span>$V(x) = \lVert ϕ(x) \rVert^2 + δ \, \texttt{pos\_def}(x, x_0)$</span>, where <span>$ϕ$</span> is the neural network and <span>$x_0$</span> is the equilibrium point.</p><p>This structure ensures <span>$V(x) ≥ 0 \, ∀ x$</span> when <span>$δ ≥ 0$</span> and <code>pos_def</code> is always nonnegative. Further, if <span>$δ &gt; 0$</span> and <code>pos_def</code> is strictly positive definite around <code>fixed_point</code>, the structure ensures that <span>$V(x)$</span> is strictly positive away from <code>fixed_point</code>. In such cases, the minimization condition reduces to ensuring <span>$V(x_0) = 0$</span>, and so <a href="../minimization/#NeuralLyapunov.DontCheckNonnegativity"><code>DontCheckNonnegativity(true)</code></a> should be used.</p><p><strong>Arguments</strong></p><ul><li><code>network_dim</code>: output dimensionality of the neural network.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>δ</code>: weight of <code>pos_def</code>, as above; defaults to 0.</li><li><code>pos_def(state, fixed_point)</code>: a function that is postive (semi-)definite in <code>state</code> around <code>fixed_point</code>; defaults to <span>$\log(1 + \lVert x - x_0 \rVert^2)$</span>.</li><li><code>grad_pos_def(state, fixed_point)</code>: the gradient of <code>pos_def</code> with respect to <code>state</code> at <code>state</code>. If <code>isnothing(grad_pos_def)</code> (as is the default), the gradient of <code>pos_def</code> will be evaluated using <code>grad</code>.</li><li><code>grad</code>: a function for evaluating gradients to be used when <code>isnothing(grad_pos_def)</code>; defaults to, and expects the same arguments as, <code>ForwardDiff.gradient</code>.</li></ul><p>Dynamics are assumed to be in <code>f(state, p, t)</code> form, as in an <code>ODEFunction</code>. For <code>f(state, input, p, t)</code>, consider using <a href="../policy_search/#NeuralLyapunov.add_policy_search"><code>add_policy_search</code></a>.</p><p>See also: <a href="../minimization/#NeuralLyapunov.DontCheckNonnegativity"><code>DontCheckNonnegativity</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/56a160fe28893be24b2b86b241bba7debb1c4e16/src/structure_specification.jl#L22-L55">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralLyapunov.PositiveSemiDefiniteStructure" href="#NeuralLyapunov.PositiveSemiDefiniteStructure"><code>NeuralLyapunov.PositiveSemiDefiniteStructure</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PositiveSemiDefiniteStructure(network_dim; &lt;keyword_arguments&gt;)</code></pre><p>Create a <a href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunovStructure</code></a> where the Lyapunov function is the product of a positive (semi-)definite function <code>pos_def</code> which does not depend on the network and a nonnegative function <code>non_neg</code> which does depend the network.</p><p>Corresponds to <span>$V(x) = \texttt{pos\_def}(x, x_0) * \texttt{non\_neg}(ϕ, x, x_0)$</span>, where <span>$ϕ$</span> is the neural network and <span>$x_0$</span> is the equilibrium point.</p><p>This structure ensures <span>$V(x) ≥ 0$</span>. Further, if <code>pos_def</code> is strictly positive definite <code>fixed_point</code> and <code>non_neg</code> is strictly positive (as is the case for the default values of <code>pos_def</code> and <code>non_neg</code>), then this structure ensures <span>$V(x)$</span> is strictly positive definite around <code>fixed_point</code>. In such cases, the minimization condition is satisfied structurally, so <a href="../minimization/#NeuralLyapunov.DontCheckNonnegativity"><code>DontCheckNonnegativity(false)</code></a> should be used.</p><p><strong>Arguments</strong></p><ul><li>network_dim: output dimensionality of the neural network.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>pos_def(state, fixed_point)</code>: a function that is postive (semi-)definite in <code>state</code> around <code>fixed_point</code>; defaults to <span>$\log(1 + \lVert x - x_0 \rVert^2)$</span>.</li><li><code>non_neg(net, state, fixed_point)</code>: a nonnegative function of the neural network; note that <code>net</code> is the neural network <span>$ϕ$</span>, and <code>net(state)</code> is the value of the neural network at a point <span>$ϕ(x)$</span>; defaults to <span>$1 + \lVert ϕ(x) \rVert^2$</span>.</li><li><code>grad_pos_def(state, fixed_point)</code>: the gradient of <code>pos_def</code> with respect to <code>state</code> at <code>state</code>. If <code>isnothing(grad_pos_def)</code> (as is the default), the gradient of <code>pos_def</code> will be evaluated using <code>grad</code>.</li><li><code>grad_non_neg(net, J_net, state, fixed_point)</code>: the gradient of <code>non_neg</code> with respect to <code>state</code> at <code>state</code>; <code>J_net</code> is a function outputting the Jacobian of <code>net</code> at the input. If <code>isnothing(grad_non_neg)</code> (as is the default), the gradient of <code>non_neg</code> will be evaluated using <code>grad</code>.</li><li><code>grad</code>: a function for evaluating gradients to be used when <code>isnothing(grad_pos_def) || isnothing(grad_non_neg)</code>; defaults to, and expects the same arguments as, <code>ForwardDiff.gradient</code>.</li></ul><p>Dynamics are assumed to be in <code>f(state, p, t)</code> form, as in an <code>ODEFunction</code>. For <code>f(state, input, p, t)</code>, consider using <a href="../policy_search/#NeuralLyapunov.add_policy_search"><code>add_policy_search</code></a>.</p><p>See also: <a href="../minimization/#NeuralLyapunov.DontCheckNonnegativity"><code>DontCheckNonnegativity</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/56a160fe28893be24b2b86b241bba7debb1c4e16/src/structure_specification.jl#L93-L133">source</a></section></article><h2 id="Defining-your-own-neural-Lyapunov-function-structure"><a class="docs-heading-anchor" href="#Defining-your-own-neural-Lyapunov-function-structure">Defining your own neural Lyapunov function structure</a><a id="Defining-your-own-neural-Lyapunov-function-structure-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-your-own-neural-Lyapunov-function-structure" title="Permalink"></a></h2><p>To define a new structure for a neural Lyapunov function, one must specify the form of the Lyapunov candidate <span>$V$</span> and its time derivative along a trajectory <span>$\dot{V}$</span>, as well as how to call the dynamics <span>$f$</span>. Additionally, the dimensionality of the output of the neural network must be known in advance.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralLyapunov.NeuralLyapunovStructure" href="#NeuralLyapunov.NeuralLyapunovStructure"><code>NeuralLyapunov.NeuralLyapunovStructure</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NeuralLyapunovStructure(V, V̇, f_call, network_dim)</code></pre><p>Specifies the structure of the neural Lyapunov function and its derivative.</p><p>Allows the user to define the Lyapunov in terms of the neural network, potentially structurally enforcing some Lyapunov conditions.</p><p><strong>Fields</strong></p><ul><li><code>V(phi::Function, state, fixed_point)</code>: outputs the value of the Lyapunov function at <code>state</code>.</li><li><code>V̇(phi::Function, J_phi::Function, dynamics::Function, state, params, t, fixed_point)</code>: outputs the time derivative of the Lyapunov function at <code>state</code>.</li><li><code>f_call(dynamics::Function, phi::Function, state, params, t)</code>: outputs the derivative of the state; this is useful for making closed-loop dynamics which depend on the neural network, such as in the policy search case.</li><li><code>network_dim</code>: the dimension of the output of the neural network.</li></ul><p><code>phi</code> and <code>J_phi</code> above are both functions of <code>state</code> alone.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/56a160fe28893be24b2b86b241bba7debb1c4e16/src/conditions_specification.jl#L1-L20">source</a></section></article><h3 id="Calling-the-dynamics"><a class="docs-heading-anchor" href="#Calling-the-dynamics">Calling the dynamics</a><a id="Calling-the-dynamics-1"></a><a class="docs-heading-anchor-permalink" href="#Calling-the-dynamics" title="Permalink"></a></h3><p>Very generally, the dynamical system can be a system of ODEs <span>$\dot{x} = f(x, u, p, t)$</span>, where <span>$u$</span> is a control input, <span>$p$</span> contains parameters, and <span>$f$</span> depends on the neural network in some way. To capture this variety, users must supply the function <code>f_call(dynamics::Function, phi::Function, state, p, t)</code>.</p><p>The most common example is when <code>dynamics</code> takes the same form as an <code>ODEFunction</code>.  i.e., <span>$\dot{x} = \texttt{dynamics}(x, p, t)$</span>. In that case, <code>f_call(dynamics, phi, state, p, t) = dynamics(state, p, t)</code>.</p><p>Suppose instead, the dynamics were supplied as a function of state alone: <span>$\dot{x} = \texttt{dynamics}(x)$</span>. Then, <code>f_call(dynamics, phi, state, p, t) = dynamics(state)</code>.</p><p>Finally, suppose <span>$\dot{x} = \texttt{dynamics}(x, u, p, t)$</span> has a unidimensional control input that is being trained (via <a href="../policy_search/">policy search</a>) to be the second output of the neural network. Then <code>f_call(dynamics, phi, state, p, t) = dynamics(state, phi(state)[2], p, t)</code>.</p><p>Note that, despite the inclusion of the time variable <span>$t$</span>, NeuralLyapunov.jl currently only supports time-invariant systems, so only <code>t = 0.0</code> is used.</p><h3 id="Structuring-V-and-\\dot{V}"><a class="docs-heading-anchor" href="#Structuring-V-and-\\dot{V}">Structuring <span>$V$</span> and <span>$\dot{V}$</span></a><a id="Structuring-V-and-\\dot{V}-1"></a><a class="docs-heading-anchor-permalink" href="#Structuring-V-and-\\dot{V}" title="Permalink"></a></h3><p>The Lyapunov candidate function <span>$V$</span> gets specified as a function <code>V(phi, state, fixed_point)</code>, where <code>phi</code> is the neural network as a function <code>phi(state)</code>. Note that this form allows <span>$V(x)$</span> to depend on the neural network evaluated at points other than just the input <span>$x$</span>.</p><p>The time derivative <span>$\dot{V}$</span> is similarly defined by a function <code>V̇(phi, J_phi, dynamics, state, params, t, fixed_point)</code>. The function <code>J_phi(state)</code> gives the Jacobian of the neural network <code>phi</code> at <code>state</code>. The function <code>dynamics</code> is as above (with parameters <code>params</code>). </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../decrease/">« Lyapunov Decrease Condition</a><a class="docs-footer-nextpage" href="../roa/">Training for Region of Attraction Identification »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Monday 27 January 2025 21:05">Monday 27 January 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
