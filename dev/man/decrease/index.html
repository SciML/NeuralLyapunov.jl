<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Lyapunov Decrease Condition · NeuralLyapunov.jl</title><meta name="title" content="Lyapunov Decrease Condition · NeuralLyapunov.jl"/><meta property="og:title" content="Lyapunov Decrease Condition · NeuralLyapunov.jl"/><meta property="twitter:title" content="Lyapunov Decrease Condition · NeuralLyapunov.jl"/><meta name="description" content="Documentation for NeuralLyapunov.jl."/><meta property="og:description" content="Documentation for NeuralLyapunov.jl."/><meta property="twitter:description" content="Documentation for NeuralLyapunov.jl."/><meta property="og:url" content="https://SciML.github.io/NeuralLyapunov.jl/man/decrease/"/><meta property="twitter:url" content="https://SciML.github.io/NeuralLyapunov.jl/man/decrease/"/><link rel="canonical" href="https://SciML.github.io/NeuralLyapunov.jl/man/decrease/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralLyapunov.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../">Components of a Neural Lyapunov Problem</a></li><li><a class="tocitem" href="../pdesystem/">Solving a Neural Lyapunov Problem</a></li><li><a class="tocitem" href="../minimization/">Lyapunov Minimization Condition</a></li><li class="is-active"><a class="tocitem" href>Lyapunov Decrease Condition</a><ul class="internal"><li><a class="tocitem" href="#Pre-defined-decrease-conditions"><span>Pre-defined decrease conditions</span></a></li><li><a class="tocitem" href="#Defining-your-own-decrease-condition"><span>Defining your own decrease condition</span></a></li></ul></li><li><a class="tocitem" href="../structure/">Structuring a Neural Lyapunov function</a></li><li><a class="tocitem" href="../roa/">Training for Region of Attraction Identification</a></li><li><a class="tocitem" href="../policy_search/">Policy Search and Network-Dependent Dynamics</a></li><li><a class="tocitem" href="../local_lyapunov/">Local Lyapunov analysis</a></li></ul></li><li><span class="tocitem">Demonstrations</span><ul><li><a class="tocitem" href="../../demos/damped_SHO/">Damped Simple Harmonic Oscillator</a></li><li><a class="tocitem" href="../../demos/roa_estimation/">Estimating the Region of Attraction</a></li><li><a class="tocitem" href="../../demos/policy_search/">Policy Search on the Driven Inverted Pendulum</a></li><li><a class="tocitem" href="../../demos/benchmarking/">Benchmarking a neural Lyapunov method</a></li></ul></li><li><span class="tocitem">Test Problem Library</span><ul><li><a class="tocitem" href="../../lib/">NeuralLyapunovProblemLibrary.jl</a></li><li><a class="tocitem" href="../../lib/pendulum/">Pendulum Model</a></li><li><a class="tocitem" href="../../lib/double_pendulum/">Double Pendulum Model</a></li><li><a class="tocitem" href="../../lib/quadrotor/">Quadrotor Models</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Lyapunov Decrease Condition</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Lyapunov Decrease Condition</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/NeuralLyapunov.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/NeuralLyapunov.jl/blob/master/docs/src/man/decrease.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Lyapunov-Decrease-Condition"><a class="docs-heading-anchor" href="#Lyapunov-Decrease-Condition">Lyapunov Decrease Condition</a><a id="Lyapunov-Decrease-Condition-1"></a><a class="docs-heading-anchor-permalink" href="#Lyapunov-Decrease-Condition" title="Permalink"></a></h1><p>To represent the condition that the Lyapunov function <span>$V(x)$</span> must decrease along system trajectories, we typically introduce a new function <span>$\dot{V}(x) = \nabla V(x) \cdot f(x)$</span>. This function represents the rate of change of <span>$V$</span> along system trajectories. That is to say, if <span>$x(t)$</span> is a trajectory defined by <span>$\frac{dx}{dt} = f(x)$</span>, then <span>$\dot{V}(x(t)) = \frac{d}{dt} [ V(x(t)) ]$</span>. It is then sufficient to show that <span>$\dot{V}(x)$</span> is negative away from the fixed point and zero at the fixed point, since a negative derivative means a decreasing function.</p><p>Put mathematically, it is sufficient to require <span>$\dot{V}(x) &lt; 0 \, \forall x \ne x_0$</span> and <span>$\dot{V}(x_0) = 0$</span>. We call such functions negative definite (around the fixed point <span>$x_0$</span>). The weaker condition that <span>$\dot{V}(x) \le 0 \, \forall x \ne x_0$</span> and <span>$\dot{V}(x_0) = 0$</span> is negative <em>semi-</em>definiteness.</p><p>The condition that <span>$\dot{V}(x_0) = 0$</span> is satisfied by the definition of <span>$\dot{V}$</span> and the fact that <span>$x_0$</span> is a fixed point, so we do not need to train for it. In cases where the dynamics have some dependence on the neural network (e.g., in <a href="../policy_search/">policy search</a>), we should rather train directly for <span>$f(x_0) = 0$</span>, since the minimization condition will also guarantee <span>$[\nabla V](x_0) = 0$</span>, so <span>$\dot{V}(x_0) = 0$</span>.</p><p>NeuralLyapunov.jl provides the <a href="#NeuralLyapunov.LyapunovDecreaseCondition"><code>LyapunovDecreaseCondition</code></a> struct for users to specify the form of the decrease condition to enforce through training.</p><article><details class="docstring" open="true"><summary id="NeuralLyapunov.LyapunovDecreaseCondition"><a class="docstring-binding" href="#NeuralLyapunov.LyapunovDecreaseCondition"><code>NeuralLyapunov.LyapunovDecreaseCondition</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">LyapunovDecreaseCondition(check_decrease, rate_metric, strength, rectifier)</code></pre><p>Specifies the form of the Lyapunov decrease condition to be used.</p><p><strong>Fields</strong></p><ul><li><code>check_decrease::Bool</code>: whether or not to train for negativity/nonpositivity of <span>$V̇(x)$</span>.</li><li><code>rate_metric</code>: should increase with second input, <span>$V̇(x)$</span>; used when <code>check_decrease == true</code>.</li><li><code>strength</code>: specifies the level of strictness for negativity training; should be zero when the two inputs are equal and nonnegative otherwise; used when <code>check_decrease</code> is <code>true</code>.</li><li><code>rectifier</code>: positive when the input is positive and (approximately) zero when the input is negative.</li></ul><p><strong>Training conditions</strong></p><p>If <code>check_decrease == true</code>, training will enforce:</p><p><span>$\texttt{rate\_metric}(V(x), V̇(x)) ≤ -\texttt{strength}(x, x_0).$</span></p><p>The inequality will be approximated by the equation:</p><p><span>$\texttt{rectifier}(\texttt{rate\_metric}(V(x), V̇(x)) + \texttt{strength}(x, x_0)) = 0.$</span></p><p>Note that the approximate equation and inequality are identical when <span>$\texttt{rectifier}(t) = \max(0, t)$</span>.</p><p>If the dynamics truly have a fixed point at <span>$x_0$</span> and <span>$V̇(x)$</span> is truly the rate of decrease of <span>$V(x)$</span> along the dynamics, then <span>$V̇(x_0)$</span> will be <span>$0$</span> and there is no need to train for <span>$V̇(x_0) = 0$</span>.</p><p><strong>Examples:</strong></p><p>Asymptotic decrease can be enforced by requiring     <span>$V̇(x) ≤ -C \lVert x - x_0 \rVert^2$</span>, for some positive <span>$C$</span>, which corresponds to</p><pre><code class="nohighlight hljs">rate_metric = (V, dVdt) -&gt; dVdt
strength = (x, x0) -&gt; C * (x - x0) ⋅ (x - x0)</code></pre><p>This can also be accomplished with <a href="#NeuralLyapunov.AsymptoticStability"><code>AsymptoticStability</code></a>.</p><p>Exponential decrease of rate <span>$k$</span> is proven by     <span>$V̇(x) ≤ - k V(x)$</span>, which corresponds to</p><pre><code class="nohighlight hljs">rate_metric = (V, dVdt) -&gt; dVdt + k * V
strength = (x, x0) -&gt; 0.0</code></pre><p>This can also be accomplished with <a href="#NeuralLyapunov.ExponentialStability"><code>ExponentialStability</code></a>.</p><p>In either case, the rectified linear unit <code>rectifier = (t) -&gt; max(zero(t), t)</code> exactly represents the inequality, but differentiable approximations of this function may be employed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/35d9244229d6f5c404e247ebfbd7f7fe199924df/src/decrease_conditions.jl#L1-L57">source</a></section></details></article><h2 id="Pre-defined-decrease-conditions"><a class="docs-heading-anchor" href="#Pre-defined-decrease-conditions">Pre-defined decrease conditions</a><a id="Pre-defined-decrease-conditions-1"></a><a class="docs-heading-anchor-permalink" href="#Pre-defined-decrease-conditions" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="NeuralLyapunov.AsymptoticStability"><a class="docstring-binding" href="#NeuralLyapunov.AsymptoticStability"><code>NeuralLyapunov.AsymptoticStability</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">AsymptoticStability(; C, strength, rectifier)</code></pre><p>Construct a <a href="#NeuralLyapunov.LyapunovDecreaseCondition"><code>LyapunovDecreaseCondition</code></a> corresponding to asymptotic stability.</p><p>The decrease condition for asymptotic stability is <span>$V̇(x) &lt; 0$</span>, which is here represented as <span>$V̇(x) ≤ - \texttt{strength}(x, x_0)$</span>, where <code>strength</code> is positive definite around <span>$x_0$</span>. By default, <span>$\texttt{strength}(x, x_0) = C \lVert x - x_0 \rVert^2$</span> for the supplied <span>$C &gt; 0$</span>. <span>$C$</span> defaults to <code>1e-6</code>.</p><p>The inequality is represented by <span>$\texttt{rectifier}(V̇(x) + \texttt{strength}(x, x_0)) = 0$</span>. The default value <code>rectifier = (t) -&gt; max(zero(t), t)</code> exactly represents the inequality, but differentiable approximations of this function may be employed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/35d9244229d6f5c404e247ebfbd7f7fe199924df/src/decrease_conditions.jl#L105-L119">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralLyapunov.ExponentialStability"><a class="docstring-binding" href="#NeuralLyapunov.ExponentialStability"><code>NeuralLyapunov.ExponentialStability</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">ExponentialStability(k; rectifier)</code></pre><p>Construct a <a href="#NeuralLyapunov.LyapunovDecreaseCondition"><code>LyapunovDecreaseCondition</code></a> corresponding to exponential stability of rate <span>$k$</span>.</p><p>The Lyapunov condition for exponential stability is <span>$V̇(x) ≤ -k V(x)$</span> for some <span>$k &gt; 0$</span>.</p><p>The inequality is represented by <span>$\texttt{rectifier}(V̇(x) + k V(x)) = 0$</span>. The default value <code>rectifier = (t) -&gt; max(zero(t), t)</code> exactly represents the inequality, but differentiable approximations of this function may be employed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/35d9244229d6f5c404e247ebfbd7f7fe199924df/src/decrease_conditions.jl#L133-L144">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralLyapunov.StabilityISL"><a class="docstring-binding" href="#NeuralLyapunov.StabilityISL"><code>NeuralLyapunov.StabilityISL</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">StabilityISL(; rectifier)</code></pre><p>Construct a <a href="#NeuralLyapunov.LyapunovDecreaseCondition"><code>LyapunovDecreaseCondition</code></a> corresponding to stability in the sense of Lyapunov (i.s.L.).</p><p>Stability i.s.L. is proven by <span>$V̇(x) ≤ 0$</span>. The inequality is represented by <span>$\texttt{rectifier}(V̇(x)) = 0$</span>. The default value <code>rectifier = (t) -&gt; max(zero(t), t)</code> exactly represents the inequality, but differentiable approximations of this function may be employed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/35d9244229d6f5c404e247ebfbd7f7fe199924df/src/decrease_conditions.jl#L85-L95">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralLyapunov.DontCheckDecrease"><a class="docstring-binding" href="#NeuralLyapunov.DontCheckDecrease"><code>NeuralLyapunov.DontCheckDecrease</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">DontCheckDecrease()</code></pre><p>Construct a <a href="#NeuralLyapunov.LyapunovDecreaseCondition"><code>LyapunovDecreaseCondition</code></a> which represents not checking for decrease of the Lyapunov function along system trajectories. This is appropriate in cases when the Lyapunov decrease condition has been structurally enforced.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/35d9244229d6f5c404e247ebfbd7f7fe199924df/src/decrease_conditions.jl#L157-L163">source</a></section></details></article><h2 id="Defining-your-own-decrease-condition"><a class="docs-heading-anchor" href="#Defining-your-own-decrease-condition">Defining your own decrease condition</a><a id="Defining-your-own-decrease-condition-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-your-own-decrease-condition" title="Permalink"></a></h2><p>If a user wishes to define their own version of the decrease condition in a form other than <span>$\texttt{rate\_metric}(V(x), \dot{V}(x)) \le - \texttt{strength}(x, x_0)$</span>, they must define their own subtype of <a href="#NeuralLyapunov.AbstractLyapunovDecreaseCondition"><code>AbstractLyapunovDecreaseCondition</code></a>.</p><article><details class="docstring" open="true"><summary id="NeuralLyapunov.AbstractLyapunovDecreaseCondition"><a class="docstring-binding" href="#NeuralLyapunov.AbstractLyapunovDecreaseCondition"><code>NeuralLyapunov.AbstractLyapunovDecreaseCondition</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">AbstractLyapunovDecreaseCondition</code></pre><p>Represents the decrease condition in a neural Lyapunov problem</p><p>All concrete <code>AbstractLyapunovDecreaseCondition</code> subtypes should define the <code>check_decrease</code> and <code>get_decrease_condition</code> functions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/35d9244229d6f5c404e247ebfbd7f7fe199924df/src/conditions_specification.jl#L37-L44">source</a></section></details></article><p>When constructing the PDESystem, <a href="../pdesystem/#NeuralLyapunov.NeuralLyapunovPDESystem"><code>NeuralLyapunovPDESystem</code></a> uses <a href="#NeuralLyapunov.check_decrease"><code>check_decrease</code></a> to determine if it should include an equation equating the result of <a href="#NeuralLyapunov.get_decrease_condition"><code>get_decrease_condition</code></a> to zero.</p><article><details class="docstring" open="true"><summary id="NeuralLyapunov.check_decrease"><a class="docstring-binding" href="#NeuralLyapunov.check_decrease"><code>NeuralLyapunov.check_decrease</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">check_decrease(cond::AbstractLyapunovDecreaseCondition)</code></pre><p>Return <code>true</code> if <code>cond</code> specifies training to meet the Lyapunov decrease condition, and <code>false</code> if <code>cond</code> specifies no training to meet this condition.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/35d9244229d6f5c404e247ebfbd7f7fe199924df/src/conditions_specification.jl#L109-L114">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralLyapunov.get_decrease_condition"><a class="docstring-binding" href="#NeuralLyapunov.get_decrease_condition"><code>NeuralLyapunov.get_decrease_condition</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">get_decrease_condition(cond::AbstractLyapunovDecreaseCondition)</code></pre><p>Return a function of <span>$V$</span>, <span>$V̇$</span>, <span>$x$</span>, and <span>$x_0$</span> that returns zero when the Lyapunov decrease condition is met and a value greater than zero when it is violated.</p><p>Note that the first two inputs, <span>$V$</span> and <span>$V̇$</span>, are functions, so the decrease condition can depend on the value of these functions at multiple points.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralLyapunov.jl/blob/35d9244229d6f5c404e247ebfbd7f7fe199924df/src/conditions_specification.jl#L122-L130">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../minimization/">« Lyapunov Minimization Condition</a><a class="docs-footer-nextpage" href="../structure/">Structuring a Neural Lyapunov function »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Saturday 10 January 2026 22:52">Saturday 10 January 2026</span>. Using Julia version 1.11.8.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
